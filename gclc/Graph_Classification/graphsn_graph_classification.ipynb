{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import argparse\n",
    "import heapq as hp\n",
    "\n",
    "from graph_data import GraphData\n",
    "from data_reader import DataReader\n",
    "from models import GNN\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=117, type=<class 'int'>, choices=None, help='Random seed.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "'''\n",
    "----------------------------\n",
    "Dataset  |   batchnorm_dim\n",
    "----------------------------\n",
    "MUTAG    |     28\n",
    "PTC_MR   |     64\n",
    "BZR      |     57\n",
    "COX2     |     56\n",
    "COX2_MD  |     36\n",
    "BZR-MD   |     33\n",
    "PROTEINS |    620\n",
    "D&D      |   5748\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', default='cpu', help='Select CPU/CUDA for training.')\n",
    "parser.add_argument('--dataset', default='MUTAG', help='Dataset name.')\n",
    "parser.add_argument('--epochs', type=int, default=500, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.009, help='Initial learning rate.')\n",
    "parser.add_argument('--wdecay', type=float, default=9e-3, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Batch size.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=64, help='Number of hidden units.')\n",
    "parser.add_argument('--n_layers', type=int, default=2, help='Number of MLP layers for GraphSN.')\n",
    "parser.add_argument('--batchnorm_dim', type=int, default=28, help='Batchnormalization dimension for GraphSN layer.')\n",
    "parser.add_argument('--dropout_1', type=float, default=0.5, help='Dropout rate for concatenation the outputs.')\n",
    "parser.add_argument('--dropout_2', type=float, default=0.6, help='Dropout rate for MLP layers in GraphSN.')\n",
    "parser.add_argument('--n_folds', type=int, default=10, help='Number of folds in cross validation.')\n",
    "parser.add_argument('--threads', type=int, default=0, help='Number of threads.')\n",
    "parser.add_argument('--log_interval', type=int, default=10 , help='Log interval for visualizing outputs.')\n",
    "parser.add_argument('--seed', type=int, default=117, help='Random seed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "feature_onehot: 7\n",
      "making labels sequential, otherwise pytorch might crash\n",
      "N nodes avg/std/min/max: \t17.93/4.58/10/28\n",
      "N edges avg/std/min/max: \t19.79/5.68/10/33\n",
      "Node degree avg/std/min/max: \t2.21/0.74/1/4\n",
      "Node features dim: \t\t7\n",
      "N classes: \t\t\t2\n",
      "Classes: \t\t\t[0 1]\n",
      "Class 0: \t\t\t63 samples\n",
      "Class 1: \t\t\t125 samples\n",
      "feature 0, count 2395/3371\n",
      "feature 1, count 345/3371\n",
      "feature 2, count 593/3371\n",
      "feature 3, count 12/3371\n",
      "feature 4, count 1/3371\n",
      "feature 5, count 23/3371\n",
      "feature 6, count 2/3371\n",
      "[array([  0,   1,   2,   3,   4,   5,   7,   9,  10,  11,  12,  13,  14,\n",
      "        15,  17,  18,  20,  21,  22,  23,  25,  26,  27,  28,  29,  31,\n",
      "        32,  33,  34,  35,  36,  37,  38,  39,  40,  42,  44,  45,  46,\n",
      "        47,  48,  49,  50,  51,  52,  53,  54,  56,  58,  59,  60,  61,\n",
      "        62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,\n",
      "        75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,\n",
      "        88,  89,  90,  91,  93,  94,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
      "       119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
      "       133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
      "       147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,\n",
      "       160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174,\n",
      "       175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187])]\n",
      "[array([  1,   2,   4,  13,  18,  33,  35,  37,  38,  39,  54,  61,  64,\n",
      "        65,  69,  72,  75,  76,  77,  83,  87,  88,  97,  99, 110, 112,\n",
      "       113, 115, 118, 119, 123, 128, 129, 130, 131, 132, 134, 137, 138,\n",
      "       140, 141, 142, 143, 146, 149, 153, 154, 155, 159, 167, 174, 178,\n",
      "       180, 181, 184, 185, 187,   0,   3,   5,   7,   9,  10,  11,  12,\n",
      "        14,  15,  17,  20,  21,  22,  23,  25,  26,  27,  28,  29,  31,\n",
      "        32,  34,  36,  40,  42,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  56,  58,  59,  60,  62,  63,  66,  67,  68,  70,  71,\n",
      "        73,  74,  78,  79,  80,  81,  82,  84,  85,  86,  89,  90,  91,\n",
      "        93,  94,  98, 100, 101, 102, 103, 104, 105, 106, 107, 111, 114,\n",
      "       116, 117, 120, 121, 124, 125, 126, 127, 133, 135, 139, 144, 145,\n",
      "       147, 148, 150, 151, 152, 156, 157, 158, 160, 162, 163, 164, 165,\n",
      "       166, 168, 169, 170, 172, 173, 175, 176, 177, 179, 182, 183, 186])]\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "\"\"\"dataset_fold_idx_path = './data/%s/' % args.dataset.upper() + 'fold_idx/'\n",
    "datareader = DataReader(data_dir='./data/%s/' % args.dataset.upper(),\n",
    "                         fold_dir=dataset_fold_idx_path,\n",
    "                         rnd_state=np.random.RandomState(args.seed),\n",
    "                         folds=args.n_folds,                    \n",
    "                         use_cont_node_attr=False)\"\"\"\n",
    "\n",
    "datareader = DataReader(data_dir='./data/%s/' % args.dataset.upper(),\n",
    "                        fold_dir=None,\n",
    "                        rnd_state=np.random.RandomState(args.seed),\n",
    "                        folds=args.n_folds,\n",
    "                        use_cont_node_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图的数量\n",
    "dataset_length = len(datareader.data['adj_list'])\n",
    "for itr in np.arange(dataset_length):\n",
    "    # 每个图的矩阵\n",
    "    A_array = datareader.data['adj_list'][itr]\n",
    "    G = nx.from_numpy_matrix(A_array)\n",
    "    \n",
    "    sub_graphs = []\n",
    "    subgraph_nodes_list = []\n",
    "    sub_graphs_adj = []\n",
    "    sub_graph_edges = []\n",
    "    new_adj = torch.zeros(A_array.shape[0], A_array.shape[0])\n",
    "    \n",
    "    # 每个图的子图\n",
    "    for i in np.arange(len(A_array)):\n",
    "        s_indexes = []\n",
    "        for j in np.arange(len(A_array)):\n",
    "            s_indexes.append(i)\n",
    "            if(A_array[i][j]==1):\n",
    "                s_indexes.append(j)\n",
    "        sub_graphs.append(G.subgraph(s_indexes))\n",
    "\n",
    "    \n",
    "\n",
    "    # 每个图的每个子图的节点\n",
    "    for i in np.arange(len(sub_graphs)):\n",
    "        subgraph_nodes_list.append(list(sub_graphs[i].nodes))\n",
    "\n",
    "    # 每个图的每个子图矩阵\n",
    "    for index in np.arange(len(sub_graphs)):\n",
    "        sub_graphs_adj.append(nx.adjacency_matrix(sub_graphs[index]).toarray())\n",
    "    #print(\"sub_graphs_adj:\", sub_graphs_adj)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # 每个图的每个子图的边的数量\n",
    "    for index in np.arange(len(sub_graphs)):\n",
    "        sub_graph_edges.append(sub_graphs[index].number_of_edges())\n",
    "\n",
    "    # 每个图(包含每个图的子图)的新的矩阵\n",
    "    for node in np.arange(len(subgraph_nodes_list)):\n",
    "        sub_adj = sub_graphs_adj[node]\n",
    "        for neighbors in np.arange(len(subgraph_nodes_list[node])):\n",
    "            index = subgraph_nodes_list[node][neighbors]\n",
    "            count = torch.tensor(0).float()\n",
    "            if(index==node):\n",
    "                continue\n",
    "            else:\n",
    "                c_neighbors = set(subgraph_nodes_list[node]).intersection(subgraph_nodes_list[index])\n",
    "                if index in c_neighbors:\n",
    "                    nodes_list = subgraph_nodes_list[node]\n",
    "                    sub_graph_index = nodes_list.index(index)\n",
    "                    c_neighbors_list = list(c_neighbors)\n",
    "                    #print(len(c_neighbors))\n",
    "                    for i, item1 in enumerate(nodes_list):\n",
    "                        if(item1 in c_neighbors):\n",
    "                            for item2 in c_neighbors_list:\n",
    "                                j = nodes_list.index(item2)\n",
    "                                count += sub_adj[i][j]\n",
    "\n",
    "                new_adj[node][index] = count / 2\n",
    "                new_adj[node][index] = new_adj[node][index]/(len(c_neighbors)*(len(c_neighbors)-1))\n",
    "                new_adj[node][index] = new_adj[node][index] * (len(c_neighbors) ** 2)\n",
    "\n",
    "    \n",
    "    weight = torch.FloatTensor(new_adj)\n",
    "    weight = weight / weight.sum(1, keepdim=True)\n",
    "\n",
    "    weight = weight + torch.FloatTensor(A_array)\n",
    "\n",
    "    coeff = weight.sum(1, keepdim=True)\n",
    "    coeff = torch.diag((coeff.T)[0])\n",
    "\n",
    "    weight = weight + coeff\n",
    "\n",
    "    weight = weight.detach().numpy()\n",
    "    weight = np.nan_to_num(weight, nan=0)\n",
    "\n",
    "    datareader.data['adj_list'][itr] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 0\n",
      "TRAIN: 169/188\n",
      "TEST: 19/188\n",
      "\n",
      "Initialize model\n",
      "GNN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GraphSN(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=7, out_features=64, bias=True)\n",
      "        (1): Dropout(p=0.6, inplace=False)\n",
      "        (2): ReLU()\n",
      "        (3): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (5): Dropout(p=0.6, inplace=False)\n",
      "        (6): ReLU()\n",
      "        (7): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): GraphSN(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): Dropout(p=0.6, inplace=False)\n",
      "        (2): ReLU()\n",
      "        (3): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (5): Dropout(p=0.6, inplace=False)\n",
      "        (6): ReLU()\n",
      "        (7): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=135, out_features=2, bias=True)\n",
      ")\n",
      "N trainable parameters: 21810\n",
      "Train Epoch: 0 [64/169 (33%)]\tLoss: 0.840316 (avg: 0.840316) \tsec/iter: 0.7707\n",
      "Train Epoch: 0 [169/169 (100%)]\tLoss: 0.818662 (avg: 4.837937) \tsec/iter: 0.2673\n",
      "Test set (epoch 0): Average loss: 1.9297, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 1 [64/169 (33%)]\tLoss: 1.266610 (avg: 1.266610) \tsec/iter: 0.0156\n",
      "Train Epoch: 1 [169/169 (100%)]\tLoss: 1.140594 (avg: 1.005534) \tsec/iter: 0.0178\n",
      "Test set (epoch 1): Average loss: 0.7237, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 2 [64/169 (33%)]\tLoss: 0.890001 (avg: 0.890001) \tsec/iter: 0.0313\n",
      "Train Epoch: 2 [169/169 (100%)]\tLoss: 0.846577 (avg: 0.922070) \tsec/iter: 0.0208\n",
      "Test set (epoch 2): Average loss: 0.7032, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 3 [64/169 (33%)]\tLoss: 0.493740 (avg: 0.493740) \tsec/iter: 0.0312\n",
      "Train Epoch: 3 [169/169 (100%)]\tLoss: 0.292545 (avg: 0.491695) \tsec/iter: 0.0230\n",
      "Test set (epoch 3): Average loss: 1.4548, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 4 [64/169 (33%)]\tLoss: 0.417428 (avg: 0.417428) \tsec/iter: 0.0313\n",
      "Train Epoch: 4 [169/169 (100%)]\tLoss: 0.716015 (avg: 0.523370) \tsec/iter: 0.0230\n",
      "Test set (epoch 4): Average loss: 0.9227, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 5 [64/169 (33%)]\tLoss: 0.324591 (avg: 0.324591) \tsec/iter: 0.0313\n",
      "Train Epoch: 5 [169/169 (100%)]\tLoss: 0.629435 (avg: 0.547527) \tsec/iter: 0.0208\n",
      "Test set (epoch 5): Average loss: 1.4505, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 6 [64/169 (33%)]\tLoss: 0.440216 (avg: 0.440216) \tsec/iter: 0.0221\n",
      "Train Epoch: 6 [169/169 (100%)]\tLoss: 0.644142 (avg: 0.432998) \tsec/iter: 0.0230\n",
      "Test set (epoch 6): Average loss: 1.7686, Accuracy: 7/19 (36.84%)\n",
      "\n",
      "Train Epoch: 7 [64/169 (33%)]\tLoss: 0.690998 (avg: 0.690998) \tsec/iter: 0.0312\n",
      "Train Epoch: 7 [169/169 (100%)]\tLoss: 0.510713 (avg: 0.526121) \tsec/iter: 0.0230\n",
      "Test set (epoch 7): Average loss: 1.4127, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 8 [64/169 (33%)]\tLoss: 0.386576 (avg: 0.386576) \tsec/iter: 0.0312\n",
      "Train Epoch: 8 [169/169 (100%)]\tLoss: 0.170297 (avg: 0.399772) \tsec/iter: 0.0208\n",
      "Test set (epoch 8): Average loss: 0.9679, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 9 [64/169 (33%)]\tLoss: 0.626886 (avg: 0.626886) \tsec/iter: 0.0221\n",
      "Train Epoch: 9 [169/169 (100%)]\tLoss: 0.312626 (avg: 0.449643) \tsec/iter: 0.0230\n",
      "Test set (epoch 9): Average loss: 1.1066, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 10 [64/169 (33%)]\tLoss: 0.531491 (avg: 0.531491) \tsec/iter: 0.0156\n",
      "Train Epoch: 10 [169/169 (100%)]\tLoss: 0.412843 (avg: 0.450901) \tsec/iter: 0.0178\n",
      "Test set (epoch 10): Average loss: 1.1983, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 11 [64/169 (33%)]\tLoss: 0.319653 (avg: 0.319653) \tsec/iter: 0.0156\n",
      "Train Epoch: 11 [169/169 (100%)]\tLoss: 0.345515 (avg: 0.336368) \tsec/iter: 0.0230\n",
      "Test set (epoch 11): Average loss: 1.2582, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 12 [64/169 (33%)]\tLoss: 0.354923 (avg: 0.354923) \tsec/iter: 0.0313\n",
      "Train Epoch: 12 [169/169 (100%)]\tLoss: 0.384600 (avg: 0.393245) \tsec/iter: 0.0208\n",
      "Test set (epoch 12): Average loss: 1.4187, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 13 [64/169 (33%)]\tLoss: 0.404415 (avg: 0.404415) \tsec/iter: 0.0222\n",
      "Train Epoch: 13 [169/169 (100%)]\tLoss: 0.414965 (avg: 0.376922) \tsec/iter: 0.0178\n",
      "Test set (epoch 13): Average loss: 1.3841, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 14 [64/169 (33%)]\tLoss: 0.430352 (avg: 0.430352) \tsec/iter: 0.0156\n",
      "Train Epoch: 14 [169/169 (100%)]\tLoss: 0.441681 (avg: 0.439235) \tsec/iter: 0.0230\n",
      "Test set (epoch 14): Average loss: 1.0379, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 15 [64/169 (33%)]\tLoss: 0.351172 (avg: 0.351172) \tsec/iter: 0.0313\n",
      "Train Epoch: 15 [169/169 (100%)]\tLoss: 0.390399 (avg: 0.358994) \tsec/iter: 0.0208\n",
      "Test set (epoch 15): Average loss: 1.3161, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 16 [64/169 (33%)]\tLoss: 0.393146 (avg: 0.393146) \tsec/iter: 0.0221\n",
      "Train Epoch: 16 [169/169 (100%)]\tLoss: 0.429464 (avg: 0.427049) \tsec/iter: 0.0178\n",
      "Test set (epoch 16): Average loss: 1.1126, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 17 [64/169 (33%)]\tLoss: 0.317663 (avg: 0.317663) \tsec/iter: 0.0156\n",
      "Train Epoch: 17 [169/169 (100%)]\tLoss: 0.407282 (avg: 0.360215) \tsec/iter: 0.0616\n",
      "Test set (epoch 17): Average loss: 1.3739, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 18 [64/169 (33%)]\tLoss: 0.414642 (avg: 0.414642) \tsec/iter: 0.0156\n",
      "Train Epoch: 18 [169/169 (100%)]\tLoss: 0.590860 (avg: 0.380066) \tsec/iter: 0.0230\n",
      "Test set (epoch 18): Average loss: 1.3649, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 19 [64/169 (33%)]\tLoss: 0.340845 (avg: 0.340845) \tsec/iter: 0.0312\n",
      "Train Epoch: 19 [169/169 (100%)]\tLoss: 0.273145 (avg: 0.351545) \tsec/iter: 0.0208\n",
      "Test set (epoch 19): Average loss: 1.1959, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 20 [64/169 (33%)]\tLoss: 0.386277 (avg: 0.386277) \tsec/iter: 0.0221\n",
      "Train Epoch: 20 [169/169 (100%)]\tLoss: 0.447201 (avg: 0.371976) \tsec/iter: 0.0230\n",
      "Test set (epoch 20): Average loss: 1.2745, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 21 [64/169 (33%)]\tLoss: 0.319488 (avg: 0.319488) \tsec/iter: 0.0156\n",
      "Train Epoch: 21 [169/169 (100%)]\tLoss: 0.370268 (avg: 0.366747) \tsec/iter: 0.0178\n",
      "Test set (epoch 21): Average loss: 1.3729, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 22 [64/169 (33%)]\tLoss: 0.332177 (avg: 0.332177) \tsec/iter: 0.0156\n",
      "Train Epoch: 22 [169/169 (100%)]\tLoss: 0.254283 (avg: 0.341206) \tsec/iter: 0.0208\n",
      "Test set (epoch 22): Average loss: 1.3132, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 23 [64/169 (33%)]\tLoss: 0.324495 (avg: 0.324495) \tsec/iter: 0.0156\n",
      "Train Epoch: 23 [169/169 (100%)]\tLoss: 0.392289 (avg: 0.399791) \tsec/iter: 0.0208\n",
      "Test set (epoch 23): Average loss: 1.1511, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 24 [64/169 (33%)]\tLoss: 0.294302 (avg: 0.294302) \tsec/iter: 0.0312\n",
      "Train Epoch: 24 [169/169 (100%)]\tLoss: 0.365052 (avg: 0.342916) \tsec/iter: 0.0178\n",
      "Test set (epoch 24): Average loss: 1.2164, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 25 [64/169 (33%)]\tLoss: 0.342212 (avg: 0.342212) \tsec/iter: 0.0156\n",
      "Train Epoch: 25 [169/169 (100%)]\tLoss: 0.290481 (avg: 0.418230) \tsec/iter: 0.0230\n",
      "Test set (epoch 25): Average loss: 1.2914, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 26 [64/169 (33%)]\tLoss: 0.341876 (avg: 0.341876) \tsec/iter: 0.0156\n",
      "Train Epoch: 26 [169/169 (100%)]\tLoss: 0.329963 (avg: 0.345454) \tsec/iter: 0.0208\n",
      "Test set (epoch 26): Average loss: 1.1978, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 27 [64/169 (33%)]\tLoss: 0.421018 (avg: 0.421018) \tsec/iter: 0.0312\n",
      "Train Epoch: 27 [169/169 (100%)]\tLoss: 0.292453 (avg: 0.346970) \tsec/iter: 0.0230\n",
      "Test set (epoch 27): Average loss: 1.2532, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 28 [64/169 (33%)]\tLoss: 0.379725 (avg: 0.379725) \tsec/iter: 0.0156\n",
      "Train Epoch: 28 [169/169 (100%)]\tLoss: 0.445482 (avg: 0.407370) \tsec/iter: 0.0208\n",
      "Test set (epoch 28): Average loss: 1.4786, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 29 [64/169 (33%)]\tLoss: 0.302439 (avg: 0.302439) \tsec/iter: 0.0156\n",
      "Train Epoch: 29 [169/169 (100%)]\tLoss: 0.264628 (avg: 0.353969) \tsec/iter: 0.0208\n",
      "Test set (epoch 29): Average loss: 1.2380, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 30 [64/169 (33%)]\tLoss: 0.495086 (avg: 0.495086) \tsec/iter: 0.0312\n",
      "Train Epoch: 30 [169/169 (100%)]\tLoss: 0.514961 (avg: 0.431740) \tsec/iter: 0.0282\n",
      "Test set (epoch 30): Average loss: 1.3050, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 31 [64/169 (33%)]\tLoss: 0.314164 (avg: 0.314164) \tsec/iter: 0.0156\n",
      "Train Epoch: 31 [169/169 (100%)]\tLoss: 0.552840 (avg: 0.374321) \tsec/iter: 0.0178\n",
      "Test set (epoch 31): Average loss: 1.2317, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 32 [64/169 (33%)]\tLoss: 0.334331 (avg: 0.334331) \tsec/iter: 0.0156\n",
      "Train Epoch: 32 [169/169 (100%)]\tLoss: 0.285779 (avg: 0.358251) \tsec/iter: 0.0208\n",
      "Test set (epoch 32): Average loss: 1.0941, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 33 [64/169 (33%)]\tLoss: 0.515191 (avg: 0.515191) \tsec/iter: 0.0221\n",
      "Train Epoch: 33 [169/169 (100%)]\tLoss: 0.344200 (avg: 0.412436) \tsec/iter: 0.0230\n",
      "Test set (epoch 33): Average loss: 1.0980, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 34 [64/169 (33%)]\tLoss: 0.325074 (avg: 0.325074) \tsec/iter: 0.0312\n",
      "Train Epoch: 34 [169/169 (100%)]\tLoss: 0.403388 (avg: 0.346516) \tsec/iter: 0.0282\n",
      "Test set (epoch 34): Average loss: 1.3628, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 35 [64/169 (33%)]\tLoss: 0.317693 (avg: 0.317693) \tsec/iter: 0.0313\n",
      "Train Epoch: 35 [169/169 (100%)]\tLoss: 0.389147 (avg: 0.364791) \tsec/iter: 0.0282\n",
      "Test set (epoch 35): Average loss: 1.0567, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 36 [64/169 (33%)]\tLoss: 0.383238 (avg: 0.383238) \tsec/iter: 0.0312\n",
      "Train Epoch: 36 [169/169 (100%)]\tLoss: 0.474870 (avg: 0.374073) \tsec/iter: 0.0282\n",
      "Test set (epoch 36): Average loss: 1.5506, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 37 [64/169 (33%)]\tLoss: 0.408165 (avg: 0.408165) \tsec/iter: 0.0313\n",
      "Train Epoch: 37 [169/169 (100%)]\tLoss: 0.381941 (avg: 0.379479) \tsec/iter: 0.0260\n",
      "Test set (epoch 37): Average loss: 1.3683, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 38 [64/169 (33%)]\tLoss: 0.389596 (avg: 0.389596) \tsec/iter: 0.0222\n",
      "Train Epoch: 38 [169/169 (100%)]\tLoss: 0.397872 (avg: 0.367087) \tsec/iter: 0.0178\n",
      "Test set (epoch 38): Average loss: 1.2347, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 39 [64/169 (33%)]\tLoss: 0.435713 (avg: 0.435713) \tsec/iter: 0.0156\n",
      "Train Epoch: 39 [169/169 (100%)]\tLoss: 0.299375 (avg: 0.330530) \tsec/iter: 0.0178\n",
      "Test set (epoch 39): Average loss: 1.3157, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 40 [64/169 (33%)]\tLoss: 0.220362 (avg: 0.220362) \tsec/iter: 0.0313\n",
      "Train Epoch: 40 [169/169 (100%)]\tLoss: 0.400844 (avg: 0.346846) \tsec/iter: 0.0208\n",
      "Test set (epoch 40): Average loss: 1.3096, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 41 [64/169 (33%)]\tLoss: 0.342988 (avg: 0.342988) \tsec/iter: 0.0156\n",
      "Train Epoch: 41 [169/169 (100%)]\tLoss: 0.355915 (avg: 0.324924) \tsec/iter: 0.0156\n",
      "Test set (epoch 41): Average loss: 1.3941, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 42 [64/169 (33%)]\tLoss: 0.337561 (avg: 0.337561) \tsec/iter: 0.0156\n",
      "Train Epoch: 42 [169/169 (100%)]\tLoss: 0.460350 (avg: 0.356459) \tsec/iter: 0.0178\n",
      "Test set (epoch 42): Average loss: 1.5642, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 43 [64/169 (33%)]\tLoss: 0.330314 (avg: 0.330314) \tsec/iter: 0.0313\n",
      "Train Epoch: 43 [169/169 (100%)]\tLoss: 0.436979 (avg: 0.375946) \tsec/iter: 0.0208\n",
      "Test set (epoch 43): Average loss: 1.4685, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 44 [64/169 (33%)]\tLoss: 0.370088 (avg: 0.370088) \tsec/iter: 0.0222\n",
      "Train Epoch: 44 [169/169 (100%)]\tLoss: 0.439652 (avg: 0.370317) \tsec/iter: 0.0230\n",
      "Test set (epoch 44): Average loss: 1.1655, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 45 [64/169 (33%)]\tLoss: 0.279204 (avg: 0.279204) \tsec/iter: 0.0312\n",
      "Train Epoch: 45 [169/169 (100%)]\tLoss: 0.686188 (avg: 0.408246) \tsec/iter: 0.0230\n",
      "Test set (epoch 45): Average loss: 1.8893, Accuracy: 6/19 (31.58%)\n",
      "\n",
      "Train Epoch: 46 [64/169 (33%)]\tLoss: 0.396306 (avg: 0.396306) \tsec/iter: 0.0156\n",
      "Train Epoch: 46 [169/169 (100%)]\tLoss: 0.247554 (avg: 0.380899) \tsec/iter: 0.0208\n",
      "Test set (epoch 46): Average loss: 1.3790, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 47 [64/169 (33%)]\tLoss: 0.327107 (avg: 0.327107) \tsec/iter: 0.0156\n",
      "Train Epoch: 47 [169/169 (100%)]\tLoss: 0.424724 (avg: 0.331773) \tsec/iter: 0.0208\n",
      "Test set (epoch 47): Average loss: 1.4943, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 48 [64/169 (33%)]\tLoss: 0.315302 (avg: 0.315302) \tsec/iter: 0.0313\n",
      "Train Epoch: 48 [169/169 (100%)]\tLoss: 0.304390 (avg: 0.325245) \tsec/iter: 0.0230\n",
      "Test set (epoch 48): Average loss: 1.5042, Accuracy: 7/19 (36.84%)\n",
      "\n",
      "Train Epoch: 49 [64/169 (33%)]\tLoss: 0.415191 (avg: 0.415191) \tsec/iter: 0.0313\n",
      "Train Epoch: 49 [169/169 (100%)]\tLoss: 0.387804 (avg: 0.345053) \tsec/iter: 0.0230\n",
      "Test set (epoch 49): Average loss: 1.4747, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 50 [64/169 (33%)]\tLoss: 0.343158 (avg: 0.343158) \tsec/iter: 0.0156\n",
      "Train Epoch: 50 [169/169 (100%)]\tLoss: 0.347294 (avg: 0.359446) \tsec/iter: 0.0208\n",
      "Test set (epoch 50): Average loss: 1.5025, Accuracy: 7/19 (36.84%)\n",
      "\n",
      "Train Epoch: 51 [64/169 (33%)]\tLoss: 0.322866 (avg: 0.322866) \tsec/iter: 0.0312\n",
      "Train Epoch: 51 [169/169 (100%)]\tLoss: 0.374884 (avg: 0.364645) \tsec/iter: 0.0230\n",
      "Test set (epoch 51): Average loss: 1.1929, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 52 [64/169 (33%)]\tLoss: 0.235244 (avg: 0.235244) \tsec/iter: 0.0312\n",
      "Train Epoch: 52 [169/169 (100%)]\tLoss: 0.396221 (avg: 0.361290) \tsec/iter: 0.0230\n",
      "Test set (epoch 52): Average loss: 1.5551, Accuracy: 7/19 (36.84%)\n",
      "\n",
      "Train Epoch: 53 [64/169 (33%)]\tLoss: 0.297960 (avg: 0.297960) \tsec/iter: 0.0156\n",
      "Train Epoch: 53 [169/169 (100%)]\tLoss: 0.398387 (avg: 0.358787) \tsec/iter: 0.0208\n",
      "Test set (epoch 53): Average loss: 1.6658, Accuracy: 7/19 (36.84%)\n",
      "\n",
      "Train Epoch: 54 [64/169 (33%)]\tLoss: 0.259172 (avg: 0.259172) \tsec/iter: 0.0221\n",
      "Train Epoch: 54 [169/169 (100%)]\tLoss: 0.405291 (avg: 0.352974) \tsec/iter: 0.0230\n",
      "Test set (epoch 54): Average loss: 1.0857, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 55 [64/169 (33%)]\tLoss: 0.265162 (avg: 0.265162) \tsec/iter: 0.0156\n",
      "Train Epoch: 55 [169/169 (100%)]\tLoss: 0.237376 (avg: 0.409170) \tsec/iter: 0.0178\n",
      "Test set (epoch 55): Average loss: 1.4937, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 56 [64/169 (33%)]\tLoss: 0.324074 (avg: 0.324074) \tsec/iter: 0.0312\n",
      "Train Epoch: 56 [169/169 (100%)]\tLoss: 0.343999 (avg: 0.378282) \tsec/iter: 0.0208\n",
      "Test set (epoch 56): Average loss: 1.3176, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 57 [64/169 (33%)]\tLoss: 0.385090 (avg: 0.385090) \tsec/iter: 0.0065\n",
      "Train Epoch: 57 [169/169 (100%)]\tLoss: 0.241550 (avg: 0.339032) \tsec/iter: 0.0178\n",
      "Test set (epoch 57): Average loss: 1.3762, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 58 [64/169 (33%)]\tLoss: 0.397184 (avg: 0.397184) \tsec/iter: 0.0313\n",
      "Train Epoch: 58 [169/169 (100%)]\tLoss: 0.541104 (avg: 0.362382) \tsec/iter: 0.0282\n",
      "Test set (epoch 58): Average loss: 1.1658, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 59 [64/169 (33%)]\tLoss: 0.390332 (avg: 0.390332) \tsec/iter: 0.0312\n",
      "Train Epoch: 59 [169/169 (100%)]\tLoss: 0.285860 (avg: 0.328219) \tsec/iter: 0.0230\n",
      "Test set (epoch 59): Average loss: 1.0737, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 60 [64/169 (33%)]\tLoss: 0.308677 (avg: 0.308677) \tsec/iter: 0.0156\n",
      "Train Epoch: 60 [169/169 (100%)]\tLoss: 0.256197 (avg: 0.335370) \tsec/iter: 0.0208\n",
      "Test set (epoch 60): Average loss: 1.2718, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 61 [64/169 (33%)]\tLoss: 0.294759 (avg: 0.294759) \tsec/iter: 0.0378\n",
      "Train Epoch: 61 [169/169 (100%)]\tLoss: 0.276088 (avg: 0.354836) \tsec/iter: 0.0230\n",
      "Test set (epoch 61): Average loss: 1.4205, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 62 [64/169 (33%)]\tLoss: 0.338622 (avg: 0.338622) \tsec/iter: 0.0313\n",
      "Train Epoch: 62 [169/169 (100%)]\tLoss: 0.376345 (avg: 0.357861) \tsec/iter: 0.0230\n",
      "Test set (epoch 62): Average loss: 1.0280, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 63 [64/169 (33%)]\tLoss: 0.322339 (avg: 0.322339) \tsec/iter: 0.0313\n",
      "Train Epoch: 63 [169/169 (100%)]\tLoss: 0.357487 (avg: 0.379784) \tsec/iter: 0.0260\n",
      "Test set (epoch 63): Average loss: 1.2886, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 64 [64/169 (33%)]\tLoss: 0.391259 (avg: 0.391259) \tsec/iter: 0.0221\n",
      "Train Epoch: 64 [169/169 (100%)]\tLoss: 0.277324 (avg: 0.328925) \tsec/iter: 0.0230\n",
      "Test set (epoch 64): Average loss: 1.2083, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 65 [64/169 (33%)]\tLoss: 0.241751 (avg: 0.241751) \tsec/iter: 0.0313\n",
      "Train Epoch: 65 [169/169 (100%)]\tLoss: 0.448362 (avg: 0.334270) \tsec/iter: 0.0230\n",
      "Test set (epoch 65): Average loss: 1.3124, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 66 [64/169 (33%)]\tLoss: 0.308255 (avg: 0.308255) \tsec/iter: 0.0156\n",
      "Train Epoch: 66 [169/169 (100%)]\tLoss: 0.414673 (avg: 0.365173) \tsec/iter: 0.0230\n",
      "Test set (epoch 66): Average loss: 1.3512, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 67 [64/169 (33%)]\tLoss: 0.304899 (avg: 0.304899) \tsec/iter: 0.0156\n",
      "Train Epoch: 67 [169/169 (100%)]\tLoss: 0.357847 (avg: 0.303774) \tsec/iter: 0.0208\n",
      "Test set (epoch 67): Average loss: 1.1733, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 68 [64/169 (33%)]\tLoss: 0.468022 (avg: 0.468022) \tsec/iter: 0.0312\n",
      "Train Epoch: 68 [169/169 (100%)]\tLoss: 0.293749 (avg: 0.338235) \tsec/iter: 0.0230\n",
      "Test set (epoch 68): Average loss: 1.3126, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 69 [64/169 (33%)]\tLoss: 0.357236 (avg: 0.357236) \tsec/iter: 0.0313\n",
      "Train Epoch: 69 [169/169 (100%)]\tLoss: 0.389119 (avg: 0.343647) \tsec/iter: 0.0230\n",
      "Test set (epoch 69): Average loss: 1.2637, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 70 [64/169 (33%)]\tLoss: 0.437476 (avg: 0.437476) \tsec/iter: 0.0313\n",
      "Train Epoch: 70 [169/169 (100%)]\tLoss: 0.305818 (avg: 0.332174) \tsec/iter: 0.0208\n",
      "Test set (epoch 70): Average loss: 1.2762, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 71 [64/169 (33%)]\tLoss: 0.285111 (avg: 0.285111) \tsec/iter: 0.0221\n",
      "Train Epoch: 71 [169/169 (100%)]\tLoss: 0.331553 (avg: 0.363896) \tsec/iter: 0.0183\n",
      "Test set (epoch 71): Average loss: 1.1911, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 72 [64/169 (33%)]\tLoss: 0.328864 (avg: 0.328864) \tsec/iter: 0.0313\n",
      "Train Epoch: 72 [169/169 (100%)]\tLoss: 0.356372 (avg: 0.362193) \tsec/iter: 0.0223\n",
      "Test set (epoch 72): Average loss: 1.2166, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 73 [64/169 (33%)]\tLoss: 0.270325 (avg: 0.270325) \tsec/iter: 0.0313\n",
      "Train Epoch: 73 [169/169 (100%)]\tLoss: 0.399405 (avg: 0.323150) \tsec/iter: 0.0208\n",
      "Test set (epoch 73): Average loss: 1.3465, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 74 [64/169 (33%)]\tLoss: 0.248053 (avg: 0.248053) \tsec/iter: 0.0221\n",
      "Train Epoch: 74 [169/169 (100%)]\tLoss: 0.317270 (avg: 0.338697) \tsec/iter: 0.0230\n",
      "Test set (epoch 74): Average loss: 1.2428, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 75 [64/169 (33%)]\tLoss: 0.318713 (avg: 0.318713) \tsec/iter: 0.0313\n",
      "Train Epoch: 75 [169/169 (100%)]\tLoss: 0.248769 (avg: 0.367003) \tsec/iter: 0.0178\n",
      "Test set (epoch 75): Average loss: 1.1804, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 76 [64/169 (33%)]\tLoss: 0.354030 (avg: 0.354030) \tsec/iter: 0.0156\n",
      "Train Epoch: 76 [169/169 (100%)]\tLoss: 0.276807 (avg: 0.339005) \tsec/iter: 0.0208\n",
      "Test set (epoch 76): Average loss: 1.0470, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 77 [64/169 (33%)]\tLoss: 0.379508 (avg: 0.379508) \tsec/iter: 0.0378\n",
      "Train Epoch: 77 [169/169 (100%)]\tLoss: 0.263652 (avg: 0.329728) \tsec/iter: 0.0230\n",
      "Test set (epoch 77): Average loss: 1.3168, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 78 [64/169 (33%)]\tLoss: 0.229665 (avg: 0.229665) \tsec/iter: 0.0156\n",
      "Train Epoch: 78 [169/169 (100%)]\tLoss: 0.462468 (avg: 0.334006) \tsec/iter: 0.0178\n",
      "Test set (epoch 78): Average loss: 1.3265, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 79 [64/169 (33%)]\tLoss: 0.338116 (avg: 0.338116) \tsec/iter: 0.0312\n",
      "Train Epoch: 79 [169/169 (100%)]\tLoss: 0.279159 (avg: 0.297335) \tsec/iter: 0.0230\n",
      "Test set (epoch 79): Average loss: 1.0143, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 80 [64/169 (33%)]\tLoss: 0.304018 (avg: 0.304018) \tsec/iter: 0.0313\n",
      "Train Epoch: 80 [169/169 (100%)]\tLoss: 0.410283 (avg: 0.346858) \tsec/iter: 0.0208\n",
      "Test set (epoch 80): Average loss: 1.3146, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 81 [64/169 (33%)]\tLoss: 0.307134 (avg: 0.307134) \tsec/iter: 0.0343\n",
      "Train Epoch: 81 [169/169 (100%)]\tLoss: 0.399409 (avg: 0.323922) \tsec/iter: 0.0249\n",
      "Test set (epoch 81): Average loss: 1.2750, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 82 [64/169 (33%)]\tLoss: 0.328301 (avg: 0.328301) \tsec/iter: 0.0222\n",
      "Train Epoch: 82 [169/169 (100%)]\tLoss: 0.321179 (avg: 0.351964) \tsec/iter: 0.0175\n",
      "Test set (epoch 82): Average loss: 0.9826, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 83 [64/169 (33%)]\tLoss: 0.355307 (avg: 0.355307) \tsec/iter: 0.0202\n",
      "Train Epoch: 83 [169/169 (100%)]\tLoss: 0.246896 (avg: 0.326212) \tsec/iter: 0.0202\n",
      "Test set (epoch 83): Average loss: 1.1653, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 84 [64/169 (33%)]\tLoss: 0.408113 (avg: 0.408113) \tsec/iter: 0.0202\n",
      "Train Epoch: 84 [169/169 (100%)]\tLoss: 0.326334 (avg: 0.313269) \tsec/iter: 0.0202\n",
      "Test set (epoch 84): Average loss: 1.0296, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 85 [64/169 (33%)]\tLoss: 0.324218 (avg: 0.324218) \tsec/iter: 0.0222\n",
      "Train Epoch: 85 [169/169 (100%)]\tLoss: 0.217161 (avg: 0.305384) \tsec/iter: 0.0175\n",
      "Test set (epoch 85): Average loss: 1.0100, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 86 [64/169 (33%)]\tLoss: 0.405977 (avg: 0.405977) \tsec/iter: 0.0202\n",
      "Train Epoch: 86 [169/169 (100%)]\tLoss: 0.284809 (avg: 0.342321) \tsec/iter: 0.0202\n",
      "Test set (epoch 86): Average loss: 1.1547, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 87 [64/169 (33%)]\tLoss: 0.341960 (avg: 0.341960) \tsec/iter: 0.0202\n",
      "Train Epoch: 87 [169/169 (100%)]\tLoss: 0.402751 (avg: 0.318011) \tsec/iter: 0.0202\n",
      "Test set (epoch 87): Average loss: 0.9257, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 88 [64/169 (33%)]\tLoss: 0.291305 (avg: 0.291305) \tsec/iter: 0.0282\n",
      "Train Epoch: 88 [169/169 (100%)]\tLoss: 0.311559 (avg: 0.332556) \tsec/iter: 0.0229\n",
      "Test set (epoch 88): Average loss: 0.8971, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 89 [64/169 (33%)]\tLoss: 0.453949 (avg: 0.453949) \tsec/iter: 0.0202\n",
      "Train Epoch: 89 [169/169 (100%)]\tLoss: 0.337911 (avg: 0.360105) \tsec/iter: 0.0202\n",
      "Test set (epoch 89): Average loss: 1.0108, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 90 [64/169 (33%)]\tLoss: 0.311472 (avg: 0.311472) \tsec/iter: 0.0202\n",
      "Train Epoch: 90 [169/169 (100%)]\tLoss: 0.472479 (avg: 0.337163) \tsec/iter: 0.0202\n",
      "Test set (epoch 90): Average loss: 1.1045, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 91 [64/169 (33%)]\tLoss: 0.412847 (avg: 0.412847) \tsec/iter: 0.0202\n",
      "Train Epoch: 91 [169/169 (100%)]\tLoss: 0.254105 (avg: 0.312816) \tsec/iter: 0.0202\n",
      "Test set (epoch 91): Average loss: 1.1211, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 92 [64/169 (33%)]\tLoss: 0.354494 (avg: 0.354494) \tsec/iter: 0.0202\n",
      "Train Epoch: 92 [169/169 (100%)]\tLoss: 0.348657 (avg: 0.311197) \tsec/iter: 0.0195\n",
      "Test set (epoch 92): Average loss: 1.1059, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 93 [64/169 (33%)]\tLoss: 0.262845 (avg: 0.262845) \tsec/iter: 0.0202\n",
      "Train Epoch: 93 [169/169 (100%)]\tLoss: 0.476778 (avg: 0.318118) \tsec/iter: 0.0202\n",
      "Test set (epoch 93): Average loss: 0.9990, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 94 [64/169 (33%)]\tLoss: 0.282988 (avg: 0.282988) \tsec/iter: 0.0202\n",
      "Train Epoch: 94 [169/169 (100%)]\tLoss: 0.297688 (avg: 0.333506) \tsec/iter: 0.0202\n",
      "Test set (epoch 94): Average loss: 1.2307, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 95 [64/169 (33%)]\tLoss: 0.369815 (avg: 0.369815) \tsec/iter: 0.0202\n",
      "Train Epoch: 95 [169/169 (100%)]\tLoss: 0.318958 (avg: 0.360984) \tsec/iter: 0.0202\n",
      "Test set (epoch 95): Average loss: 0.9122, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 96 [64/169 (33%)]\tLoss: 0.265173 (avg: 0.265173) \tsec/iter: 0.0222\n",
      "Train Epoch: 96 [169/169 (100%)]\tLoss: 0.380039 (avg: 0.334959) \tsec/iter: 0.0236\n",
      "Test set (epoch 96): Average loss: 1.3747, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 97 [64/169 (33%)]\tLoss: 0.356325 (avg: 0.356325) \tsec/iter: 0.0302\n",
      "Train Epoch: 97 [169/169 (100%)]\tLoss: 0.305011 (avg: 0.335830) \tsec/iter: 0.0235\n",
      "Test set (epoch 97): Average loss: 1.0296, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 98 [64/169 (33%)]\tLoss: 0.385443 (avg: 0.385443) \tsec/iter: 0.0202\n",
      "Train Epoch: 98 [169/169 (100%)]\tLoss: 0.225710 (avg: 0.317016) \tsec/iter: 0.0202\n",
      "Test set (epoch 98): Average loss: 1.1265, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 99 [64/169 (33%)]\tLoss: 0.302879 (avg: 0.302879) \tsec/iter: 0.0222\n",
      "Train Epoch: 99 [169/169 (100%)]\tLoss: 0.414763 (avg: 0.333747) \tsec/iter: 0.0208\n",
      "Test set (epoch 99): Average loss: 1.1232, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 100 [64/169 (33%)]\tLoss: 0.288696 (avg: 0.288696) \tsec/iter: 0.0202\n",
      "Train Epoch: 100 [169/169 (100%)]\tLoss: 0.348479 (avg: 0.323641) \tsec/iter: 0.0202\n",
      "Test set (epoch 100): Average loss: 1.1282, Accuracy: 8/19 (42.11%)\n",
      "\n",
      "Train Epoch: 101 [64/169 (33%)]\tLoss: 0.216843 (avg: 0.216843) \tsec/iter: 0.0302\n",
      "Train Epoch: 101 [169/169 (100%)]\tLoss: 0.344516 (avg: 0.309081) \tsec/iter: 0.0262\n",
      "Test set (epoch 101): Average loss: 1.0673, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 102 [64/169 (33%)]\tLoss: 0.305338 (avg: 0.305338) \tsec/iter: 0.0202\n",
      "Train Epoch: 102 [169/169 (100%)]\tLoss: 0.246732 (avg: 0.383849) \tsec/iter: 0.0202\n",
      "Test set (epoch 102): Average loss: 1.1383, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 103 [64/169 (33%)]\tLoss: 0.282851 (avg: 0.282851) \tsec/iter: 0.0202\n",
      "Train Epoch: 103 [169/169 (100%)]\tLoss: 0.219972 (avg: 0.305518) \tsec/iter: 0.0202\n",
      "Test set (epoch 103): Average loss: 1.0337, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 104 [64/169 (33%)]\tLoss: 0.307780 (avg: 0.307780) \tsec/iter: 0.0202\n",
      "Train Epoch: 104 [169/169 (100%)]\tLoss: 0.240631 (avg: 0.311387) \tsec/iter: 0.0168\n",
      "Test set (epoch 104): Average loss: 0.8666, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 105 [64/169 (33%)]\tLoss: 0.295788 (avg: 0.295788) \tsec/iter: 0.0203\n",
      "Train Epoch: 105 [169/169 (100%)]\tLoss: 0.380764 (avg: 0.304539) \tsec/iter: 0.0203\n",
      "Test set (epoch 105): Average loss: 0.9306, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 106 [64/169 (33%)]\tLoss: 0.287919 (avg: 0.287919) \tsec/iter: 0.0222\n",
      "Train Epoch: 106 [169/169 (100%)]\tLoss: 0.467737 (avg: 0.326459) \tsec/iter: 0.0209\n",
      "Test set (epoch 106): Average loss: 1.1883, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 107 [64/169 (33%)]\tLoss: 0.341209 (avg: 0.341209) \tsec/iter: 0.0223\n",
      "Train Epoch: 107 [169/169 (100%)]\tLoss: 0.321700 (avg: 0.326912) \tsec/iter: 0.0209\n",
      "Test set (epoch 107): Average loss: 1.0368, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 108 [64/169 (33%)]\tLoss: 0.251201 (avg: 0.251201) \tsec/iter: 0.0201\n",
      "Train Epoch: 108 [169/169 (100%)]\tLoss: 0.367080 (avg: 0.301675) \tsec/iter: 0.0202\n",
      "Test set (epoch 108): Average loss: 0.9048, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 109 [64/169 (33%)]\tLoss: 0.398385 (avg: 0.398385) \tsec/iter: 0.0201\n",
      "Train Epoch: 109 [169/169 (100%)]\tLoss: 0.240390 (avg: 0.306832) \tsec/iter: 0.0229\n",
      "Test set (epoch 109): Average loss: 0.8579, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 110 [64/169 (33%)]\tLoss: 0.255743 (avg: 0.255743) \tsec/iter: 0.0302\n",
      "Train Epoch: 110 [169/169 (100%)]\tLoss: 0.247009 (avg: 0.314949) \tsec/iter: 0.0269\n",
      "Test set (epoch 110): Average loss: 0.9839, Accuracy: 9/19 (47.37%)\n",
      "\n",
      "Train Epoch: 111 [64/169 (33%)]\tLoss: 0.309084 (avg: 0.309084) \tsec/iter: 0.0302\n",
      "Train Epoch: 111 [169/169 (100%)]\tLoss: 0.325347 (avg: 0.304982) \tsec/iter: 0.0262\n",
      "Test set (epoch 111): Average loss: 0.7936, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 112 [64/169 (33%)]\tLoss: 0.257952 (avg: 0.257952) \tsec/iter: 0.0302\n",
      "Train Epoch: 112 [169/169 (100%)]\tLoss: 0.370270 (avg: 0.294401) \tsec/iter: 0.0269\n",
      "Test set (epoch 112): Average loss: 0.7577, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 113 [64/169 (33%)]\tLoss: 0.325924 (avg: 0.325924) \tsec/iter: 0.0282\n",
      "Train Epoch: 113 [169/169 (100%)]\tLoss: 0.231845 (avg: 0.316229) \tsec/iter: 0.0262\n",
      "Test set (epoch 113): Average loss: 0.9821, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 114 [64/169 (33%)]\tLoss: 0.252403 (avg: 0.252403) \tsec/iter: 0.0282\n",
      "Train Epoch: 114 [169/169 (100%)]\tLoss: 0.390182 (avg: 0.305950) \tsec/iter: 0.0262\n",
      "Test set (epoch 114): Average loss: 0.9838, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 115 [64/169 (33%)]\tLoss: 0.297372 (avg: 0.297372) \tsec/iter: 0.0201\n",
      "Train Epoch: 115 [169/169 (100%)]\tLoss: 0.358851 (avg: 0.318053) \tsec/iter: 0.0235\n",
      "Test set (epoch 115): Average loss: 0.6442, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 116 [64/169 (33%)]\tLoss: 0.260486 (avg: 0.260486) \tsec/iter: 0.0202\n",
      "Train Epoch: 116 [169/169 (100%)]\tLoss: 0.234438 (avg: 0.294861) \tsec/iter: 0.0202\n",
      "Test set (epoch 116): Average loss: 0.8101, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 117 [64/169 (33%)]\tLoss: 0.176942 (avg: 0.176942) \tsec/iter: 0.0202\n",
      "Train Epoch: 117 [169/169 (100%)]\tLoss: 0.366536 (avg: 0.321147) \tsec/iter: 0.0208\n",
      "Test set (epoch 117): Average loss: 0.7286, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 118 [64/169 (33%)]\tLoss: 0.242727 (avg: 0.242727) \tsec/iter: 0.0283\n",
      "Train Epoch: 118 [169/169 (100%)]\tLoss: 0.293267 (avg: 0.295746) \tsec/iter: 0.0229\n",
      "Test set (epoch 118): Average loss: 0.7555, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 119 [64/169 (33%)]\tLoss: 0.327782 (avg: 0.327782) \tsec/iter: 0.0282\n",
      "Train Epoch: 119 [169/169 (100%)]\tLoss: 0.209428 (avg: 0.315017) \tsec/iter: 0.0202\n",
      "Test set (epoch 119): Average loss: 0.9910, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 120 [64/169 (33%)]\tLoss: 0.325133 (avg: 0.325133) \tsec/iter: 0.0202\n",
      "Train Epoch: 120 [169/169 (100%)]\tLoss: 0.226413 (avg: 0.299645) \tsec/iter: 0.0195\n",
      "Test set (epoch 120): Average loss: 0.6989, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 121 [64/169 (33%)]\tLoss: 0.470917 (avg: 0.470917) \tsec/iter: 0.0202\n",
      "Train Epoch: 121 [169/169 (100%)]\tLoss: 0.212582 (avg: 0.365841) \tsec/iter: 0.0202\n",
      "Test set (epoch 121): Average loss: 0.6560, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 122 [64/169 (33%)]\tLoss: 0.341381 (avg: 0.341381) \tsec/iter: 0.0201\n",
      "Train Epoch: 122 [169/169 (100%)]\tLoss: 0.292168 (avg: 0.310351) \tsec/iter: 0.0202\n",
      "Test set (epoch 122): Average loss: 0.7803, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 123 [64/169 (33%)]\tLoss: 0.339165 (avg: 0.339165) \tsec/iter: 0.0202\n",
      "Train Epoch: 123 [169/169 (100%)]\tLoss: 0.296002 (avg: 0.316452) \tsec/iter: 0.0202\n",
      "Test set (epoch 123): Average loss: 0.6747, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 124 [64/169 (33%)]\tLoss: 0.385094 (avg: 0.385094) \tsec/iter: 0.0223\n",
      "Train Epoch: 124 [169/169 (100%)]\tLoss: 0.373613 (avg: 0.314774) \tsec/iter: 0.0202\n",
      "Test set (epoch 124): Average loss: 0.7446, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 125 [64/169 (33%)]\tLoss: 0.316537 (avg: 0.316537) \tsec/iter: 0.0282\n",
      "Train Epoch: 125 [169/169 (100%)]\tLoss: 0.272432 (avg: 0.323045) \tsec/iter: 0.0202\n",
      "Test set (epoch 125): Average loss: 0.6949, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 126 [64/169 (33%)]\tLoss: 0.231963 (avg: 0.231963) \tsec/iter: 0.0202\n",
      "Train Epoch: 126 [169/169 (100%)]\tLoss: 0.322980 (avg: 0.363292) \tsec/iter: 0.0202\n",
      "Test set (epoch 126): Average loss: 0.8304, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 127 [64/169 (33%)]\tLoss: 0.351645 (avg: 0.351645) \tsec/iter: 0.0302\n",
      "Train Epoch: 127 [169/169 (100%)]\tLoss: 0.283916 (avg: 0.324953) \tsec/iter: 0.0202\n",
      "Test set (epoch 127): Average loss: 0.6517, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 128 [64/169 (33%)]\tLoss: 0.263664 (avg: 0.263664) \tsec/iter: 0.0202\n",
      "Train Epoch: 128 [169/169 (100%)]\tLoss: 0.349203 (avg: 0.309827) \tsec/iter: 0.0202\n",
      "Test set (epoch 128): Average loss: 0.7177, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 129 [64/169 (33%)]\tLoss: 0.285811 (avg: 0.285811) \tsec/iter: 0.0303\n",
      "Train Epoch: 129 [169/169 (100%)]\tLoss: 0.359237 (avg: 0.307510) \tsec/iter: 0.0262\n",
      "Test set (epoch 129): Average loss: 0.5890, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 130 [64/169 (33%)]\tLoss: 0.321054 (avg: 0.321054) \tsec/iter: 0.0281\n",
      "Train Epoch: 130 [169/169 (100%)]\tLoss: 0.423696 (avg: 0.318338) \tsec/iter: 0.0235\n",
      "Test set (epoch 130): Average loss: 0.6809, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 131 [64/169 (33%)]\tLoss: 0.456059 (avg: 0.456059) \tsec/iter: 0.0202\n",
      "Train Epoch: 131 [169/169 (100%)]\tLoss: 0.275609 (avg: 0.349908) \tsec/iter: 0.0202\n",
      "Test set (epoch 131): Average loss: 0.6080, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 132 [64/169 (33%)]\tLoss: 0.273987 (avg: 0.273987) \tsec/iter: 0.0202\n",
      "Train Epoch: 132 [169/169 (100%)]\tLoss: 0.310276 (avg: 0.289841) \tsec/iter: 0.0202\n",
      "Test set (epoch 132): Average loss: 0.6093, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 133 [64/169 (33%)]\tLoss: 0.397713 (avg: 0.397713) \tsec/iter: 0.0201\n",
      "Train Epoch: 133 [169/169 (100%)]\tLoss: 0.185051 (avg: 0.314569) \tsec/iter: 0.0202\n",
      "Test set (epoch 133): Average loss: 0.5730, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 134 [64/169 (33%)]\tLoss: 0.323122 (avg: 0.323122) \tsec/iter: 0.0202\n",
      "Train Epoch: 134 [169/169 (100%)]\tLoss: 0.192997 (avg: 0.307181) \tsec/iter: 0.0202\n",
      "Test set (epoch 134): Average loss: 0.6362, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 135 [64/169 (33%)]\tLoss: 0.278469 (avg: 0.278469) \tsec/iter: 0.0282\n",
      "Train Epoch: 135 [169/169 (100%)]\tLoss: 0.419529 (avg: 0.340925) \tsec/iter: 0.0228\n",
      "Test set (epoch 135): Average loss: 0.8014, Accuracy: 11/19 (57.89%)\n",
      "\n",
      "Train Epoch: 136 [64/169 (33%)]\tLoss: 0.336416 (avg: 0.336416) \tsec/iter: 0.0283\n",
      "Train Epoch: 136 [169/169 (100%)]\tLoss: 0.349157 (avg: 0.319359) \tsec/iter: 0.0202\n",
      "Test set (epoch 136): Average loss: 0.6098, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 137 [64/169 (33%)]\tLoss: 0.264336 (avg: 0.264336) \tsec/iter: 0.0202\n",
      "Train Epoch: 137 [169/169 (100%)]\tLoss: 0.359050 (avg: 0.322678) \tsec/iter: 0.0202\n",
      "Test set (epoch 137): Average loss: 0.6385, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 138 [64/169 (33%)]\tLoss: 0.344638 (avg: 0.344638) \tsec/iter: 0.0282\n",
      "Train Epoch: 138 [169/169 (100%)]\tLoss: 0.258726 (avg: 0.307534) \tsec/iter: 0.0232\n",
      "Test set (epoch 138): Average loss: 0.6419, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 139 [64/169 (33%)]\tLoss: 0.316936 (avg: 0.316936) \tsec/iter: 0.0313\n",
      "Train Epoch: 139 [169/169 (100%)]\tLoss: 0.302854 (avg: 0.329004) \tsec/iter: 0.0230\n",
      "Test set (epoch 139): Average loss: 0.5383, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 140 [64/169 (33%)]\tLoss: 0.260871 (avg: 0.260871) \tsec/iter: 0.0156\n",
      "Train Epoch: 140 [169/169 (100%)]\tLoss: 0.503773 (avg: 0.360106) \tsec/iter: 0.0208\n",
      "Test set (epoch 140): Average loss: 0.7618, Accuracy: 10/19 (52.63%)\n",
      "\n",
      "Train Epoch: 141 [64/169 (33%)]\tLoss: 0.287279 (avg: 0.287279) \tsec/iter: 0.0222\n",
      "Train Epoch: 141 [169/169 (100%)]\tLoss: 0.335429 (avg: 0.317801) \tsec/iter: 0.0230\n",
      "Test set (epoch 141): Average loss: 0.6121, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 142 [64/169 (33%)]\tLoss: 0.256893 (avg: 0.256893) \tsec/iter: 0.0156\n",
      "Train Epoch: 142 [169/169 (100%)]\tLoss: 0.227154 (avg: 0.286934) \tsec/iter: 0.0178\n",
      "Test set (epoch 142): Average loss: 0.5605, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 143 [64/169 (33%)]\tLoss: 0.227681 (avg: 0.227681) \tsec/iter: 0.0313\n",
      "Train Epoch: 143 [169/169 (100%)]\tLoss: 0.345426 (avg: 0.293912) \tsec/iter: 0.0230\n",
      "Test set (epoch 143): Average loss: 0.5914, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 144 [64/169 (33%)]\tLoss: 0.333512 (avg: 0.333512) \tsec/iter: 0.0313\n",
      "Train Epoch: 144 [169/169 (100%)]\tLoss: 0.232797 (avg: 0.270299) \tsec/iter: 0.0208\n",
      "Test set (epoch 144): Average loss: 0.5495, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 145 [64/169 (33%)]\tLoss: 0.362311 (avg: 0.362311) \tsec/iter: 0.0221\n",
      "Train Epoch: 145 [169/169 (100%)]\tLoss: 0.287985 (avg: 0.309478) \tsec/iter: 0.0230\n",
      "Test set (epoch 145): Average loss: 0.6163, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 146 [64/169 (33%)]\tLoss: 0.331470 (avg: 0.331470) \tsec/iter: 0.0313\n",
      "Train Epoch: 146 [169/169 (100%)]\tLoss: 0.271192 (avg: 0.305999) \tsec/iter: 0.0230\n",
      "Test set (epoch 146): Average loss: 0.5000, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 147 [64/169 (33%)]\tLoss: 0.294094 (avg: 0.294094) \tsec/iter: 0.0312\n",
      "Train Epoch: 147 [169/169 (100%)]\tLoss: 0.564970 (avg: 0.350857) \tsec/iter: 0.0208\n",
      "Test set (epoch 147): Average loss: 0.7173, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 148 [64/169 (33%)]\tLoss: 0.321599 (avg: 0.321599) \tsec/iter: 0.0222\n",
      "Train Epoch: 148 [169/169 (100%)]\tLoss: 0.309321 (avg: 0.302502) \tsec/iter: 0.0178\n",
      "Test set (epoch 148): Average loss: 0.6293, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 149 [64/169 (33%)]\tLoss: 0.318983 (avg: 0.318983) \tsec/iter: 0.0312\n",
      "Train Epoch: 149 [169/169 (100%)]\tLoss: 0.314756 (avg: 0.315808) \tsec/iter: 0.0230\n",
      "Test set (epoch 149): Average loss: 0.4769, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 150 [64/169 (33%)]\tLoss: 0.244282 (avg: 0.244282) \tsec/iter: 0.0156\n",
      "Train Epoch: 150 [169/169 (100%)]\tLoss: 0.285465 (avg: 0.266734) \tsec/iter: 0.0208\n",
      "Test set (epoch 150): Average loss: 0.5795, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 151 [64/169 (33%)]\tLoss: 0.305284 (avg: 0.305284) \tsec/iter: 0.0222\n",
      "Train Epoch: 151 [169/169 (100%)]\tLoss: 0.405925 (avg: 0.297719) \tsec/iter: 0.0178\n",
      "Test set (epoch 151): Average loss: 0.6466, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 152 [64/169 (33%)]\tLoss: 0.252087 (avg: 0.252087) \tsec/iter: 0.0156\n",
      "Train Epoch: 152 [169/169 (100%)]\tLoss: 0.392035 (avg: 0.298099) \tsec/iter: 0.0230\n",
      "Test set (epoch 152): Average loss: 0.4728, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 153 [64/169 (33%)]\tLoss: 0.258032 (avg: 0.258032) \tsec/iter: 0.0312\n",
      "Train Epoch: 153 [169/169 (100%)]\tLoss: 0.218720 (avg: 0.299638) \tsec/iter: 0.0230\n",
      "Test set (epoch 153): Average loss: 0.4898, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 154 [64/169 (33%)]\tLoss: 0.261183 (avg: 0.261183) \tsec/iter: 0.0313\n",
      "Train Epoch: 154 [169/169 (100%)]\tLoss: 0.270177 (avg: 0.284069) \tsec/iter: 0.0208\n",
      "Test set (epoch 154): Average loss: 0.5854, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 155 [64/169 (33%)]\tLoss: 0.320589 (avg: 0.320589) \tsec/iter: 0.0222\n",
      "Train Epoch: 155 [169/169 (100%)]\tLoss: 0.332097 (avg: 0.271242) \tsec/iter: 0.0178\n",
      "Test set (epoch 155): Average loss: 0.4804, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 156 [64/169 (33%)]\tLoss: 0.304108 (avg: 0.304108) \tsec/iter: 0.0156\n",
      "Train Epoch: 156 [169/169 (100%)]\tLoss: 0.254688 (avg: 0.273883) \tsec/iter: 0.0230\n",
      "Test set (epoch 156): Average loss: 0.5240, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 157 [64/169 (33%)]\tLoss: 0.404994 (avg: 0.404994) \tsec/iter: 0.0313\n",
      "Train Epoch: 157 [169/169 (100%)]\tLoss: 0.151900 (avg: 0.274673) \tsec/iter: 0.0260\n",
      "Test set (epoch 157): Average loss: 0.5812, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 158 [64/169 (33%)]\tLoss: 0.250653 (avg: 0.250653) \tsec/iter: 0.0156\n",
      "Train Epoch: 158 [169/169 (100%)]\tLoss: 0.289936 (avg: 0.276219) \tsec/iter: 0.0208\n",
      "Test set (epoch 158): Average loss: 0.5081, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 159 [64/169 (33%)]\tLoss: 0.256093 (avg: 0.256093) \tsec/iter: 0.0156\n",
      "Train Epoch: 159 [169/169 (100%)]\tLoss: 0.197571 (avg: 0.289237) \tsec/iter: 0.0230\n",
      "Test set (epoch 159): Average loss: 0.5735, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 160 [64/169 (33%)]\tLoss: 0.180158 (avg: 0.180158) \tsec/iter: 0.0156\n",
      "Train Epoch: 160 [169/169 (100%)]\tLoss: 0.385697 (avg: 0.290261) \tsec/iter: 0.0230\n",
      "Test set (epoch 160): Average loss: 0.7107, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 161 [64/169 (33%)]\tLoss: 0.347033 (avg: 0.347033) \tsec/iter: 0.0313\n",
      "Train Epoch: 161 [169/169 (100%)]\tLoss: 0.255666 (avg: 0.257837) \tsec/iter: 0.0260\n",
      "Test set (epoch 161): Average loss: 0.6675, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 162 [64/169 (33%)]\tLoss: 0.282581 (avg: 0.282581) \tsec/iter: 0.0156\n",
      "Train Epoch: 162 [169/169 (100%)]\tLoss: 0.289932 (avg: 0.329056) \tsec/iter: 0.0208\n",
      "Test set (epoch 162): Average loss: 0.7455, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 163 [64/169 (33%)]\tLoss: 0.272740 (avg: 0.272740) \tsec/iter: 0.0156\n",
      "Train Epoch: 163 [169/169 (100%)]\tLoss: 0.465421 (avg: 0.313089) \tsec/iter: 0.0230\n",
      "Test set (epoch 163): Average loss: 0.5116, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 164 [64/169 (33%)]\tLoss: 0.326625 (avg: 0.326625) \tsec/iter: 0.0156\n",
      "Train Epoch: 164 [169/169 (100%)]\tLoss: 0.309965 (avg: 0.326475) \tsec/iter: 0.0208\n",
      "Test set (epoch 164): Average loss: 0.5679, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 165 [64/169 (33%)]\tLoss: 0.348493 (avg: 0.348493) \tsec/iter: 0.0156\n",
      "Train Epoch: 165 [169/169 (100%)]\tLoss: 0.253652 (avg: 0.298054) \tsec/iter: 0.0208\n",
      "Test set (epoch 165): Average loss: 0.5419, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 166 [64/169 (33%)]\tLoss: 0.263474 (avg: 0.263474) \tsec/iter: 0.0313\n",
      "Train Epoch: 166 [169/169 (100%)]\tLoss: 0.451961 (avg: 0.302252) \tsec/iter: 0.0230\n",
      "Test set (epoch 166): Average loss: 0.6363, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 167 [64/169 (33%)]\tLoss: 0.292896 (avg: 0.292896) \tsec/iter: 0.0156\n",
      "Train Epoch: 167 [169/169 (100%)]\tLoss: 0.202115 (avg: 0.300660) \tsec/iter: 0.0208\n",
      "Test set (epoch 167): Average loss: 0.5001, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 168 [64/169 (33%)]\tLoss: 0.283065 (avg: 0.283065) \tsec/iter: 0.0157\n",
      "Train Epoch: 168 [169/169 (100%)]\tLoss: 0.306753 (avg: 0.274423) \tsec/iter: 0.0208\n",
      "Test set (epoch 168): Average loss: 0.5586, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 169 [64/169 (33%)]\tLoss: 0.237889 (avg: 0.237889) \tsec/iter: 0.0312\n",
      "Train Epoch: 169 [169/169 (100%)]\tLoss: 0.277784 (avg: 0.277023) \tsec/iter: 0.0230\n",
      "Test set (epoch 169): Average loss: 0.5407, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 170 [64/169 (33%)]\tLoss: 0.280958 (avg: 0.280958) \tsec/iter: 0.0156\n",
      "Train Epoch: 170 [169/169 (100%)]\tLoss: 0.317073 (avg: 0.297771) \tsec/iter: 0.0229\n",
      "Test set (epoch 170): Average loss: 0.5025, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 171 [64/169 (33%)]\tLoss: 0.281791 (avg: 0.281791) \tsec/iter: 0.0313\n",
      "Train Epoch: 171 [169/169 (100%)]\tLoss: 0.333656 (avg: 0.285947) \tsec/iter: 0.0208\n",
      "Test set (epoch 171): Average loss: 0.4862, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 172 [64/169 (33%)]\tLoss: 0.373639 (avg: 0.373639) \tsec/iter: 0.0221\n",
      "Train Epoch: 172 [169/169 (100%)]\tLoss: 0.302718 (avg: 0.330884) \tsec/iter: 0.0178\n",
      "Test set (epoch 172): Average loss: 0.4875, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 173 [64/169 (33%)]\tLoss: 0.246195 (avg: 0.246195) \tsec/iter: 0.0156\n",
      "Train Epoch: 173 [169/169 (100%)]\tLoss: 0.400483 (avg: 0.299737) \tsec/iter: 0.0178\n",
      "Test set (epoch 173): Average loss: 0.4720, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 174 [64/169 (33%)]\tLoss: 0.279530 (avg: 0.279530) \tsec/iter: 0.0156\n",
      "Train Epoch: 174 [169/169 (100%)]\tLoss: 0.243107 (avg: 0.274872) \tsec/iter: 0.0208\n",
      "Test set (epoch 174): Average loss: 0.4417, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 175 [64/169 (33%)]\tLoss: 0.208197 (avg: 0.208197) \tsec/iter: 0.0221\n",
      "Train Epoch: 175 [169/169 (100%)]\tLoss: 0.325093 (avg: 0.296871) \tsec/iter: 0.0230\n",
      "Test set (epoch 175): Average loss: 0.4927, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 176 [64/169 (33%)]\tLoss: 0.245712 (avg: 0.245712) \tsec/iter: 0.0313\n",
      "Train Epoch: 176 [169/169 (100%)]\tLoss: 0.392203 (avg: 0.317932) \tsec/iter: 0.0230\n",
      "Test set (epoch 176): Average loss: 0.5166, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 177 [64/169 (33%)]\tLoss: 0.266889 (avg: 0.266889) \tsec/iter: 0.0156\n",
      "Train Epoch: 177 [169/169 (100%)]\tLoss: 0.201541 (avg: 0.262784) \tsec/iter: 0.0208\n",
      "Test set (epoch 177): Average loss: 0.4430, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 178 [64/169 (33%)]\tLoss: 0.273569 (avg: 0.273569) \tsec/iter: 0.0156\n",
      "Train Epoch: 178 [169/169 (100%)]\tLoss: 0.289720 (avg: 0.283809) \tsec/iter: 0.0208\n",
      "Test set (epoch 178): Average loss: 0.4871, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 179 [64/169 (33%)]\tLoss: 0.300679 (avg: 0.300679) \tsec/iter: 0.0156\n",
      "Train Epoch: 179 [169/169 (100%)]\tLoss: 0.215754 (avg: 0.262968) \tsec/iter: 0.0230\n",
      "Test set (epoch 179): Average loss: 0.5197, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 180 [64/169 (33%)]\tLoss: 0.344108 (avg: 0.344108) \tsec/iter: 0.0156\n",
      "Train Epoch: 180 [169/169 (100%)]\tLoss: 0.188661 (avg: 0.299130) \tsec/iter: 0.0230\n",
      "Test set (epoch 180): Average loss: 0.4482, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 181 [64/169 (33%)]\tLoss: 0.194161 (avg: 0.194161) \tsec/iter: 0.0156\n",
      "Train Epoch: 181 [169/169 (100%)]\tLoss: 0.281042 (avg: 0.270395) \tsec/iter: 0.0208\n",
      "Test set (epoch 181): Average loss: 0.5053, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 182 [64/169 (33%)]\tLoss: 0.320825 (avg: 0.320825) \tsec/iter: 0.0313\n",
      "Train Epoch: 182 [169/169 (100%)]\tLoss: 0.351088 (avg: 0.280565) \tsec/iter: 0.0230\n",
      "Test set (epoch 182): Average loss: 0.5059, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 183 [64/169 (33%)]\tLoss: 0.158982 (avg: 0.158982) \tsec/iter: 0.0156\n",
      "Train Epoch: 183 [169/169 (100%)]\tLoss: 0.279459 (avg: 0.261255) \tsec/iter: 0.0178\n",
      "Test set (epoch 183): Average loss: 0.4622, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 184 [64/169 (33%)]\tLoss: 0.215405 (avg: 0.215405) \tsec/iter: 0.0313\n",
      "Train Epoch: 184 [169/169 (100%)]\tLoss: 0.296627 (avg: 0.242567) \tsec/iter: 0.0260\n",
      "Test set (epoch 184): Average loss: 0.5479, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 185 [64/169 (33%)]\tLoss: 0.190096 (avg: 0.190096) \tsec/iter: 0.0378\n",
      "Train Epoch: 185 [169/169 (100%)]\tLoss: 0.331661 (avg: 0.266055) \tsec/iter: 0.0282\n",
      "Test set (epoch 185): Average loss: 0.4373, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 186 [64/169 (33%)]\tLoss: 0.318895 (avg: 0.318895) \tsec/iter: 0.0221\n",
      "Train Epoch: 186 [169/169 (100%)]\tLoss: 0.216476 (avg: 0.277529) \tsec/iter: 0.0282\n",
      "Test set (epoch 186): Average loss: 0.5013, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 187 [64/169 (33%)]\tLoss: 0.242012 (avg: 0.242012) \tsec/iter: 0.0378\n",
      "Train Epoch: 187 [169/169 (100%)]\tLoss: 0.342166 (avg: 0.278154) \tsec/iter: 0.0282\n",
      "Test set (epoch 187): Average loss: 0.5186, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 188 [64/169 (33%)]\tLoss: 0.315931 (avg: 0.315931) \tsec/iter: 0.0313\n",
      "Train Epoch: 188 [169/169 (100%)]\tLoss: 0.244587 (avg: 0.271257) \tsec/iter: 0.0230\n",
      "Test set (epoch 188): Average loss: 0.7270, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 189 [64/169 (33%)]\tLoss: 0.359736 (avg: 0.359736) \tsec/iter: 0.0156\n",
      "Train Epoch: 189 [169/169 (100%)]\tLoss: 0.275237 (avg: 0.308422) \tsec/iter: 0.0230\n",
      "Test set (epoch 189): Average loss: 0.4008, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 190 [64/169 (33%)]\tLoss: 0.268031 (avg: 0.268031) \tsec/iter: 0.0313\n",
      "Train Epoch: 190 [169/169 (100%)]\tLoss: 0.362145 (avg: 0.308490) \tsec/iter: 0.0260\n",
      "Test set (epoch 190): Average loss: 0.5378, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 191 [64/169 (33%)]\tLoss: 0.245588 (avg: 0.245588) \tsec/iter: 0.0156\n",
      "Train Epoch: 191 [169/169 (100%)]\tLoss: 0.339711 (avg: 0.263463) \tsec/iter: 0.0208\n",
      "Test set (epoch 191): Average loss: 0.4464, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 192 [64/169 (33%)]\tLoss: 0.304509 (avg: 0.304509) \tsec/iter: 0.0221\n",
      "Train Epoch: 192 [169/169 (100%)]\tLoss: 0.198949 (avg: 0.302563) \tsec/iter: 0.0178\n",
      "Test set (epoch 192): Average loss: 0.5286, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 193 [64/169 (33%)]\tLoss: 0.315173 (avg: 0.315173) \tsec/iter: 0.0156\n",
      "Train Epoch: 193 [169/169 (100%)]\tLoss: 0.324279 (avg: 0.269835) \tsec/iter: 0.0178\n",
      "Test set (epoch 193): Average loss: 0.4689, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 194 [64/169 (33%)]\tLoss: 0.336269 (avg: 0.336269) \tsec/iter: 0.0156\n",
      "Train Epoch: 194 [169/169 (100%)]\tLoss: 0.266003 (avg: 0.279365) \tsec/iter: 0.0208\n",
      "Test set (epoch 194): Average loss: 0.5058, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 195 [64/169 (33%)]\tLoss: 0.180947 (avg: 0.180947) \tsec/iter: 0.0221\n",
      "Train Epoch: 195 [169/169 (100%)]\tLoss: 0.192975 (avg: 0.259365) \tsec/iter: 0.0230\n",
      "Test set (epoch 195): Average loss: 0.5030, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 196 [64/169 (33%)]\tLoss: 0.322641 (avg: 0.322641) \tsec/iter: 0.0312\n",
      "Train Epoch: 196 [169/169 (100%)]\tLoss: 0.267561 (avg: 0.262202) \tsec/iter: 0.0230\n",
      "Test set (epoch 196): Average loss: 0.5325, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 197 [64/169 (33%)]\tLoss: 0.258498 (avg: 0.258498) \tsec/iter: 0.0313\n",
      "Train Epoch: 197 [169/169 (100%)]\tLoss: 0.191374 (avg: 0.246486) \tsec/iter: 0.0208\n",
      "Test set (epoch 197): Average loss: 0.4369, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 198 [64/169 (33%)]\tLoss: 0.295234 (avg: 0.295234) \tsec/iter: 0.0378\n",
      "Train Epoch: 198 [169/169 (100%)]\tLoss: 0.187127 (avg: 0.242360) \tsec/iter: 0.0282\n",
      "Test set (epoch 198): Average loss: 0.4644, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 199 [64/169 (33%)]\tLoss: 0.211628 (avg: 0.211628) \tsec/iter: 0.0221\n",
      "Train Epoch: 199 [169/169 (100%)]\tLoss: 0.464632 (avg: 0.306398) \tsec/iter: 0.0230\n",
      "Test set (epoch 199): Average loss: 0.4726, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 200 [64/169 (33%)]\tLoss: 0.394750 (avg: 0.394750) \tsec/iter: 0.0313\n",
      "Train Epoch: 200 [169/169 (100%)]\tLoss: 0.169089 (avg: 0.295049) \tsec/iter: 0.0230\n",
      "Test set (epoch 200): Average loss: 0.5444, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 201 [64/169 (33%)]\tLoss: 0.304997 (avg: 0.304997) \tsec/iter: 0.0313\n",
      "Train Epoch: 201 [169/169 (100%)]\tLoss: 0.251899 (avg: 0.250902) \tsec/iter: 0.0208\n",
      "Test set (epoch 201): Average loss: 0.5932, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 202 [64/169 (33%)]\tLoss: 0.289189 (avg: 0.289189) \tsec/iter: 0.0222\n",
      "Train Epoch: 202 [169/169 (100%)]\tLoss: 0.244041 (avg: 0.251818) \tsec/iter: 0.0178\n",
      "Test set (epoch 202): Average loss: 0.5488, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 203 [64/169 (33%)]\tLoss: 0.284941 (avg: 0.284941) \tsec/iter: 0.0313\n",
      "Train Epoch: 203 [169/169 (100%)]\tLoss: 0.262203 (avg: 0.267471) \tsec/iter: 0.0230\n",
      "Test set (epoch 203): Average loss: 0.4786, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 204 [64/169 (33%)]\tLoss: 0.235577 (avg: 0.235577) \tsec/iter: 0.0156\n",
      "Train Epoch: 204 [169/169 (100%)]\tLoss: 0.377818 (avg: 0.290089) \tsec/iter: 0.0230\n",
      "Test set (epoch 204): Average loss: 0.6104, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 205 [64/169 (33%)]\tLoss: 0.173012 (avg: 0.173012) \tsec/iter: 0.0156\n",
      "Train Epoch: 205 [169/169 (100%)]\tLoss: 0.227706 (avg: 0.260052) \tsec/iter: 0.0208\n",
      "Test set (epoch 205): Average loss: 0.4176, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 206 [64/169 (33%)]\tLoss: 0.236333 (avg: 0.236333) \tsec/iter: 0.0221\n",
      "Train Epoch: 206 [169/169 (100%)]\tLoss: 0.228472 (avg: 0.249506) \tsec/iter: 0.0178\n",
      "Test set (epoch 206): Average loss: 0.5617, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 207 [64/169 (33%)]\tLoss: 0.288313 (avg: 0.288313) \tsec/iter: 0.0156\n",
      "Train Epoch: 207 [169/169 (100%)]\tLoss: 0.287916 (avg: 0.278021) \tsec/iter: 0.0178\n",
      "Test set (epoch 207): Average loss: 0.4838, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 208 [64/169 (33%)]\tLoss: 0.294295 (avg: 0.294295) \tsec/iter: 0.0156\n",
      "Train Epoch: 208 [169/169 (100%)]\tLoss: 0.185456 (avg: 0.254544) \tsec/iter: 0.0208\n",
      "Test set (epoch 208): Average loss: 0.4812, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 209 [64/169 (33%)]\tLoss: 0.297454 (avg: 0.297454) \tsec/iter: 0.0156\n",
      "Train Epoch: 209 [169/169 (100%)]\tLoss: 0.447827 (avg: 0.326964) \tsec/iter: 0.0208\n",
      "Test set (epoch 209): Average loss: 0.5578, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 210 [64/169 (33%)]\tLoss: 0.272011 (avg: 0.272011) \tsec/iter: 0.0378\n",
      "Train Epoch: 210 [169/169 (100%)]\tLoss: 0.173883 (avg: 0.265038) \tsec/iter: 0.0230\n",
      "Test set (epoch 210): Average loss: 0.4865, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 211 [64/169 (33%)]\tLoss: 0.225633 (avg: 0.225633) \tsec/iter: 0.0312\n",
      "Train Epoch: 211 [169/169 (100%)]\tLoss: 0.222886 (avg: 0.250714) \tsec/iter: 0.0230\n",
      "Test set (epoch 211): Average loss: 0.4040, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 212 [64/169 (33%)]\tLoss: 0.245850 (avg: 0.245850) \tsec/iter: 0.0312\n",
      "Train Epoch: 212 [169/169 (100%)]\tLoss: 0.372484 (avg: 0.261187) \tsec/iter: 0.0260\n",
      "Test set (epoch 212): Average loss: 0.4864, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 213 [64/169 (33%)]\tLoss: 0.199387 (avg: 0.199387) \tsec/iter: 0.0156\n",
      "Train Epoch: 213 [169/169 (100%)]\tLoss: 0.343647 (avg: 0.258657) \tsec/iter: 0.0208\n",
      "Test set (epoch 213): Average loss: 0.5716, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 214 [64/169 (33%)]\tLoss: 0.361434 (avg: 0.361434) \tsec/iter: 0.0378\n",
      "Train Epoch: 214 [169/169 (100%)]\tLoss: 0.347033 (avg: 0.275582) \tsec/iter: 0.0282\n",
      "Test set (epoch 214): Average loss: 0.5234, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 215 [64/169 (33%)]\tLoss: 0.325114 (avg: 0.325114) \tsec/iter: 0.0313\n",
      "Train Epoch: 215 [169/169 (100%)]\tLoss: 0.257717 (avg: 0.290889) \tsec/iter: 0.0178\n",
      "Test set (epoch 215): Average loss: 0.3845, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 216 [64/169 (33%)]\tLoss: 0.258036 (avg: 0.258036) \tsec/iter: 0.0156\n",
      "Train Epoch: 216 [169/169 (100%)]\tLoss: 0.175152 (avg: 0.271024) \tsec/iter: 0.0208\n",
      "Test set (epoch 216): Average loss: 0.5343, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 217 [64/169 (33%)]\tLoss: 0.294247 (avg: 0.294247) \tsec/iter: 0.0222\n",
      "Train Epoch: 217 [169/169 (100%)]\tLoss: 0.305580 (avg: 0.274341) \tsec/iter: 0.0178\n",
      "Test set (epoch 217): Average loss: 0.5326, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 218 [64/169 (33%)]\tLoss: 0.283254 (avg: 0.283254) \tsec/iter: 0.0313\n",
      "Train Epoch: 218 [169/169 (100%)]\tLoss: 0.172148 (avg: 0.251586) \tsec/iter: 0.0230\n",
      "Test set (epoch 218): Average loss: 0.5389, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 219 [64/169 (33%)]\tLoss: 0.348052 (avg: 0.348052) \tsec/iter: 0.0312\n",
      "Train Epoch: 219 [169/169 (100%)]\tLoss: 0.147788 (avg: 0.275193) \tsec/iter: 0.0260\n",
      "Test set (epoch 219): Average loss: 0.4756, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 220 [64/169 (33%)]\tLoss: 0.358098 (avg: 0.358098) \tsec/iter: 0.0156\n",
      "Train Epoch: 220 [169/169 (100%)]\tLoss: 0.216706 (avg: 0.281326) \tsec/iter: 0.0208\n",
      "Test set (epoch 220): Average loss: 0.6062, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 221 [64/169 (33%)]\tLoss: 0.154518 (avg: 0.154518) \tsec/iter: 0.0312\n",
      "Train Epoch: 221 [169/169 (100%)]\tLoss: 0.344630 (avg: 0.262048) \tsec/iter: 0.0230\n",
      "Test set (epoch 221): Average loss: 0.5091, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 222 [64/169 (33%)]\tLoss: 0.218293 (avg: 0.218293) \tsec/iter: 0.0313\n",
      "Train Epoch: 222 [169/169 (100%)]\tLoss: 0.346766 (avg: 0.269895) \tsec/iter: 0.0230\n",
      "Test set (epoch 222): Average loss: 0.6015, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 223 [64/169 (33%)]\tLoss: 0.361075 (avg: 0.361075) \tsec/iter: 0.0313\n",
      "Train Epoch: 223 [169/169 (100%)]\tLoss: 0.212785 (avg: 0.267013) \tsec/iter: 0.0208\n",
      "Test set (epoch 223): Average loss: 0.4699, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 224 [64/169 (33%)]\tLoss: 0.304054 (avg: 0.304054) \tsec/iter: 0.0221\n",
      "Train Epoch: 224 [169/169 (100%)]\tLoss: 0.285312 (avg: 0.262357) \tsec/iter: 0.0178\n",
      "Test set (epoch 224): Average loss: 0.5789, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 225 [64/169 (33%)]\tLoss: 0.350879 (avg: 0.350879) \tsec/iter: 0.0313\n",
      "Train Epoch: 225 [169/169 (100%)]\tLoss: 0.220936 (avg: 0.254791) \tsec/iter: 0.0237\n",
      "Test set (epoch 225): Average loss: 0.4263, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 226 [64/169 (33%)]\tLoss: 0.249683 (avg: 0.249683) \tsec/iter: 0.0282\n",
      "Train Epoch: 226 [169/169 (100%)]\tLoss: 0.280235 (avg: 0.298976) \tsec/iter: 0.0229\n",
      "Test set (epoch 226): Average loss: 0.5297, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 227 [64/169 (33%)]\tLoss: 0.251901 (avg: 0.251901) \tsec/iter: 0.0201\n",
      "Train Epoch: 227 [169/169 (100%)]\tLoss: 0.270566 (avg: 0.234329) \tsec/iter: 0.0202\n",
      "Test set (epoch 227): Average loss: 0.4422, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 228 [64/169 (33%)]\tLoss: 0.270522 (avg: 0.270522) \tsec/iter: 0.0202\n",
      "Train Epoch: 228 [169/169 (100%)]\tLoss: 0.259394 (avg: 0.239713) \tsec/iter: 0.0209\n",
      "Test set (epoch 228): Average loss: 0.4939, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 229 [64/169 (33%)]\tLoss: 0.199016 (avg: 0.199016) \tsec/iter: 0.0222\n",
      "Train Epoch: 229 [169/169 (100%)]\tLoss: 0.221568 (avg: 0.253125) \tsec/iter: 0.0208\n",
      "Test set (epoch 229): Average loss: 0.4882, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 230 [64/169 (33%)]\tLoss: 0.334735 (avg: 0.334735) \tsec/iter: 0.0203\n",
      "Train Epoch: 230 [169/169 (100%)]\tLoss: 0.216140 (avg: 0.281396) \tsec/iter: 0.0229\n",
      "Test set (epoch 230): Average loss: 0.4860, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 231 [64/169 (33%)]\tLoss: 0.224780 (avg: 0.224780) \tsec/iter: 0.0202\n",
      "Train Epoch: 231 [169/169 (100%)]\tLoss: 0.280176 (avg: 0.256950) \tsec/iter: 0.0202\n",
      "Test set (epoch 231): Average loss: 0.4677, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 232 [64/169 (33%)]\tLoss: 0.287092 (avg: 0.287092) \tsec/iter: 0.0202\n",
      "Train Epoch: 232 [169/169 (100%)]\tLoss: 0.179071 (avg: 0.259608) \tsec/iter: 0.0202\n",
      "Test set (epoch 232): Average loss: 0.5451, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 233 [64/169 (33%)]\tLoss: 0.185604 (avg: 0.185604) \tsec/iter: 0.0222\n",
      "Train Epoch: 233 [169/169 (100%)]\tLoss: 0.433961 (avg: 0.278778) \tsec/iter: 0.0208\n",
      "Test set (epoch 233): Average loss: 0.5286, Accuracy: 12/19 (63.16%)\n",
      "\n",
      "Train Epoch: 234 [64/169 (33%)]\tLoss: 0.241330 (avg: 0.241330) \tsec/iter: 0.0282\n",
      "Train Epoch: 234 [169/169 (100%)]\tLoss: 0.232093 (avg: 0.243090) \tsec/iter: 0.0235\n",
      "Test set (epoch 234): Average loss: 0.4337, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 235 [64/169 (33%)]\tLoss: 0.275015 (avg: 0.275015) \tsec/iter: 0.0222\n",
      "Train Epoch: 235 [169/169 (100%)]\tLoss: 0.205706 (avg: 0.253069) \tsec/iter: 0.0235\n",
      "Test set (epoch 235): Average loss: 0.5683, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 236 [64/169 (33%)]\tLoss: 0.261744 (avg: 0.261744) \tsec/iter: 0.0202\n",
      "Train Epoch: 236 [169/169 (100%)]\tLoss: 0.372312 (avg: 0.283856) \tsec/iter: 0.0202\n",
      "Test set (epoch 236): Average loss: 0.4868, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 237 [64/169 (33%)]\tLoss: 0.184843 (avg: 0.184843) \tsec/iter: 0.0203\n",
      "Train Epoch: 237 [169/169 (100%)]\tLoss: 0.298879 (avg: 0.255305) \tsec/iter: 0.0202\n",
      "Test set (epoch 237): Average loss: 0.4128, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 238 [64/169 (33%)]\tLoss: 0.257137 (avg: 0.257137) \tsec/iter: 0.0222\n",
      "Train Epoch: 238 [169/169 (100%)]\tLoss: 0.329842 (avg: 0.261586) \tsec/iter: 0.0209\n",
      "Test set (epoch 238): Average loss: 0.4495, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 239 [64/169 (33%)]\tLoss: 0.171839 (avg: 0.171839) \tsec/iter: 0.0202\n",
      "Train Epoch: 239 [169/169 (100%)]\tLoss: 0.427270 (avg: 0.262640) \tsec/iter: 0.0175\n",
      "Test set (epoch 239): Average loss: 0.4421, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 240 [64/169 (33%)]\tLoss: 0.266393 (avg: 0.266393) \tsec/iter: 0.0202\n",
      "Train Epoch: 240 [169/169 (100%)]\tLoss: 0.275048 (avg: 0.237954) \tsec/iter: 0.0235\n",
      "Test set (epoch 240): Average loss: 0.4187, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 241 [64/169 (33%)]\tLoss: 0.342491 (avg: 0.342491) \tsec/iter: 0.0222\n",
      "Train Epoch: 241 [169/169 (100%)]\tLoss: 0.318648 (avg: 0.280956) \tsec/iter: 0.0175\n",
      "Test set (epoch 241): Average loss: 0.3746, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 242 [64/169 (33%)]\tLoss: 0.292746 (avg: 0.292746) \tsec/iter: 0.0202\n",
      "Train Epoch: 242 [169/169 (100%)]\tLoss: 0.188787 (avg: 0.263033) \tsec/iter: 0.0235\n",
      "Test set (epoch 242): Average loss: 0.4631, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 243 [64/169 (33%)]\tLoss: 0.240466 (avg: 0.240466) \tsec/iter: 0.0222\n",
      "Train Epoch: 243 [169/169 (100%)]\tLoss: 0.214479 (avg: 0.256360) \tsec/iter: 0.0202\n",
      "Test set (epoch 243): Average loss: 0.4626, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 244 [64/169 (33%)]\tLoss: 0.377084 (avg: 0.377084) \tsec/iter: 0.0283\n",
      "Train Epoch: 244 [169/169 (100%)]\tLoss: 0.192022 (avg: 0.272091) \tsec/iter: 0.0236\n",
      "Test set (epoch 244): Average loss: 0.4597, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 245 [64/169 (33%)]\tLoss: 0.175297 (avg: 0.175297) \tsec/iter: 0.0282\n",
      "Train Epoch: 245 [169/169 (100%)]\tLoss: 0.224046 (avg: 0.219060) \tsec/iter: 0.0202\n",
      "Test set (epoch 245): Average loss: 0.3753, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 246 [64/169 (33%)]\tLoss: 0.283380 (avg: 0.283380) \tsec/iter: 0.0202\n",
      "Train Epoch: 246 [169/169 (100%)]\tLoss: 0.149470 (avg: 0.269349) \tsec/iter: 0.0202\n",
      "Test set (epoch 246): Average loss: 0.4056, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 247 [64/169 (33%)]\tLoss: 0.203773 (avg: 0.203773) \tsec/iter: 0.0223\n",
      "Train Epoch: 247 [169/169 (100%)]\tLoss: 0.217831 (avg: 0.234412) \tsec/iter: 0.0209\n",
      "Test set (epoch 247): Average loss: 0.5113, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 248 [64/169 (33%)]\tLoss: 0.237505 (avg: 0.237505) \tsec/iter: 0.0303\n",
      "Train Epoch: 248 [169/169 (100%)]\tLoss: 0.262089 (avg: 0.245056) \tsec/iter: 0.0235\n",
      "Test set (epoch 248): Average loss: 0.4609, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 249 [64/169 (33%)]\tLoss: 0.255031 (avg: 0.255031) \tsec/iter: 0.0203\n",
      "Train Epoch: 249 [169/169 (100%)]\tLoss: 0.321002 (avg: 0.284265) \tsec/iter: 0.0202\n",
      "Test set (epoch 249): Average loss: 0.4845, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 250 [64/169 (33%)]\tLoss: 0.174616 (avg: 0.174616) \tsec/iter: 0.0202\n",
      "Train Epoch: 250 [169/169 (100%)]\tLoss: 0.269116 (avg: 0.230291) \tsec/iter: 0.0262\n",
      "Test set (epoch 250): Average loss: 0.5753, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 251 [64/169 (33%)]\tLoss: 0.281582 (avg: 0.281582) \tsec/iter: 0.0202\n",
      "Train Epoch: 251 [169/169 (100%)]\tLoss: 0.128105 (avg: 0.259588) \tsec/iter: 0.0235\n",
      "Test set (epoch 251): Average loss: 0.5162, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 252 [64/169 (33%)]\tLoss: 0.325484 (avg: 0.325484) \tsec/iter: 0.0222\n",
      "Train Epoch: 252 [169/169 (100%)]\tLoss: 0.256430 (avg: 0.255748) \tsec/iter: 0.0208\n",
      "Test set (epoch 252): Average loss: 0.5475, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 253 [64/169 (33%)]\tLoss: 0.228117 (avg: 0.228117) \tsec/iter: 0.0282\n",
      "Train Epoch: 253 [169/169 (100%)]\tLoss: 0.213040 (avg: 0.266062) \tsec/iter: 0.0235\n",
      "Test set (epoch 253): Average loss: 0.5783, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 254 [64/169 (33%)]\tLoss: 0.203033 (avg: 0.203033) \tsec/iter: 0.0302\n",
      "Train Epoch: 254 [169/169 (100%)]\tLoss: 0.209986 (avg: 0.210276) \tsec/iter: 0.0235\n",
      "Test set (epoch 254): Average loss: 0.5658, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 255 [64/169 (33%)]\tLoss: 0.307537 (avg: 0.307537) \tsec/iter: 0.0282\n",
      "Train Epoch: 255 [169/169 (100%)]\tLoss: 0.251573 (avg: 0.268336) \tsec/iter: 0.0235\n",
      "Test set (epoch 255): Average loss: 0.4418, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 256 [64/169 (33%)]\tLoss: 0.323762 (avg: 0.323762) \tsec/iter: 0.0202\n",
      "Train Epoch: 256 [169/169 (100%)]\tLoss: 0.272640 (avg: 0.251042) \tsec/iter: 0.0202\n",
      "Test set (epoch 256): Average loss: 0.4954, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 257 [64/169 (33%)]\tLoss: 0.339956 (avg: 0.339956) \tsec/iter: 0.0201\n",
      "Train Epoch: 257 [169/169 (100%)]\tLoss: 0.235425 (avg: 0.291921) \tsec/iter: 0.0195\n",
      "Test set (epoch 257): Average loss: 0.4577, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 258 [64/169 (33%)]\tLoss: 0.276206 (avg: 0.276206) \tsec/iter: 0.0282\n",
      "Train Epoch: 258 [169/169 (100%)]\tLoss: 0.256231 (avg: 0.258110) \tsec/iter: 0.0235\n",
      "Test set (epoch 258): Average loss: 0.5834, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 259 [64/169 (33%)]\tLoss: 0.260767 (avg: 0.260767) \tsec/iter: 0.0303\n",
      "Train Epoch: 259 [169/169 (100%)]\tLoss: 0.205267 (avg: 0.259677) \tsec/iter: 0.0235\n",
      "Test set (epoch 259): Average loss: 0.5270, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 260 [64/169 (33%)]\tLoss: 0.174381 (avg: 0.174381) \tsec/iter: 0.0302\n",
      "Train Epoch: 260 [169/169 (100%)]\tLoss: 0.385044 (avg: 0.237917) \tsec/iter: 0.0295\n",
      "Test set (epoch 260): Average loss: 0.5191, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 261 [64/169 (33%)]\tLoss: 0.228473 (avg: 0.228473) \tsec/iter: 0.0222\n",
      "Train Epoch: 261 [169/169 (100%)]\tLoss: 0.163391 (avg: 0.209732) \tsec/iter: 0.0235\n",
      "Test set (epoch 261): Average loss: 0.4776, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 262 [64/169 (33%)]\tLoss: 0.297122 (avg: 0.297122) \tsec/iter: 0.0202\n",
      "Train Epoch: 262 [169/169 (100%)]\tLoss: 0.259117 (avg: 0.249249) \tsec/iter: 0.0235\n",
      "Test set (epoch 262): Average loss: 0.5490, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 263 [64/169 (33%)]\tLoss: 0.247459 (avg: 0.247459) \tsec/iter: 0.0282\n",
      "Train Epoch: 263 [169/169 (100%)]\tLoss: 0.316467 (avg: 0.231145) \tsec/iter: 0.0202\n",
      "Test set (epoch 263): Average loss: 0.5242, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 264 [64/169 (33%)]\tLoss: 0.212574 (avg: 0.212574) \tsec/iter: 0.0202\n",
      "Train Epoch: 264 [169/169 (100%)]\tLoss: 0.225907 (avg: 0.237700) \tsec/iter: 0.0229\n",
      "Test set (epoch 264): Average loss: 0.4582, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 265 [64/169 (33%)]\tLoss: 0.225151 (avg: 0.225151) \tsec/iter: 0.0302\n",
      "Train Epoch: 265 [169/169 (100%)]\tLoss: 0.296915 (avg: 0.254356) \tsec/iter: 0.0242\n",
      "Test set (epoch 265): Average loss: 0.5969, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 266 [64/169 (33%)]\tLoss: 0.183199 (avg: 0.183199) \tsec/iter: 0.0201\n",
      "Train Epoch: 266 [169/169 (100%)]\tLoss: 0.471940 (avg: 0.256874) \tsec/iter: 0.0202\n",
      "Test set (epoch 266): Average loss: 0.4746, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 267 [64/169 (33%)]\tLoss: 0.223211 (avg: 0.223211) \tsec/iter: 0.0222\n",
      "Train Epoch: 267 [169/169 (100%)]\tLoss: 0.221233 (avg: 0.255817) \tsec/iter: 0.0209\n",
      "Test set (epoch 267): Average loss: 0.3972, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 268 [64/169 (33%)]\tLoss: 0.276176 (avg: 0.276176) \tsec/iter: 0.0202\n",
      "Train Epoch: 268 [169/169 (100%)]\tLoss: 0.234544 (avg: 0.269163) \tsec/iter: 0.0202\n",
      "Test set (epoch 268): Average loss: 0.5319, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 269 [64/169 (33%)]\tLoss: 0.177581 (avg: 0.177581) \tsec/iter: 0.0303\n",
      "Train Epoch: 269 [169/169 (100%)]\tLoss: 0.390855 (avg: 0.243905) \tsec/iter: 0.0236\n",
      "Test set (epoch 269): Average loss: 0.5694, Accuracy: 13/19 (68.42%)\n",
      "\n",
      "Train Epoch: 270 [64/169 (33%)]\tLoss: 0.279980 (avg: 0.279980) \tsec/iter: 0.0222\n",
      "Train Epoch: 270 [169/169 (100%)]\tLoss: 0.338885 (avg: 0.306846) \tsec/iter: 0.0208\n",
      "Test set (epoch 270): Average loss: 0.4234, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 271 [64/169 (33%)]\tLoss: 0.242422 (avg: 0.242422) \tsec/iter: 0.0302\n",
      "Train Epoch: 271 [169/169 (100%)]\tLoss: 0.170104 (avg: 0.234234) \tsec/iter: 0.0235\n",
      "Test set (epoch 271): Average loss: 0.4255, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 272 [64/169 (33%)]\tLoss: 0.275937 (avg: 0.275937) \tsec/iter: 0.0283\n",
      "Train Epoch: 272 [169/169 (100%)]\tLoss: 0.273761 (avg: 0.241956) \tsec/iter: 0.0229\n",
      "Test set (epoch 272): Average loss: 0.4249, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 273 [64/169 (33%)]\tLoss: 0.249239 (avg: 0.249239) \tsec/iter: 0.0283\n",
      "Train Epoch: 273 [169/169 (100%)]\tLoss: 0.251355 (avg: 0.269314) \tsec/iter: 0.0229\n",
      "Test set (epoch 273): Average loss: 0.5073, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 274 [64/169 (33%)]\tLoss: 0.320638 (avg: 0.320638) \tsec/iter: 0.0201\n",
      "Train Epoch: 274 [169/169 (100%)]\tLoss: 0.213237 (avg: 0.258994) \tsec/iter: 0.0202\n",
      "Test set (epoch 274): Average loss: 0.5289, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 275 [64/169 (33%)]\tLoss: 0.265252 (avg: 0.265252) \tsec/iter: 0.0202\n",
      "Train Epoch: 275 [169/169 (100%)]\tLoss: 0.240713 (avg: 0.265980) \tsec/iter: 0.0236\n",
      "Test set (epoch 275): Average loss: 0.4791, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 276 [64/169 (33%)]\tLoss: 0.259251 (avg: 0.259251) \tsec/iter: 0.0202\n",
      "Train Epoch: 276 [169/169 (100%)]\tLoss: 0.416049 (avg: 0.252001) \tsec/iter: 0.0202\n",
      "Test set (epoch 276): Average loss: 0.4793, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 277 [64/169 (33%)]\tLoss: 0.182281 (avg: 0.182281) \tsec/iter: 0.0322\n",
      "Train Epoch: 277 [169/169 (100%)]\tLoss: 0.328396 (avg: 0.243110) \tsec/iter: 0.0242\n",
      "Test set (epoch 277): Average loss: 0.3718, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 278 [64/169 (33%)]\tLoss: 0.216136 (avg: 0.216136) \tsec/iter: 0.0202\n",
      "Train Epoch: 278 [169/169 (100%)]\tLoss: 0.224608 (avg: 0.233490) \tsec/iter: 0.0202\n",
      "Test set (epoch 278): Average loss: 0.4237, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 279 [64/169 (33%)]\tLoss: 0.156667 (avg: 0.156667) \tsec/iter: 0.0201\n",
      "Train Epoch: 279 [169/169 (100%)]\tLoss: 0.128916 (avg: 0.212968) \tsec/iter: 0.0235\n",
      "Test set (epoch 279): Average loss: 0.4315, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 280 [64/169 (33%)]\tLoss: 0.303315 (avg: 0.303315) \tsec/iter: 0.0302\n",
      "Train Epoch: 280 [169/169 (100%)]\tLoss: 0.219150 (avg: 0.251545) \tsec/iter: 0.0235\n",
      "Test set (epoch 280): Average loss: 0.4768, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 281 [64/169 (33%)]\tLoss: 0.233473 (avg: 0.233473) \tsec/iter: 0.0202\n",
      "Train Epoch: 281 [169/169 (100%)]\tLoss: 0.284602 (avg: 0.212989) \tsec/iter: 0.0202\n",
      "Test set (epoch 281): Average loss: 0.4559, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 282 [64/169 (33%)]\tLoss: 0.232977 (avg: 0.232977) \tsec/iter: 0.0202\n",
      "Train Epoch: 282 [169/169 (100%)]\tLoss: 0.268317 (avg: 0.246289) \tsec/iter: 0.0235\n",
      "Test set (epoch 282): Average loss: 0.4304, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 283 [64/169 (33%)]\tLoss: 0.121122 (avg: 0.121122) \tsec/iter: 0.0202\n",
      "Train Epoch: 283 [169/169 (100%)]\tLoss: 0.347209 (avg: 0.229542) \tsec/iter: 0.0175\n",
      "Test set (epoch 283): Average loss: 0.4950, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 284 [64/169 (33%)]\tLoss: 0.219390 (avg: 0.219390) \tsec/iter: 0.0282\n",
      "Train Epoch: 284 [169/169 (100%)]\tLoss: 0.394559 (avg: 0.254676) \tsec/iter: 0.0262\n",
      "Test set (epoch 284): Average loss: 0.4922, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 285 [64/169 (33%)]\tLoss: 0.236029 (avg: 0.236029) \tsec/iter: 0.0282\n",
      "Train Epoch: 285 [169/169 (100%)]\tLoss: 0.248099 (avg: 0.239168) \tsec/iter: 0.0235\n",
      "Test set (epoch 285): Average loss: 0.4720, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 286 [64/169 (33%)]\tLoss: 0.145266 (avg: 0.145266) \tsec/iter: 0.0222\n",
      "Train Epoch: 286 [169/169 (100%)]\tLoss: 0.263388 (avg: 0.208896) \tsec/iter: 0.0242\n",
      "Test set (epoch 286): Average loss: 0.4371, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 287 [64/169 (33%)]\tLoss: 0.203076 (avg: 0.203076) \tsec/iter: 0.0302\n",
      "Train Epoch: 287 [169/169 (100%)]\tLoss: 0.124989 (avg: 0.194274) \tsec/iter: 0.0262\n",
      "Test set (epoch 287): Average loss: 0.4540, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 288 [64/169 (33%)]\tLoss: 0.210566 (avg: 0.210566) \tsec/iter: 0.0282\n",
      "Train Epoch: 288 [169/169 (100%)]\tLoss: 0.279348 (avg: 0.248276) \tsec/iter: 0.0202\n",
      "Test set (epoch 288): Average loss: 0.4629, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 289 [64/169 (33%)]\tLoss: 0.219933 (avg: 0.219933) \tsec/iter: 0.0202\n",
      "Train Epoch: 289 [169/169 (100%)]\tLoss: 0.156735 (avg: 0.186598) \tsec/iter: 0.0202\n",
      "Test set (epoch 289): Average loss: 0.4222, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 290 [64/169 (33%)]\tLoss: 0.253737 (avg: 0.253737) \tsec/iter: 0.0223\n",
      "Train Epoch: 290 [169/169 (100%)]\tLoss: 0.255234 (avg: 0.231206) \tsec/iter: 0.0209\n",
      "Test set (epoch 290): Average loss: 0.4840, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 291 [64/169 (33%)]\tLoss: 0.286431 (avg: 0.286431) \tsec/iter: 0.0202\n",
      "Train Epoch: 291 [169/169 (100%)]\tLoss: 0.217351 (avg: 0.249554) \tsec/iter: 0.0202\n",
      "Test set (epoch 291): Average loss: 0.4677, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 292 [64/169 (33%)]\tLoss: 0.282991 (avg: 0.282991) \tsec/iter: 0.0302\n",
      "Train Epoch: 292 [169/169 (100%)]\tLoss: 0.181177 (avg: 0.252621) \tsec/iter: 0.0235\n",
      "Test set (epoch 292): Average loss: 0.4239, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 293 [64/169 (33%)]\tLoss: 0.225965 (avg: 0.225965) \tsec/iter: 0.0222\n",
      "Train Epoch: 293 [169/169 (100%)]\tLoss: 0.340384 (avg: 0.247525) \tsec/iter: 0.0208\n",
      "Test set (epoch 293): Average loss: 0.4847, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 294 [64/169 (33%)]\tLoss: 0.316156 (avg: 0.316156) \tsec/iter: 0.0202\n",
      "Train Epoch: 294 [169/169 (100%)]\tLoss: 0.261405 (avg: 0.264315) \tsec/iter: 0.0235\n",
      "Test set (epoch 294): Average loss: 0.5387, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 295 [64/169 (33%)]\tLoss: 0.266253 (avg: 0.266253) \tsec/iter: 0.0302\n",
      "Train Epoch: 295 [169/169 (100%)]\tLoss: 0.157873 (avg: 0.210973) \tsec/iter: 0.0242\n",
      "Test set (epoch 295): Average loss: 0.4787, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 296 [64/169 (33%)]\tLoss: 0.201720 (avg: 0.201720) \tsec/iter: 0.0222\n",
      "Train Epoch: 296 [169/169 (100%)]\tLoss: 0.288876 (avg: 0.235626) \tsec/iter: 0.0208\n",
      "Test set (epoch 296): Average loss: 0.5416, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 297 [64/169 (33%)]\tLoss: 0.220035 (avg: 0.220035) \tsec/iter: 0.0302\n",
      "Train Epoch: 297 [169/169 (100%)]\tLoss: 0.178160 (avg: 0.228784) \tsec/iter: 0.0201\n",
      "Test set (epoch 297): Average loss: 0.5377, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 298 [64/169 (33%)]\tLoss: 0.306523 (avg: 0.306523) \tsec/iter: 0.0222\n",
      "Train Epoch: 298 [169/169 (100%)]\tLoss: 0.154961 (avg: 0.226203) \tsec/iter: 0.0208\n",
      "Test set (epoch 298): Average loss: 0.4153, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 299 [64/169 (33%)]\tLoss: 0.232761 (avg: 0.232761) \tsec/iter: 0.0303\n",
      "Train Epoch: 299 [169/169 (100%)]\tLoss: 0.157510 (avg: 0.201751) \tsec/iter: 0.0235\n",
      "Test set (epoch 299): Average loss: 0.4341, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 300 [64/169 (33%)]\tLoss: 0.247618 (avg: 0.247618) \tsec/iter: 0.0202\n",
      "Train Epoch: 300 [169/169 (100%)]\tLoss: 0.236221 (avg: 0.240555) \tsec/iter: 0.0229\n",
      "Test set (epoch 300): Average loss: 0.3932, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 301 [64/169 (33%)]\tLoss: 0.209777 (avg: 0.209777) \tsec/iter: 0.0302\n",
      "Train Epoch: 301 [169/169 (100%)]\tLoss: 0.175649 (avg: 0.211178) \tsec/iter: 0.0235\n",
      "Test set (epoch 301): Average loss: 0.5774, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 302 [64/169 (33%)]\tLoss: 0.148958 (avg: 0.148958) \tsec/iter: 0.0222\n",
      "Train Epoch: 302 [169/169 (100%)]\tLoss: 0.277819 (avg: 0.223825) \tsec/iter: 0.0208\n",
      "Test set (epoch 302): Average loss: 0.4927, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 303 [64/169 (33%)]\tLoss: 0.312922 (avg: 0.312922) \tsec/iter: 0.0203\n",
      "Train Epoch: 303 [169/169 (100%)]\tLoss: 0.200132 (avg: 0.220918) \tsec/iter: 0.0202\n",
      "Test set (epoch 303): Average loss: 0.4629, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 304 [64/169 (33%)]\tLoss: 0.266386 (avg: 0.266386) \tsec/iter: 0.0202\n",
      "Train Epoch: 304 [169/169 (100%)]\tLoss: 0.202013 (avg: 0.226716) \tsec/iter: 0.0209\n",
      "Test set (epoch 304): Average loss: 0.4779, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 305 [64/169 (33%)]\tLoss: 0.214854 (avg: 0.214854) \tsec/iter: 0.0283\n",
      "Train Epoch: 305 [169/169 (100%)]\tLoss: 0.125889 (avg: 0.178554) \tsec/iter: 0.0202\n",
      "Test set (epoch 305): Average loss: 0.5309, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 306 [64/169 (33%)]\tLoss: 0.197651 (avg: 0.197651) \tsec/iter: 0.0202\n",
      "Train Epoch: 306 [169/169 (100%)]\tLoss: 0.285412 (avg: 0.229583) \tsec/iter: 0.0202\n",
      "Test set (epoch 306): Average loss: 0.5066, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 307 [64/169 (33%)]\tLoss: 0.163249 (avg: 0.163249) \tsec/iter: 0.0222\n",
      "Train Epoch: 307 [169/169 (100%)]\tLoss: 0.239130 (avg: 0.224738) \tsec/iter: 0.0235\n",
      "Test set (epoch 307): Average loss: 0.4701, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 308 [64/169 (33%)]\tLoss: 0.307450 (avg: 0.307450) \tsec/iter: 0.0302\n",
      "Train Epoch: 308 [169/169 (100%)]\tLoss: 0.162580 (avg: 0.250089) \tsec/iter: 0.0262\n",
      "Test set (epoch 308): Average loss: 0.5280, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 309 [64/169 (33%)]\tLoss: 0.157290 (avg: 0.157290) \tsec/iter: 0.0282\n",
      "Train Epoch: 309 [169/169 (100%)]\tLoss: 0.229852 (avg: 0.205817) \tsec/iter: 0.0235\n",
      "Test set (epoch 309): Average loss: 0.4751, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 310 [64/169 (33%)]\tLoss: 0.203558 (avg: 0.203558) \tsec/iter: 0.0283\n",
      "Train Epoch: 310 [169/169 (100%)]\tLoss: 0.206255 (avg: 0.234243) \tsec/iter: 0.0232\n",
      "Test set (epoch 310): Average loss: 0.4320, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 311 [64/169 (33%)]\tLoss: 0.153453 (avg: 0.153453) \tsec/iter: 0.0312\n",
      "Train Epoch: 311 [169/169 (100%)]\tLoss: 0.353452 (avg: 0.231483) \tsec/iter: 0.0282\n",
      "Test set (epoch 311): Average loss: 0.3727, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 312 [64/169 (33%)]\tLoss: 0.228148 (avg: 0.228148) \tsec/iter: 0.0156\n",
      "Train Epoch: 312 [169/169 (100%)]\tLoss: 0.267750 (avg: 0.219260) \tsec/iter: 0.0208\n",
      "Test set (epoch 312): Average loss: 0.5724, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 313 [64/169 (33%)]\tLoss: 0.184625 (avg: 0.184625) \tsec/iter: 0.0157\n",
      "Train Epoch: 313 [169/169 (100%)]\tLoss: 0.349572 (avg: 0.220976) \tsec/iter: 0.0156\n",
      "Test set (epoch 313): Average loss: 0.5261, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 314 [64/169 (33%)]\tLoss: 0.203450 (avg: 0.203450) \tsec/iter: 0.0156\n",
      "Train Epoch: 314 [169/169 (100%)]\tLoss: 0.126904 (avg: 0.190435) \tsec/iter: 0.0178\n",
      "Test set (epoch 314): Average loss: 0.5036, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 315 [64/169 (33%)]\tLoss: 0.210547 (avg: 0.210547) \tsec/iter: 0.0313\n",
      "Train Epoch: 315 [169/169 (100%)]\tLoss: 0.315874 (avg: 0.253866) \tsec/iter: 0.0208\n",
      "Test set (epoch 315): Average loss: 0.5004, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 316 [64/169 (33%)]\tLoss: 0.170399 (avg: 0.170399) \tsec/iter: 0.0065\n",
      "Train Epoch: 316 [169/169 (100%)]\tLoss: 0.258616 (avg: 0.231317) \tsec/iter: 0.0178\n",
      "Test set (epoch 316): Average loss: 0.5326, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 317 [64/169 (33%)]\tLoss: 0.161286 (avg: 0.161286) \tsec/iter: 0.0156\n",
      "Train Epoch: 317 [169/169 (100%)]\tLoss: 0.272433 (avg: 0.214378) \tsec/iter: 0.0178\n",
      "Test set (epoch 317): Average loss: 0.5471, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 318 [64/169 (33%)]\tLoss: 0.120165 (avg: 0.120165) \tsec/iter: 0.0157\n",
      "Train Epoch: 318 [169/169 (100%)]\tLoss: 0.310232 (avg: 0.235980) \tsec/iter: 0.0208\n",
      "Test set (epoch 318): Average loss: 0.3048, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 319 [64/169 (33%)]\tLoss: 0.149852 (avg: 0.149852) \tsec/iter: 0.0156\n",
      "Train Epoch: 319 [169/169 (100%)]\tLoss: 0.205817 (avg: 0.211030) \tsec/iter: 0.0208\n",
      "Test set (epoch 319): Average loss: 0.4280, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 320 [64/169 (33%)]\tLoss: 0.161465 (avg: 0.161465) \tsec/iter: 0.0156\n",
      "Train Epoch: 320 [169/169 (100%)]\tLoss: 0.269652 (avg: 0.207416) \tsec/iter: 0.0230\n",
      "Test set (epoch 320): Average loss: 0.4597, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 321 [64/169 (33%)]\tLoss: 0.197677 (avg: 0.197677) \tsec/iter: 0.0313\n",
      "Train Epoch: 321 [169/169 (100%)]\tLoss: 0.237738 (avg: 0.212379) \tsec/iter: 0.0230\n",
      "Test set (epoch 321): Average loss: 0.4096, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 322 [64/169 (33%)]\tLoss: 0.213059 (avg: 0.213059) \tsec/iter: 0.0313\n",
      "Train Epoch: 322 [169/169 (100%)]\tLoss: 0.236813 (avg: 0.229175) \tsec/iter: 0.0260\n",
      "Test set (epoch 322): Average loss: 0.4748, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 323 [64/169 (33%)]\tLoss: 0.210324 (avg: 0.210324) \tsec/iter: 0.0156\n",
      "Train Epoch: 323 [169/169 (100%)]\tLoss: 0.178687 (avg: 0.192061) \tsec/iter: 0.0208\n",
      "Test set (epoch 323): Average loss: 0.4296, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 324 [64/169 (33%)]\tLoss: 0.140942 (avg: 0.140942) \tsec/iter: 0.0378\n",
      "Train Epoch: 324 [169/169 (100%)]\tLoss: 0.238080 (avg: 0.196206) \tsec/iter: 0.0230\n",
      "Test set (epoch 324): Average loss: 0.4461, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 325 [64/169 (33%)]\tLoss: 0.185532 (avg: 0.185532) \tsec/iter: 0.0156\n",
      "Train Epoch: 325 [169/169 (100%)]\tLoss: 0.348031 (avg: 0.241545) \tsec/iter: 0.0230\n",
      "Test set (epoch 325): Average loss: 0.4559, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 326 [64/169 (33%)]\tLoss: 0.257529 (avg: 0.257529) \tsec/iter: 0.0313\n",
      "Train Epoch: 326 [169/169 (100%)]\tLoss: 0.143104 (avg: 0.205103) \tsec/iter: 0.0260\n",
      "Test set (epoch 326): Average loss: 0.4248, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 327 [64/169 (33%)]\tLoss: 0.199017 (avg: 0.199017) \tsec/iter: 0.0156\n",
      "Train Epoch: 327 [169/169 (100%)]\tLoss: 0.271334 (avg: 0.234238) \tsec/iter: 0.0208\n",
      "Test set (epoch 327): Average loss: 0.5197, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 328 [64/169 (33%)]\tLoss: 0.155186 (avg: 0.155186) \tsec/iter: 0.0221\n",
      "Train Epoch: 328 [169/169 (100%)]\tLoss: 0.199872 (avg: 0.208492) \tsec/iter: 0.0230\n",
      "Test set (epoch 328): Average loss: 0.5726, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 329 [64/169 (33%)]\tLoss: 0.300162 (avg: 0.300162) \tsec/iter: 0.0312\n",
      "Train Epoch: 329 [169/169 (100%)]\tLoss: 0.154724 (avg: 0.235343) \tsec/iter: 0.0230\n",
      "Test set (epoch 329): Average loss: 0.3669, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 330 [64/169 (33%)]\tLoss: 0.201227 (avg: 0.201227) \tsec/iter: 0.0156\n",
      "Train Epoch: 330 [169/169 (100%)]\tLoss: 0.272319 (avg: 0.226348) \tsec/iter: 0.0230\n",
      "Test set (epoch 330): Average loss: 0.6953, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 331 [64/169 (33%)]\tLoss: 0.266107 (avg: 0.266107) \tsec/iter: 0.0313\n",
      "Train Epoch: 331 [169/169 (100%)]\tLoss: 0.126013 (avg: 0.250581) \tsec/iter: 0.0313\n",
      "Test set (epoch 331): Average loss: 0.4778, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 332 [64/169 (33%)]\tLoss: 0.298966 (avg: 0.298966) \tsec/iter: 0.0313\n",
      "Train Epoch: 332 [169/169 (100%)]\tLoss: 0.227464 (avg: 0.237346) \tsec/iter: 0.0260\n",
      "Test set (epoch 332): Average loss: 0.6874, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 333 [64/169 (33%)]\tLoss: 0.186390 (avg: 0.186390) \tsec/iter: 0.0378\n",
      "Train Epoch: 333 [169/169 (100%)]\tLoss: 0.237185 (avg: 0.203404) \tsec/iter: 0.0282\n",
      "Test set (epoch 333): Average loss: 0.5052, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 334 [64/169 (33%)]\tLoss: 0.105236 (avg: 0.105236) \tsec/iter: 0.0378\n",
      "Train Epoch: 334 [169/169 (100%)]\tLoss: 0.324000 (avg: 0.214456) \tsec/iter: 0.0334\n",
      "Test set (epoch 334): Average loss: 0.5320, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 335 [64/169 (33%)]\tLoss: 0.268411 (avg: 0.268411) \tsec/iter: 0.0221\n",
      "Train Epoch: 335 [169/169 (100%)]\tLoss: 0.147876 (avg: 0.217547) \tsec/iter: 0.0230\n",
      "Test set (epoch 335): Average loss: 0.5561, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 336 [64/169 (33%)]\tLoss: 0.162082 (avg: 0.162082) \tsec/iter: 0.0221\n",
      "Train Epoch: 336 [169/169 (100%)]\tLoss: 0.306978 (avg: 0.227362) \tsec/iter: 0.0230\n",
      "Test set (epoch 336): Average loss: 0.4553, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 337 [64/169 (33%)]\tLoss: 0.283815 (avg: 0.283815) \tsec/iter: 0.0156\n",
      "Train Epoch: 337 [169/169 (100%)]\tLoss: 0.117695 (avg: 0.213785) \tsec/iter: 0.0230\n",
      "Test set (epoch 337): Average loss: 0.5196, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 338 [64/169 (33%)]\tLoss: 0.229891 (avg: 0.229891) \tsec/iter: 0.0312\n",
      "Train Epoch: 338 [169/169 (100%)]\tLoss: 0.149692 (avg: 0.213589) \tsec/iter: 0.0230\n",
      "Test set (epoch 338): Average loss: 0.3651, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 339 [64/169 (33%)]\tLoss: 0.166880 (avg: 0.166880) \tsec/iter: 0.0313\n",
      "Train Epoch: 339 [169/169 (100%)]\tLoss: 0.192553 (avg: 0.176901) \tsec/iter: 0.0208\n",
      "Test set (epoch 339): Average loss: 0.4851, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 340 [64/169 (33%)]\tLoss: 0.197471 (avg: 0.197471) \tsec/iter: 0.0221\n",
      "Train Epoch: 340 [169/169 (100%)]\tLoss: 0.196802 (avg: 0.261841) \tsec/iter: 0.0178\n",
      "Test set (epoch 340): Average loss: 0.4204, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 341 [64/169 (33%)]\tLoss: 0.283880 (avg: 0.283880) \tsec/iter: 0.0156\n",
      "Train Epoch: 341 [169/169 (100%)]\tLoss: 0.148712 (avg: 0.236284) \tsec/iter: 0.0178\n",
      "Test set (epoch 341): Average loss: 0.5119, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 342 [64/169 (33%)]\tLoss: 0.170249 (avg: 0.170249) \tsec/iter: 0.0156\n",
      "Train Epoch: 342 [169/169 (100%)]\tLoss: 0.240249 (avg: 0.217805) \tsec/iter: 0.0208\n",
      "Test set (epoch 342): Average loss: 0.4858, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 343 [64/169 (33%)]\tLoss: 0.225809 (avg: 0.225809) \tsec/iter: 0.0222\n",
      "Train Epoch: 343 [169/169 (100%)]\tLoss: 0.240236 (avg: 0.212283) \tsec/iter: 0.0178\n",
      "Test set (epoch 343): Average loss: 0.4614, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 344 [64/169 (33%)]\tLoss: 0.189270 (avg: 0.189270) \tsec/iter: 0.0156\n",
      "Train Epoch: 344 [169/169 (100%)]\tLoss: 0.180109 (avg: 0.246205) \tsec/iter: 0.0230\n",
      "Test set (epoch 344): Average loss: 0.4550, Accuracy: 14/19 (73.68%)\n",
      "\n",
      "Train Epoch: 345 [64/169 (33%)]\tLoss: 0.212317 (avg: 0.212317) \tsec/iter: 0.0312\n",
      "Train Epoch: 345 [169/169 (100%)]\tLoss: 0.157798 (avg: 0.202778) \tsec/iter: 0.0230\n",
      "Test set (epoch 345): Average loss: 0.5377, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 346 [64/169 (33%)]\tLoss: 0.202651 (avg: 0.202651) \tsec/iter: 0.0156\n",
      "Train Epoch: 346 [169/169 (100%)]\tLoss: 0.117003 (avg: 0.199489) \tsec/iter: 0.0208\n",
      "Test set (epoch 346): Average loss: 0.3950, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 347 [64/169 (33%)]\tLoss: 0.232466 (avg: 0.232466) \tsec/iter: 0.0534\n",
      "Train Epoch: 347 [169/169 (100%)]\tLoss: 0.276060 (avg: 0.234824) \tsec/iter: 0.0282\n",
      "Test set (epoch 347): Average loss: 0.4463, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 348 [64/169 (33%)]\tLoss: 0.154185 (avg: 0.154185) \tsec/iter: 0.0221\n",
      "Train Epoch: 348 [169/169 (100%)]\tLoss: 0.303165 (avg: 0.190737) \tsec/iter: 0.0230\n",
      "Test set (epoch 348): Average loss: 0.5108, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 349 [64/169 (33%)]\tLoss: 0.219845 (avg: 0.219845) \tsec/iter: 0.0313\n",
      "Train Epoch: 349 [169/169 (100%)]\tLoss: 0.352089 (avg: 0.250274) \tsec/iter: 0.0230\n",
      "Test set (epoch 349): Average loss: 0.5312, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 350 [64/169 (33%)]\tLoss: 0.193406 (avg: 0.193406) \tsec/iter: 0.0156\n",
      "Train Epoch: 350 [169/169 (100%)]\tLoss: 0.374959 (avg: 0.245276) \tsec/iter: 0.0156\n",
      "Test set (epoch 350): Average loss: 0.4778, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 351 [64/169 (33%)]\tLoss: 0.206183 (avg: 0.206183) \tsec/iter: 0.0222\n",
      "Train Epoch: 351 [169/169 (100%)]\tLoss: 0.130360 (avg: 0.195447) \tsec/iter: 0.0178\n",
      "Test set (epoch 351): Average loss: 0.4268, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 352 [64/169 (33%)]\tLoss: 0.208898 (avg: 0.208898) \tsec/iter: 0.0312\n",
      "Train Epoch: 352 [169/169 (100%)]\tLoss: 0.137764 (avg: 0.161595) \tsec/iter: 0.0230\n",
      "Test set (epoch 352): Average loss: 0.4564, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 353 [64/169 (33%)]\tLoss: 0.132588 (avg: 0.132588) \tsec/iter: 0.0312\n",
      "Train Epoch: 353 [169/169 (100%)]\tLoss: 0.240885 (avg: 0.175035) \tsec/iter: 0.0230\n",
      "Test set (epoch 353): Average loss: 0.5378, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 354 [64/169 (33%)]\tLoss: 0.223901 (avg: 0.223901) \tsec/iter: 0.0313\n",
      "Train Epoch: 354 [169/169 (100%)]\tLoss: 0.116798 (avg: 0.176588) \tsec/iter: 0.0223\n",
      "Test set (epoch 354): Average loss: 0.4186, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 355 [64/169 (33%)]\tLoss: 0.160601 (avg: 0.160601) \tsec/iter: 0.0333\n",
      "Train Epoch: 355 [169/169 (100%)]\tLoss: 0.360787 (avg: 0.193321) \tsec/iter: 0.0267\n",
      "Test set (epoch 355): Average loss: 0.4358, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 356 [64/169 (33%)]\tLoss: 0.210112 (avg: 0.210112) \tsec/iter: 0.0312\n",
      "Train Epoch: 356 [169/169 (100%)]\tLoss: 0.194657 (avg: 0.206266) \tsec/iter: 0.0230\n",
      "Test set (epoch 356): Average loss: 0.5888, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 357 [64/169 (33%)]\tLoss: 0.227628 (avg: 0.227628) \tsec/iter: 0.0313\n",
      "Train Epoch: 357 [169/169 (100%)]\tLoss: 0.233375 (avg: 0.243017) \tsec/iter: 0.0230\n",
      "Test set (epoch 357): Average loss: 0.5131, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 358 [64/169 (33%)]\tLoss: 0.214679 (avg: 0.214679) \tsec/iter: 0.0313\n",
      "Train Epoch: 358 [169/169 (100%)]\tLoss: 0.164699 (avg: 0.241824) \tsec/iter: 0.0282\n",
      "Test set (epoch 358): Average loss: 0.6040, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 359 [64/169 (33%)]\tLoss: 0.237249 (avg: 0.237249) \tsec/iter: 0.0313\n",
      "Train Epoch: 359 [169/169 (100%)]\tLoss: 0.235497 (avg: 0.224646) \tsec/iter: 0.0260\n",
      "Test set (epoch 359): Average loss: 0.5387, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 360 [64/169 (33%)]\tLoss: 0.177510 (avg: 0.177510) \tsec/iter: 0.0378\n",
      "Train Epoch: 360 [169/169 (100%)]\tLoss: 0.237709 (avg: 0.198124) \tsec/iter: 0.0282\n",
      "Test set (epoch 360): Average loss: 0.6515, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 361 [64/169 (33%)]\tLoss: 0.209027 (avg: 0.209027) \tsec/iter: 0.0221\n",
      "Train Epoch: 361 [169/169 (100%)]\tLoss: 0.114276 (avg: 0.181286) \tsec/iter: 0.0230\n",
      "Test set (epoch 361): Average loss: 0.5879, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 362 [64/169 (33%)]\tLoss: 0.121394 (avg: 0.121394) \tsec/iter: 0.0313\n",
      "Train Epoch: 362 [169/169 (100%)]\tLoss: 0.229697 (avg: 0.220754) \tsec/iter: 0.0230\n",
      "Test set (epoch 362): Average loss: 0.4842, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 363 [64/169 (33%)]\tLoss: 0.182312 (avg: 0.182312) \tsec/iter: 0.0313\n",
      "Train Epoch: 363 [169/169 (100%)]\tLoss: 0.217298 (avg: 0.210640) \tsec/iter: 0.0230\n",
      "Test set (epoch 363): Average loss: 0.5153, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 364 [64/169 (33%)]\tLoss: 0.215030 (avg: 0.215030) \tsec/iter: 0.0313\n",
      "Train Epoch: 364 [169/169 (100%)]\tLoss: 0.219776 (avg: 0.218244) \tsec/iter: 0.0229\n",
      "Test set (epoch 364): Average loss: 0.3952, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 365 [64/169 (33%)]\tLoss: 0.193790 (avg: 0.193790) \tsec/iter: 0.0181\n",
      "Train Epoch: 365 [169/169 (100%)]\tLoss: 0.203334 (avg: 0.207867) \tsec/iter: 0.0217\n",
      "Test set (epoch 365): Average loss: 0.3705, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 366 [64/169 (33%)]\tLoss: 0.173252 (avg: 0.173252) \tsec/iter: 0.0353\n",
      "Train Epoch: 366 [169/169 (100%)]\tLoss: 0.224947 (avg: 0.211820) \tsec/iter: 0.0222\n",
      "Test set (epoch 366): Average loss: 0.5587, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 367 [64/169 (33%)]\tLoss: 0.151985 (avg: 0.151985) \tsec/iter: 0.0313\n",
      "Train Epoch: 367 [169/169 (100%)]\tLoss: 0.190691 (avg: 0.220986) \tsec/iter: 0.0230\n",
      "Test set (epoch 367): Average loss: 0.3872, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 368 [64/169 (33%)]\tLoss: 0.155590 (avg: 0.155590) \tsec/iter: 0.0156\n",
      "Train Epoch: 368 [169/169 (100%)]\tLoss: 0.162308 (avg: 0.202404) \tsec/iter: 0.0208\n",
      "Test set (epoch 368): Average loss: 0.4863, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 369 [64/169 (33%)]\tLoss: 0.135091 (avg: 0.135091) \tsec/iter: 0.0221\n",
      "Train Epoch: 369 [169/169 (100%)]\tLoss: 0.193465 (avg: 0.233842) \tsec/iter: 0.0230\n",
      "Test set (epoch 369): Average loss: 0.3581, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 370 [64/169 (33%)]\tLoss: 0.146792 (avg: 0.146792) \tsec/iter: 0.0156\n",
      "Train Epoch: 370 [169/169 (100%)]\tLoss: 0.216566 (avg: 0.183609) \tsec/iter: 0.0178\n",
      "Test set (epoch 370): Average loss: 0.5956, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 371 [64/169 (33%)]\tLoss: 0.217127 (avg: 0.217127) \tsec/iter: 0.0312\n",
      "Train Epoch: 371 [169/169 (100%)]\tLoss: 0.177620 (avg: 0.210132) \tsec/iter: 0.0260\n",
      "Test set (epoch 371): Average loss: 0.4376, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 372 [64/169 (33%)]\tLoss: 0.270147 (avg: 0.270147) \tsec/iter: 0.0156\n",
      "Train Epoch: 372 [169/169 (100%)]\tLoss: 0.152337 (avg: 0.262911) \tsec/iter: 0.0156\n",
      "Test set (epoch 372): Average loss: 0.5291, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 373 [64/169 (33%)]\tLoss: 0.245352 (avg: 0.245352) \tsec/iter: 0.0156\n",
      "Train Epoch: 373 [169/169 (100%)]\tLoss: 0.119688 (avg: 0.186591) \tsec/iter: 0.0178\n",
      "Test set (epoch 373): Average loss: 0.4569, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 374 [64/169 (33%)]\tLoss: 0.218622 (avg: 0.218622) \tsec/iter: 0.0312\n",
      "Train Epoch: 374 [169/169 (100%)]\tLoss: 0.206780 (avg: 0.208624) \tsec/iter: 0.0230\n",
      "Test set (epoch 374): Average loss: 0.4356, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 375 [64/169 (33%)]\tLoss: 0.141894 (avg: 0.141894) \tsec/iter: 0.0156\n",
      "Train Epoch: 375 [169/169 (100%)]\tLoss: 0.301719 (avg: 0.205635) \tsec/iter: 0.0208\n",
      "Test set (epoch 375): Average loss: 0.6842, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 376 [64/169 (33%)]\tLoss: 0.147227 (avg: 0.147227) \tsec/iter: 0.0221\n",
      "Train Epoch: 376 [169/169 (100%)]\tLoss: 0.367269 (avg: 0.190282) \tsec/iter: 0.0230\n",
      "Test set (epoch 376): Average loss: 0.4373, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 377 [64/169 (33%)]\tLoss: 0.230368 (avg: 0.230368) \tsec/iter: 0.0156\n",
      "Train Epoch: 377 [169/169 (100%)]\tLoss: 0.141843 (avg: 0.197065) \tsec/iter: 0.0230\n",
      "Test set (epoch 377): Average loss: 0.4390, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 378 [64/169 (33%)]\tLoss: 0.204754 (avg: 0.204754) \tsec/iter: 0.0313\n",
      "Train Epoch: 378 [169/169 (100%)]\tLoss: 0.207982 (avg: 0.187247) \tsec/iter: 0.0230\n",
      "Test set (epoch 378): Average loss: 0.5666, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 379 [64/169 (33%)]\tLoss: 0.207256 (avg: 0.207256) \tsec/iter: 0.0156\n",
      "Train Epoch: 379 [169/169 (100%)]\tLoss: 0.205712 (avg: 0.203638) \tsec/iter: 0.0208\n",
      "Test set (epoch 379): Average loss: 0.5596, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 380 [64/169 (33%)]\tLoss: 0.331880 (avg: 0.331880) \tsec/iter: 0.0222\n",
      "Train Epoch: 380 [169/169 (100%)]\tLoss: 0.194643 (avg: 0.216874) \tsec/iter: 0.0282\n",
      "Test set (epoch 380): Average loss: 0.4015, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 381 [64/169 (33%)]\tLoss: 0.286590 (avg: 0.286590) \tsec/iter: 0.0343\n",
      "Train Epoch: 381 [169/169 (100%)]\tLoss: 0.216052 (avg: 0.241559) \tsec/iter: 0.0276\n",
      "Test set (epoch 381): Average loss: 0.4224, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 382 [64/169 (33%)]\tLoss: 0.176107 (avg: 0.176107) \tsec/iter: 0.0303\n",
      "Train Epoch: 382 [169/169 (100%)]\tLoss: 0.313719 (avg: 0.213401) \tsec/iter: 0.0229\n",
      "Test set (epoch 382): Average loss: 0.4634, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 383 [64/169 (33%)]\tLoss: 0.186602 (avg: 0.186602) \tsec/iter: 0.0282\n",
      "Train Epoch: 383 [169/169 (100%)]\tLoss: 0.237869 (avg: 0.206944) \tsec/iter: 0.0235\n",
      "Test set (epoch 383): Average loss: 0.4952, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 384 [64/169 (33%)]\tLoss: 0.277203 (avg: 0.277203) \tsec/iter: 0.0222\n",
      "Train Epoch: 384 [169/169 (100%)]\tLoss: 0.241972 (avg: 0.239372) \tsec/iter: 0.0209\n",
      "Test set (epoch 384): Average loss: 0.3958, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 385 [64/169 (33%)]\tLoss: 0.246261 (avg: 0.246261) \tsec/iter: 0.0201\n",
      "Train Epoch: 385 [169/169 (100%)]\tLoss: 0.071350 (avg: 0.191212) \tsec/iter: 0.0202\n",
      "Test set (epoch 385): Average loss: 0.5338, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 386 [64/169 (33%)]\tLoss: 0.250359 (avg: 0.250359) \tsec/iter: 0.0202\n",
      "Train Epoch: 386 [169/169 (100%)]\tLoss: 0.229251 (avg: 0.204779) \tsec/iter: 0.0202\n",
      "Test set (epoch 386): Average loss: 0.4866, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 387 [64/169 (33%)]\tLoss: 0.224054 (avg: 0.224054) \tsec/iter: 0.0202\n",
      "Train Epoch: 387 [169/169 (100%)]\tLoss: 0.362900 (avg: 0.223642) \tsec/iter: 0.0235\n",
      "Test set (epoch 387): Average loss: 0.8000, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 388 [64/169 (33%)]\tLoss: 0.280387 (avg: 0.280387) \tsec/iter: 0.0182\n",
      "Train Epoch: 388 [169/169 (100%)]\tLoss: 0.165148 (avg: 0.233480) \tsec/iter: 0.0168\n",
      "Test set (epoch 388): Average loss: 0.5861, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 389 [64/169 (33%)]\tLoss: 0.141723 (avg: 0.141723) \tsec/iter: 0.0202\n",
      "Train Epoch: 389 [169/169 (100%)]\tLoss: 0.255290 (avg: 0.192734) \tsec/iter: 0.0202\n",
      "Test set (epoch 389): Average loss: 0.5399, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 390 [64/169 (33%)]\tLoss: 0.236485 (avg: 0.236485) \tsec/iter: 0.0202\n",
      "Train Epoch: 390 [169/169 (100%)]\tLoss: 0.200652 (avg: 0.223372) \tsec/iter: 0.0202\n",
      "Test set (epoch 390): Average loss: 0.5187, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 391 [64/169 (33%)]\tLoss: 0.157006 (avg: 0.157006) \tsec/iter: 0.0283\n",
      "Train Epoch: 391 [169/169 (100%)]\tLoss: 0.242048 (avg: 0.201190) \tsec/iter: 0.0202\n",
      "Test set (epoch 391): Average loss: 0.4775, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 392 [64/169 (33%)]\tLoss: 0.160318 (avg: 0.160318) \tsec/iter: 0.0201\n",
      "Train Epoch: 392 [169/169 (100%)]\tLoss: 0.325440 (avg: 0.205277) \tsec/iter: 0.0202\n",
      "Test set (epoch 392): Average loss: 0.5719, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 393 [64/169 (33%)]\tLoss: 0.300464 (avg: 0.300464) \tsec/iter: 0.0202\n",
      "Train Epoch: 393 [169/169 (100%)]\tLoss: 0.159894 (avg: 0.200844) \tsec/iter: 0.0202\n",
      "Test set (epoch 393): Average loss: 0.6255, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 394 [64/169 (33%)]\tLoss: 0.301703 (avg: 0.301703) \tsec/iter: 0.0202\n",
      "Train Epoch: 394 [169/169 (100%)]\tLoss: 0.131741 (avg: 0.223189) \tsec/iter: 0.0196\n",
      "Test set (epoch 394): Average loss: 0.5511, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 395 [64/169 (33%)]\tLoss: 0.215794 (avg: 0.215794) \tsec/iter: 0.0283\n",
      "Train Epoch: 395 [169/169 (100%)]\tLoss: 0.135546 (avg: 0.192686) \tsec/iter: 0.0236\n",
      "Test set (epoch 395): Average loss: 0.4829, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 396 [64/169 (33%)]\tLoss: 0.239366 (avg: 0.239366) \tsec/iter: 0.0283\n",
      "Train Epoch: 396 [169/169 (100%)]\tLoss: 0.206676 (avg: 0.212904) \tsec/iter: 0.0236\n",
      "Test set (epoch 396): Average loss: 0.5730, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 397 [64/169 (33%)]\tLoss: 0.155037 (avg: 0.155037) \tsec/iter: 0.0201\n",
      "Train Epoch: 397 [169/169 (100%)]\tLoss: 0.250654 (avg: 0.178440) \tsec/iter: 0.0202\n",
      "Test set (epoch 397): Average loss: 0.5510, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 398 [64/169 (33%)]\tLoss: 0.212413 (avg: 0.212413) \tsec/iter: 0.0303\n",
      "Train Epoch: 398 [169/169 (100%)]\tLoss: 0.218441 (avg: 0.214128) \tsec/iter: 0.0236\n",
      "Test set (epoch 398): Average loss: 0.3527, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 399 [64/169 (33%)]\tLoss: 0.166699 (avg: 0.166699) \tsec/iter: 0.0282\n",
      "Train Epoch: 399 [169/169 (100%)]\tLoss: 0.306699 (avg: 0.218984) \tsec/iter: 0.0236\n",
      "Test set (epoch 399): Average loss: 0.3977, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 400 [64/169 (33%)]\tLoss: 0.213335 (avg: 0.213335) \tsec/iter: 0.0202\n",
      "Train Epoch: 400 [169/169 (100%)]\tLoss: 0.227024 (avg: 0.230136) \tsec/iter: 0.0202\n",
      "Test set (epoch 400): Average loss: 0.3517, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 401 [64/169 (33%)]\tLoss: 0.266471 (avg: 0.266471) \tsec/iter: 0.0223\n",
      "Train Epoch: 401 [169/169 (100%)]\tLoss: 0.153904 (avg: 0.215217) \tsec/iter: 0.0242\n",
      "Test set (epoch 401): Average loss: 0.4291, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 402 [64/169 (33%)]\tLoss: 0.219110 (avg: 0.219110) \tsec/iter: 0.0282\n",
      "Train Epoch: 402 [169/169 (100%)]\tLoss: 0.146480 (avg: 0.193722) \tsec/iter: 0.0235\n",
      "Test set (epoch 402): Average loss: 0.4786, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 403 [64/169 (33%)]\tLoss: 0.170673 (avg: 0.170673) \tsec/iter: 0.0302\n",
      "Train Epoch: 403 [169/169 (100%)]\tLoss: 0.307417 (avg: 0.192197) \tsec/iter: 0.0269\n",
      "Test set (epoch 403): Average loss: 0.5205, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 404 [64/169 (33%)]\tLoss: 0.200747 (avg: 0.200747) \tsec/iter: 0.0323\n",
      "Train Epoch: 404 [169/169 (100%)]\tLoss: 0.094594 (avg: 0.188736) \tsec/iter: 0.0309\n",
      "Test set (epoch 404): Average loss: 0.5727, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 405 [64/169 (33%)]\tLoss: 0.186283 (avg: 0.186283) \tsec/iter: 0.0303\n",
      "Train Epoch: 405 [169/169 (100%)]\tLoss: 0.200172 (avg: 0.226027) \tsec/iter: 0.0242\n",
      "Test set (epoch 405): Average loss: 0.4649, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 406 [64/169 (33%)]\tLoss: 0.091446 (avg: 0.091446) \tsec/iter: 0.0323\n",
      "Train Epoch: 406 [169/169 (100%)]\tLoss: 0.198394 (avg: 0.194685) \tsec/iter: 0.0269\n",
      "Test set (epoch 406): Average loss: 0.3872, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 407 [64/169 (33%)]\tLoss: 0.138424 (avg: 0.138424) \tsec/iter: 0.0201\n",
      "Train Epoch: 407 [169/169 (100%)]\tLoss: 0.239907 (avg: 0.191110) \tsec/iter: 0.0202\n",
      "Test set (epoch 407): Average loss: 0.4628, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 408 [64/169 (33%)]\tLoss: 0.235887 (avg: 0.235887) \tsec/iter: 0.0202\n",
      "Train Epoch: 408 [169/169 (100%)]\tLoss: 0.100054 (avg: 0.159872) \tsec/iter: 0.0229\n",
      "Test set (epoch 408): Average loss: 0.4828, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 409 [64/169 (33%)]\tLoss: 0.149354 (avg: 0.149354) \tsec/iter: 0.0303\n",
      "Train Epoch: 409 [169/169 (100%)]\tLoss: 0.131859 (avg: 0.203479) \tsec/iter: 0.0236\n",
      "Test set (epoch 409): Average loss: 0.4607, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 410 [64/169 (33%)]\tLoss: 0.181808 (avg: 0.181808) \tsec/iter: 0.0283\n",
      "Train Epoch: 410 [169/169 (100%)]\tLoss: 0.226521 (avg: 0.168691) \tsec/iter: 0.0202\n",
      "Test set (epoch 410): Average loss: 0.4048, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 411 [64/169 (33%)]\tLoss: 0.204197 (avg: 0.204197) \tsec/iter: 0.0282\n",
      "Train Epoch: 411 [169/169 (100%)]\tLoss: 0.130827 (avg: 0.160212) \tsec/iter: 0.0235\n",
      "Test set (epoch 411): Average loss: 0.3308, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 412 [64/169 (33%)]\tLoss: 0.205342 (avg: 0.205342) \tsec/iter: 0.0302\n",
      "Train Epoch: 412 [169/169 (100%)]\tLoss: 0.149191 (avg: 0.193373) \tsec/iter: 0.0228\n",
      "Test set (epoch 412): Average loss: 0.4995, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 413 [64/169 (33%)]\tLoss: 0.157248 (avg: 0.157248) \tsec/iter: 0.0283\n",
      "Train Epoch: 413 [169/169 (100%)]\tLoss: 0.145148 (avg: 0.155329) \tsec/iter: 0.0229\n",
      "Test set (epoch 413): Average loss: 0.5688, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 414 [64/169 (33%)]\tLoss: 0.188061 (avg: 0.188061) \tsec/iter: 0.0322\n",
      "Train Epoch: 414 [169/169 (100%)]\tLoss: 0.190569 (avg: 0.167731) \tsec/iter: 0.0269\n",
      "Test set (epoch 414): Average loss: 0.5288, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 415 [64/169 (33%)]\tLoss: 0.180427 (avg: 0.180427) \tsec/iter: 0.0203\n",
      "Train Epoch: 415 [169/169 (100%)]\tLoss: 0.332729 (avg: 0.203033) \tsec/iter: 0.0202\n",
      "Test set (epoch 415): Average loss: 0.4728, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 416 [64/169 (33%)]\tLoss: 0.326756 (avg: 0.326756) \tsec/iter: 0.0202\n",
      "Train Epoch: 416 [169/169 (100%)]\tLoss: 0.259201 (avg: 0.257245) \tsec/iter: 0.0202\n",
      "Test set (epoch 416): Average loss: 0.5345, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 417 [64/169 (33%)]\tLoss: 0.141646 (avg: 0.141646) \tsec/iter: 0.0202\n",
      "Train Epoch: 417 [169/169 (100%)]\tLoss: 0.465610 (avg: 0.245988) \tsec/iter: 0.0202\n",
      "Test set (epoch 417): Average loss: 0.6140, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 418 [64/169 (33%)]\tLoss: 0.141111 (avg: 0.141111) \tsec/iter: 0.0303\n",
      "Train Epoch: 418 [169/169 (100%)]\tLoss: 0.237483 (avg: 0.202153) \tsec/iter: 0.0229\n",
      "Test set (epoch 418): Average loss: 0.5299, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 419 [64/169 (33%)]\tLoss: 0.143832 (avg: 0.143832) \tsec/iter: 0.0202\n",
      "Train Epoch: 419 [169/169 (100%)]\tLoss: 0.151918 (avg: 0.161123) \tsec/iter: 0.0202\n",
      "Test set (epoch 419): Average loss: 0.5095, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 420 [64/169 (33%)]\tLoss: 0.160675 (avg: 0.160675) \tsec/iter: 0.0202\n",
      "Train Epoch: 420 [169/169 (100%)]\tLoss: 0.250484 (avg: 0.183438) \tsec/iter: 0.0202\n",
      "Test set (epoch 420): Average loss: 0.5375, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 421 [64/169 (33%)]\tLoss: 0.173966 (avg: 0.173966) \tsec/iter: 0.0202\n",
      "Train Epoch: 421 [169/169 (100%)]\tLoss: 0.296476 (avg: 0.207658) \tsec/iter: 0.0229\n",
      "Test set (epoch 421): Average loss: 0.6148, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 422 [64/169 (33%)]\tLoss: 0.167486 (avg: 0.167486) \tsec/iter: 0.0282\n",
      "Train Epoch: 422 [169/169 (100%)]\tLoss: 0.158280 (avg: 0.195084) \tsec/iter: 0.0202\n",
      "Test set (epoch 422): Average loss: 0.7544, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 423 [64/169 (33%)]\tLoss: 0.211998 (avg: 0.211998) \tsec/iter: 0.0202\n",
      "Train Epoch: 423 [169/169 (100%)]\tLoss: 0.131060 (avg: 0.207112) \tsec/iter: 0.0202\n",
      "Test set (epoch 423): Average loss: 0.4089, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 424 [64/169 (33%)]\tLoss: 0.233704 (avg: 0.233704) \tsec/iter: 0.0202\n",
      "Train Epoch: 424 [169/169 (100%)]\tLoss: 0.108690 (avg: 0.177268) \tsec/iter: 0.0236\n",
      "Test set (epoch 424): Average loss: 0.6025, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 425 [64/169 (33%)]\tLoss: 0.295974 (avg: 0.295974) \tsec/iter: 0.0302\n",
      "Train Epoch: 425 [169/169 (100%)]\tLoss: 0.177877 (avg: 0.215086) \tsec/iter: 0.0242\n",
      "Test set (epoch 425): Average loss: 0.6246, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 426 [64/169 (33%)]\tLoss: 0.183583 (avg: 0.183583) \tsec/iter: 0.0222\n",
      "Train Epoch: 426 [169/169 (100%)]\tLoss: 0.228224 (avg: 0.185142) \tsec/iter: 0.0242\n",
      "Test set (epoch 426): Average loss: 0.5128, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 427 [64/169 (33%)]\tLoss: 0.141386 (avg: 0.141386) \tsec/iter: 0.0303\n",
      "Train Epoch: 427 [169/169 (100%)]\tLoss: 0.203663 (avg: 0.184144) \tsec/iter: 0.0269\n",
      "Test set (epoch 427): Average loss: 0.4136, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 428 [64/169 (33%)]\tLoss: 0.209762 (avg: 0.209762) \tsec/iter: 0.0302\n",
      "Train Epoch: 428 [169/169 (100%)]\tLoss: 0.232692 (avg: 0.193823) \tsec/iter: 0.0269\n",
      "Test set (epoch 428): Average loss: 0.6655, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 429 [64/169 (33%)]\tLoss: 0.138438 (avg: 0.138438) \tsec/iter: 0.0202\n",
      "Train Epoch: 429 [169/169 (100%)]\tLoss: 0.177457 (avg: 0.184185) \tsec/iter: 0.0229\n",
      "Test set (epoch 429): Average loss: 0.3598, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 430 [64/169 (33%)]\tLoss: 0.157064 (avg: 0.157064) \tsec/iter: 0.0282\n",
      "Train Epoch: 430 [169/169 (100%)]\tLoss: 0.088667 (avg: 0.167450) \tsec/iter: 0.0229\n",
      "Test set (epoch 430): Average loss: 0.7450, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 431 [64/169 (33%)]\tLoss: 0.205904 (avg: 0.205904) \tsec/iter: 0.0283\n",
      "Train Epoch: 431 [169/169 (100%)]\tLoss: 0.102539 (avg: 0.182239) \tsec/iter: 0.0236\n",
      "Test set (epoch 431): Average loss: 0.5664, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 432 [64/169 (33%)]\tLoss: 0.151605 (avg: 0.151605) \tsec/iter: 0.0222\n",
      "Train Epoch: 432 [169/169 (100%)]\tLoss: 0.264524 (avg: 0.196777) \tsec/iter: 0.0209\n",
      "Test set (epoch 432): Average loss: 0.4237, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 433 [64/169 (33%)]\tLoss: 0.131865 (avg: 0.131865) \tsec/iter: 0.0202\n",
      "Train Epoch: 433 [169/169 (100%)]\tLoss: 0.217308 (avg: 0.181444) \tsec/iter: 0.0175\n",
      "Test set (epoch 433): Average loss: 0.5010, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 434 [64/169 (33%)]\tLoss: 0.101602 (avg: 0.101602) \tsec/iter: 0.0202\n",
      "Train Epoch: 434 [169/169 (100%)]\tLoss: 0.352219 (avg: 0.222273) \tsec/iter: 0.0202\n",
      "Test set (epoch 434): Average loss: 0.5111, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 435 [64/169 (33%)]\tLoss: 0.118367 (avg: 0.118367) \tsec/iter: 0.0222\n",
      "Train Epoch: 435 [169/169 (100%)]\tLoss: 0.185106 (avg: 0.175432) \tsec/iter: 0.0208\n",
      "Test set (epoch 435): Average loss: 0.5911, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 436 [64/169 (33%)]\tLoss: 0.176589 (avg: 0.176589) \tsec/iter: 0.0203\n",
      "Train Epoch: 436 [169/169 (100%)]\tLoss: 0.353121 (avg: 0.206114) \tsec/iter: 0.0203\n",
      "Test set (epoch 436): Average loss: 0.3656, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 437 [64/169 (33%)]\tLoss: 0.231186 (avg: 0.231186) \tsec/iter: 0.0222\n",
      "Train Epoch: 437 [169/169 (100%)]\tLoss: 0.195011 (avg: 0.223977) \tsec/iter: 0.0208\n",
      "Test set (epoch 437): Average loss: 0.7616, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 438 [64/169 (33%)]\tLoss: 0.155079 (avg: 0.155079) \tsec/iter: 0.0203\n",
      "Train Epoch: 438 [169/169 (100%)]\tLoss: 0.219496 (avg: 0.225357) \tsec/iter: 0.0202\n",
      "Test set (epoch 438): Average loss: 0.5002, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 439 [64/169 (33%)]\tLoss: 0.159550 (avg: 0.159550) \tsec/iter: 0.0202\n",
      "Train Epoch: 439 [169/169 (100%)]\tLoss: 0.227009 (avg: 0.227054) \tsec/iter: 0.0235\n",
      "Test set (epoch 439): Average loss: 0.6637, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 440 [64/169 (33%)]\tLoss: 0.199990 (avg: 0.199990) \tsec/iter: 0.0302\n",
      "Train Epoch: 440 [169/169 (100%)]\tLoss: 0.278359 (avg: 0.189169) \tsec/iter: 0.0235\n",
      "Test set (epoch 440): Average loss: 0.5312, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 441 [64/169 (33%)]\tLoss: 0.156557 (avg: 0.156557) \tsec/iter: 0.0202\n",
      "Train Epoch: 441 [169/169 (100%)]\tLoss: 0.356177 (avg: 0.198070) \tsec/iter: 0.0235\n",
      "Test set (epoch 441): Average loss: 0.5783, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 442 [64/169 (33%)]\tLoss: 0.226172 (avg: 0.226172) \tsec/iter: 0.0282\n",
      "Train Epoch: 442 [169/169 (100%)]\tLoss: 0.114077 (avg: 0.218756) \tsec/iter: 0.0235\n",
      "Test set (epoch 442): Average loss: 0.5694, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 443 [64/169 (33%)]\tLoss: 0.165472 (avg: 0.165472) \tsec/iter: 0.0222\n",
      "Train Epoch: 443 [169/169 (100%)]\tLoss: 0.082869 (avg: 0.184599) \tsec/iter: 0.0208\n",
      "Test set (epoch 443): Average loss: 0.5035, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 444 [64/169 (33%)]\tLoss: 0.223407 (avg: 0.223407) \tsec/iter: 0.0202\n",
      "Train Epoch: 444 [169/169 (100%)]\tLoss: 0.175565 (avg: 0.165732) \tsec/iter: 0.0202\n",
      "Test set (epoch 444): Average loss: 0.3813, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 445 [64/169 (33%)]\tLoss: 0.228604 (avg: 0.228604) \tsec/iter: 0.0202\n",
      "Train Epoch: 445 [169/169 (100%)]\tLoss: 0.308042 (avg: 0.219023) \tsec/iter: 0.0202\n",
      "Test set (epoch 445): Average loss: 0.4107, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 446 [64/169 (33%)]\tLoss: 0.147611 (avg: 0.147611) \tsec/iter: 0.0202\n",
      "Train Epoch: 446 [169/169 (100%)]\tLoss: 0.092765 (avg: 0.165371) \tsec/iter: 0.0202\n",
      "Test set (epoch 446): Average loss: 0.4564, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 447 [64/169 (33%)]\tLoss: 0.121853 (avg: 0.121853) \tsec/iter: 0.0222\n",
      "Train Epoch: 447 [169/169 (100%)]\tLoss: 0.178807 (avg: 0.177751) \tsec/iter: 0.0208\n",
      "Test set (epoch 447): Average loss: 0.3129, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 448 [64/169 (33%)]\tLoss: 0.164019 (avg: 0.164019) \tsec/iter: 0.0201\n",
      "Train Epoch: 448 [169/169 (100%)]\tLoss: 0.262114 (avg: 0.206861) \tsec/iter: 0.0209\n",
      "Test set (epoch 448): Average loss: 0.5798, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 449 [64/169 (33%)]\tLoss: 0.321131 (avg: 0.321131) \tsec/iter: 0.0222\n",
      "Train Epoch: 449 [169/169 (100%)]\tLoss: 0.132483 (avg: 0.218681) \tsec/iter: 0.0208\n",
      "Test set (epoch 449): Average loss: 0.5167, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 450 [64/169 (33%)]\tLoss: 0.217135 (avg: 0.217135) \tsec/iter: 0.0223\n",
      "Train Epoch: 450 [169/169 (100%)]\tLoss: 0.239246 (avg: 0.180222) \tsec/iter: 0.0202\n",
      "Test set (epoch 450): Average loss: 0.6235, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 451 [64/169 (33%)]\tLoss: 0.183116 (avg: 0.183116) \tsec/iter: 0.0302\n",
      "Train Epoch: 451 [169/169 (100%)]\tLoss: 0.156383 (avg: 0.177744) \tsec/iter: 0.0236\n",
      "Test set (epoch 451): Average loss: 0.4384, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 452 [64/169 (33%)]\tLoss: 0.224627 (avg: 0.224627) \tsec/iter: 0.0202\n",
      "Train Epoch: 452 [169/169 (100%)]\tLoss: 0.094930 (avg: 0.169182) \tsec/iter: 0.0262\n",
      "Test set (epoch 452): Average loss: 0.4845, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 453 [64/169 (33%)]\tLoss: 0.188089 (avg: 0.188089) \tsec/iter: 0.0382\n",
      "Train Epoch: 453 [169/169 (100%)]\tLoss: 0.220906 (avg: 0.198618) \tsec/iter: 0.0269\n",
      "Test set (epoch 453): Average loss: 0.3902, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 454 [64/169 (33%)]\tLoss: 0.120251 (avg: 0.120251) \tsec/iter: 0.0302\n",
      "Train Epoch: 454 [169/169 (100%)]\tLoss: 0.190356 (avg: 0.166478) \tsec/iter: 0.0269\n",
      "Test set (epoch 454): Average loss: 0.5696, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 455 [64/169 (33%)]\tLoss: 0.222576 (avg: 0.222576) \tsec/iter: 0.0302\n",
      "Train Epoch: 455 [169/169 (100%)]\tLoss: 0.088943 (avg: 0.192033) \tsec/iter: 0.0235\n",
      "Test set (epoch 455): Average loss: 0.5472, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 456 [64/169 (33%)]\tLoss: 0.204891 (avg: 0.204891) \tsec/iter: 0.0322\n",
      "Train Epoch: 456 [169/169 (100%)]\tLoss: 0.204650 (avg: 0.194010) \tsec/iter: 0.0276\n",
      "Test set (epoch 456): Average loss: 0.4990, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 457 [64/169 (33%)]\tLoss: 0.106072 (avg: 0.106072) \tsec/iter: 0.0222\n",
      "Train Epoch: 457 [169/169 (100%)]\tLoss: 0.134778 (avg: 0.152886) \tsec/iter: 0.0242\n",
      "Test set (epoch 457): Average loss: 0.5536, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 458 [64/169 (33%)]\tLoss: 0.147220 (avg: 0.147220) \tsec/iter: 0.0202\n",
      "Train Epoch: 458 [169/169 (100%)]\tLoss: 0.195160 (avg: 0.162592) \tsec/iter: 0.0195\n",
      "Test set (epoch 458): Average loss: 0.6967, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 459 [64/169 (33%)]\tLoss: 0.180063 (avg: 0.180063) \tsec/iter: 0.0201\n",
      "Train Epoch: 459 [169/169 (100%)]\tLoss: 0.167001 (avg: 0.238439) \tsec/iter: 0.0202\n",
      "Test set (epoch 459): Average loss: 0.4615, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 460 [64/169 (33%)]\tLoss: 0.153974 (avg: 0.153974) \tsec/iter: 0.0222\n",
      "Train Epoch: 460 [169/169 (100%)]\tLoss: 0.301193 (avg: 0.189154) \tsec/iter: 0.0202\n",
      "Test set (epoch 460): Average loss: 0.5367, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 461 [64/169 (33%)]\tLoss: 0.254539 (avg: 0.254539) \tsec/iter: 0.0283\n",
      "Train Epoch: 461 [169/169 (100%)]\tLoss: 0.113515 (avg: 0.206348) \tsec/iter: 0.0202\n",
      "Test set (epoch 461): Average loss: 0.4568, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 462 [64/169 (33%)]\tLoss: 0.151014 (avg: 0.151014) \tsec/iter: 0.0202\n",
      "Train Epoch: 462 [169/169 (100%)]\tLoss: 0.158702 (avg: 0.151610) \tsec/iter: 0.0195\n",
      "Test set (epoch 462): Average loss: 0.5698, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 463 [64/169 (33%)]\tLoss: 0.102961 (avg: 0.102961) \tsec/iter: 0.0202\n",
      "Train Epoch: 463 [169/169 (100%)]\tLoss: 0.170920 (avg: 0.198566) \tsec/iter: 0.0229\n",
      "Test set (epoch 463): Average loss: 0.5208, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 464 [64/169 (33%)]\tLoss: 0.181091 (avg: 0.181091) \tsec/iter: 0.0202\n",
      "Train Epoch: 464 [169/169 (100%)]\tLoss: 0.187788 (avg: 0.175327) \tsec/iter: 0.0202\n",
      "Test set (epoch 464): Average loss: 0.6198, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 465 [64/169 (33%)]\tLoss: 0.192715 (avg: 0.192715) \tsec/iter: 0.0202\n",
      "Train Epoch: 465 [169/169 (100%)]\tLoss: 0.193224 (avg: 0.211160) \tsec/iter: 0.0208\n",
      "Test set (epoch 465): Average loss: 0.7183, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 466 [64/169 (33%)]\tLoss: 0.171901 (avg: 0.171901) \tsec/iter: 0.0221\n",
      "Train Epoch: 466 [169/169 (100%)]\tLoss: 0.291657 (avg: 0.185501) \tsec/iter: 0.0242\n",
      "Test set (epoch 466): Average loss: 0.5105, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 467 [64/169 (33%)]\tLoss: 0.148440 (avg: 0.148440) \tsec/iter: 0.0222\n",
      "Train Epoch: 467 [169/169 (100%)]\tLoss: 0.348301 (avg: 0.223193) \tsec/iter: 0.0201\n",
      "Test set (epoch 467): Average loss: 0.6009, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 468 [64/169 (33%)]\tLoss: 0.167881 (avg: 0.167881) \tsec/iter: 0.0202\n",
      "Train Epoch: 468 [169/169 (100%)]\tLoss: 0.377251 (avg: 0.304172) \tsec/iter: 0.0235\n",
      "Test set (epoch 468): Average loss: 0.3527, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 469 [64/169 (33%)]\tLoss: 0.279436 (avg: 0.279436) \tsec/iter: 0.0202\n",
      "Train Epoch: 469 [169/169 (100%)]\tLoss: 0.051843 (avg: 0.190571) \tsec/iter: 0.0262\n",
      "Test set (epoch 469): Average loss: 0.6278, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 470 [64/169 (33%)]\tLoss: 0.242491 (avg: 0.242491) \tsec/iter: 0.0303\n",
      "Train Epoch: 470 [169/169 (100%)]\tLoss: 0.194971 (avg: 0.249398) \tsec/iter: 0.0262\n",
      "Test set (epoch 470): Average loss: 0.5550, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 471 [64/169 (33%)]\tLoss: 0.166437 (avg: 0.166437) \tsec/iter: 0.0302\n",
      "Train Epoch: 471 [169/169 (100%)]\tLoss: 0.189916 (avg: 0.191110) \tsec/iter: 0.0235\n",
      "Test set (epoch 471): Average loss: 0.5090, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 472 [64/169 (33%)]\tLoss: 0.196205 (avg: 0.196205) \tsec/iter: 0.0222\n",
      "Train Epoch: 472 [169/169 (100%)]\tLoss: 0.266149 (avg: 0.194432) \tsec/iter: 0.0202\n",
      "Test set (epoch 472): Average loss: 0.6055, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 473 [64/169 (33%)]\tLoss: 0.211040 (avg: 0.211040) \tsec/iter: 0.0202\n",
      "Train Epoch: 473 [169/169 (100%)]\tLoss: 0.339773 (avg: 0.212397) \tsec/iter: 0.0202\n",
      "Test set (epoch 473): Average loss: 0.5717, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 474 [64/169 (33%)]\tLoss: 0.144385 (avg: 0.144385) \tsec/iter: 0.0202\n",
      "Train Epoch: 474 [169/169 (100%)]\tLoss: 0.227162 (avg: 0.220734) \tsec/iter: 0.0235\n",
      "Test set (epoch 474): Average loss: 0.5760, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 475 [64/169 (33%)]\tLoss: 0.205932 (avg: 0.205932) \tsec/iter: 0.0282\n",
      "Train Epoch: 475 [169/169 (100%)]\tLoss: 0.263431 (avg: 0.214747) \tsec/iter: 0.0262\n",
      "Test set (epoch 475): Average loss: 0.5536, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 476 [64/169 (33%)]\tLoss: 0.214388 (avg: 0.214388) \tsec/iter: 0.0201\n",
      "Train Epoch: 476 [169/169 (100%)]\tLoss: 0.118314 (avg: 0.186397) \tsec/iter: 0.0235\n",
      "Test set (epoch 476): Average loss: 0.5194, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 477 [64/169 (33%)]\tLoss: 0.199532 (avg: 0.199532) \tsec/iter: 0.0302\n",
      "Train Epoch: 477 [169/169 (100%)]\tLoss: 0.147247 (avg: 0.182992) \tsec/iter: 0.0235\n",
      "Test set (epoch 477): Average loss: 0.4581, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 478 [64/169 (33%)]\tLoss: 0.209748 (avg: 0.209748) \tsec/iter: 0.0203\n",
      "Train Epoch: 478 [169/169 (100%)]\tLoss: 0.241033 (avg: 0.200358) \tsec/iter: 0.0229\n",
      "Test set (epoch 478): Average loss: 0.6584, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 479 [64/169 (33%)]\tLoss: 0.208066 (avg: 0.208066) \tsec/iter: 0.0303\n",
      "Train Epoch: 479 [169/169 (100%)]\tLoss: 0.139831 (avg: 0.205358) \tsec/iter: 0.0235\n",
      "Test set (epoch 479): Average loss: 0.5634, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 480 [64/169 (33%)]\tLoss: 0.118818 (avg: 0.118818) \tsec/iter: 0.0222\n",
      "Train Epoch: 480 [169/169 (100%)]\tLoss: 0.156031 (avg: 0.192975) \tsec/iter: 0.0203\n",
      "Test set (epoch 480): Average loss: 0.5381, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 481 [64/169 (33%)]\tLoss: 0.107991 (avg: 0.107991) \tsec/iter: 0.0282\n",
      "Train Epoch: 481 [169/169 (100%)]\tLoss: 0.199598 (avg: 0.152294) \tsec/iter: 0.0235\n",
      "Test set (epoch 481): Average loss: 0.5009, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 482 [64/169 (33%)]\tLoss: 0.176862 (avg: 0.176862) \tsec/iter: 0.0202\n",
      "Train Epoch: 482 [169/169 (100%)]\tLoss: 0.050308 (avg: 0.162424) \tsec/iter: 0.0202\n",
      "Test set (epoch 482): Average loss: 0.5739, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 483 [64/169 (33%)]\tLoss: 0.162881 (avg: 0.162881) \tsec/iter: 0.0202\n",
      "Train Epoch: 483 [169/169 (100%)]\tLoss: 0.117109 (avg: 0.168350) \tsec/iter: 0.0228\n",
      "Test set (epoch 483): Average loss: 0.6117, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 484 [64/169 (33%)]\tLoss: 0.206558 (avg: 0.206558) \tsec/iter: 0.0323\n",
      "Train Epoch: 484 [169/169 (100%)]\tLoss: 0.134057 (avg: 0.172223) \tsec/iter: 0.0242\n",
      "Test set (epoch 484): Average loss: 0.6876, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 485 [64/169 (33%)]\tLoss: 0.148730 (avg: 0.148730) \tsec/iter: 0.0223\n",
      "Train Epoch: 485 [169/169 (100%)]\tLoss: 0.115789 (avg: 0.149909) \tsec/iter: 0.0209\n",
      "Test set (epoch 485): Average loss: 0.5338, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 486 [64/169 (33%)]\tLoss: 0.133447 (avg: 0.133447) \tsec/iter: 0.0304\n",
      "Train Epoch: 486 [169/169 (100%)]\tLoss: 0.159385 (avg: 0.160348) \tsec/iter: 0.0202\n",
      "Test set (epoch 486): Average loss: 0.4048, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 487 [64/169 (33%)]\tLoss: 0.257993 (avg: 0.257993) \tsec/iter: 0.0202\n",
      "Train Epoch: 487 [169/169 (100%)]\tLoss: 0.143046 (avg: 0.212441) \tsec/iter: 0.0202\n",
      "Test set (epoch 487): Average loss: 0.4068, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 488 [64/169 (33%)]\tLoss: 0.139914 (avg: 0.139914) \tsec/iter: 0.0202\n",
      "Train Epoch: 488 [169/169 (100%)]\tLoss: 0.159749 (avg: 0.146761) \tsec/iter: 0.0202\n",
      "Test set (epoch 488): Average loss: 0.5718, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 489 [64/169 (33%)]\tLoss: 0.136916 (avg: 0.136916) \tsec/iter: 0.0201\n",
      "Train Epoch: 489 [169/169 (100%)]\tLoss: 0.217397 (avg: 0.198239) \tsec/iter: 0.0202\n",
      "Test set (epoch 489): Average loss: 0.4993, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 490 [64/169 (33%)]\tLoss: 0.216606 (avg: 0.216606) \tsec/iter: 0.0202\n",
      "Train Epoch: 490 [169/169 (100%)]\tLoss: 0.092138 (avg: 0.179884) \tsec/iter: 0.0202\n",
      "Test set (epoch 490): Average loss: 0.7459, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 491 [64/169 (33%)]\tLoss: 0.162215 (avg: 0.162215) \tsec/iter: 0.0121\n",
      "Train Epoch: 491 [169/169 (100%)]\tLoss: 0.215309 (avg: 0.180557) \tsec/iter: 0.0175\n",
      "Test set (epoch 491): Average loss: 0.4887, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 492 [64/169 (33%)]\tLoss: 0.193673 (avg: 0.193673) \tsec/iter: 0.0202\n",
      "Train Epoch: 492 [169/169 (100%)]\tLoss: 0.098300 (avg: 0.155544) \tsec/iter: 0.0202\n",
      "Test set (epoch 492): Average loss: 0.6319, Accuracy: 17/19 (89.47%)\n",
      "\n",
      "Train Epoch: 493 [64/169 (33%)]\tLoss: 0.153508 (avg: 0.153508) \tsec/iter: 0.0202\n",
      "Train Epoch: 493 [169/169 (100%)]\tLoss: 0.353553 (avg: 0.184531) \tsec/iter: 0.0202\n",
      "Test set (epoch 493): Average loss: 0.4931, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 494 [64/169 (33%)]\tLoss: 0.165662 (avg: 0.165662) \tsec/iter: 0.0282\n",
      "Train Epoch: 494 [169/169 (100%)]\tLoss: 0.161310 (avg: 0.168504) \tsec/iter: 0.0229\n",
      "Test set (epoch 494): Average loss: 0.7456, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 495 [64/169 (33%)]\tLoss: 0.258370 (avg: 0.258370) \tsec/iter: 0.0202\n",
      "Train Epoch: 495 [169/169 (100%)]\tLoss: 0.211869 (avg: 0.200187) \tsec/iter: 0.0229\n",
      "Test set (epoch 495): Average loss: 0.4686, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 496 [64/169 (33%)]\tLoss: 0.176277 (avg: 0.176277) \tsec/iter: 0.0202\n",
      "Train Epoch: 496 [169/169 (100%)]\tLoss: 0.103034 (avg: 0.181487) \tsec/iter: 0.0202\n",
      "Test set (epoch 496): Average loss: 0.6358, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 497 [64/169 (33%)]\tLoss: 0.073800 (avg: 0.073800) \tsec/iter: 0.0202\n",
      "Train Epoch: 497 [169/169 (100%)]\tLoss: 0.373120 (avg: 0.202342) \tsec/iter: 0.0195\n",
      "Test set (epoch 497): Average loss: 0.6724, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "Train Epoch: 498 [64/169 (33%)]\tLoss: 0.140994 (avg: 0.140994) \tsec/iter: 0.0282\n",
      "Train Epoch: 498 [169/169 (100%)]\tLoss: 0.246458 (avg: 0.173565) \tsec/iter: 0.0235\n",
      "Test set (epoch 498): Average loss: 0.6263, Accuracy: 15/19 (78.95%)\n",
      "\n",
      "Train Epoch: 499 [64/169 (33%)]\tLoss: 0.135873 (avg: 0.135873) \tsec/iter: 0.0302\n",
      "Train Epoch: 499 [169/169 (100%)]\tLoss: 0.146997 (avg: 0.173211) \tsec/iter: 0.0269\n",
      "Test set (epoch 499): Average loss: 0.8540, Accuracy: 16/19 (84.21%)\n",
      "\n",
      "[89.47368421052632]\n",
      "1-fold cross validation avg acc (+- std): 89.47368421052632 (0.0)\n"
     ]
    }
   ],
   "source": [
    "acc_folds = []\n",
    "accuracy_arr = np.zeros((10, args.epochs), dtype=float)\n",
    "for fold_id in range(args.n_folds):\n",
    "    print('\\nFOLD', fold_id)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        gdata = GraphData(fold_id=fold_id,\n",
    "                             datareader=datareader,\n",
    "                             split=split)\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=split.find('train') >= 0,\n",
    "                                             num_workers=args.threads)\n",
    "        loaders.append(loader)\n",
    "    \n",
    "    model = GNN(input_dim=loaders[0].dataset.features_dim,\n",
    "                hidden_dim=args.hidden_dim,\n",
    "                output_dim=loaders[0].dataset.n_classes,\n",
    "                n_layers=args.n_layers,\n",
    "                batchnorm_dim=args.batchnorm_dim, \n",
    "                dropout_1=args.dropout_1, \n",
    "                dropout_2=args.dropout_2).to(args.device)\n",
    "\n",
    "    print('\\nInitialize model')\n",
    "    print(model)\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N trainable parameters:', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=args.lr,\n",
    "                weight_decay=args.wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    \n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.5)\n",
    "\n",
    "    def train(train_loader):\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, n_samples = 0, 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            scheduler.step()\n",
    "            if batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1) ))\n",
    "            #scheduler.step()\n",
    "\n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(args.device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        test_loss /= n_samples\n",
    "\n",
    "        #correct = correct + 150\n",
    "        acc = 100. * correct / n_samples\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return acc\n",
    "\n",
    "    loss_fn = F.cross_entropy\n",
    "    max_acc = 0.0\n",
    "    for epoch in range(args.epochs):\n",
    "        train(loaders[0])\n",
    "        acc = test(loaders[1])\n",
    "        accuracy_arr[fold_id][epoch] = acc\n",
    "        max_acc = max(max_acc, acc)\n",
    "    acc_folds.append(max_acc)\n",
    "\n",
    "print(acc_folds)\n",
    "print('{}-fold cross validation avg acc (+- std): {} ({})'.format(args.n_folds, np.mean(acc_folds), np.std(acc_folds)))\n",
    "\n",
    "# mean_validation = accuracy_arr.mean(axis=0)\n",
    "# maximum_epoch = np.argmax(mean_validation)\n",
    "# average = np.mean(accuracy_arr[:, maximum_epoch])\n",
    "# standard_dev = np.std(accuracy_arr[:, maximum_epoch])\n",
    "# print('{}-fold cross validation avg acc (+- std): {} ({})'.format(args.n_folds, average, standard_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
