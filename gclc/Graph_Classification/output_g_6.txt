Loading data
Using device cuda
[DEBUG] 数据集绝对路径: /home/xuke/zxy_code/gclc/Graph_Classification/data/APPLICATIONS
[DEBUG] 路径是否存在? True
Directory /home/xuke/zxy_code/gclc/Graph_Classification/data/APPLICATIONS/ exists. Files: ['APPLICATIONS_A.txt', 'APPLICATIONS_graph_indicator.txt', 'APPLICATIONS_graph_labels.txt', 'APPLICATIONS_edge_attribute.txt', 'APPLICATIONS_node_labels.txt']
Reading graph nodes and relations...
Reading node features...
Reading adjacency list and edge features...
Reading targets...
Data loading complete.
Class 1: 1000 samples
Class 2: 1000 samples
Class 3: 1000 samples
Class 4: 1000 samples
Class 5: 1000 samples
Class 6: 1000 samples
Class 7: 1000 samples
Class 8: 1000 samples
Class 9: 1000 samples
Class 10: 1000 samples
Class 11: 1000 samples
Class 12: 1000 samples
Class 13: 1000 samples
Class 14: 1000 samples
Class 15: 1000 samples
Class 16: 708 samples
Class 17: 1000 samples
Class 18: 1000 samples
Class 19: 1000 samples
Class 20: 1000 samples
Class 21: 1000 samples
Data samped complete.
feature_onehot: 1461
N nodes avg/std/min/max: 	26.04/5.07/16/30
N edges avg/std/min/max: 	34.40/7.33/15/48
Node degree avg/std/min/max: 	1.90/0.74/0/4
Node features dim: 		1461
N classes: 			21
Classes: 			[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Class 0: 			1000 samples
Class 1: 			1000 samples
Class 2: 			1000 samples
Class 3: 			1000 samples
Class 4: 			1000 samples
Class 5: 			1000 samples
Class 6: 			1000 samples
Class 7: 			1000 samples
Class 8: 			1000 samples
Class 9: 			1000 samples
Class 10: 			1000 samples
Class 11: 			1000 samples
Class 12: 			1000 samples
Class 13: 			1000 samples
Class 14: 			1000 samples
Class 15: 			708 samples
Class 16: 			1000 samples
Class 17: 			1000 samples
Class 18: 			1000 samples
Class 19: 			1000 samples
Class 20: 			1000 samples
feature 54, count 128340/539235
feature 55, count 14/539235
feature 56, count 8/539235
feature 57, count 1/539235
feature 58, count 40/539235
feature 59, count 1/539235
feature 60, count 28661/539235
feature 61, count 10/539235
feature 62, count 38/539235
feature 63, count 656/539235
feature 64, count 28/539235
feature 65, count 1/539235
feature 66, count 60267/539235
feature 67, count 305/539235
feature 68, count 38/539235
feature 69, count 36/539235
feature 70, count 120/539235
feature 71, count 20/539235
feature 72, count 7/539235
feature 73, count 12/539235
feature 74, count 3844/539235
feature 75, count 248/539235
feature 76, count 17/539235
feature 77, count 209/539235
feature 78, count 13667/539235
feature 79, count 4/539235
feature 80, count 363/539235
feature 81, count 168/539235
feature 82, count 89/539235
feature 83, count 7/539235
feature 84, count 662/539235
feature 85, count 1440/539235
feature 86, count 54/539235
feature 87, count 58/539235
feature 88, count 673/539235
feature 89, count 123/539235
feature 90, count 384/539235
feature 91, count 21/539235
feature 92, count 6746/539235
feature 93, count 113/539235
feature 94, count 325/539235
feature 95, count 22/539235
feature 96, count 2868/539235
feature 97, count 1741/539235
feature 98, count 21/539235
feature 99, count 713/539235
feature 100, count 529/539235
feature 101, count 307/539235
feature 102, count 121/539235
feature 103, count 5/539235
feature 104, count 999/539235
feature 105, count 1032/539235
feature 106, count 27/539235
feature 107, count 312/539235
feature 108, count 103/539235
feature 109, count 214/539235
feature 110, count 18/539235
feature 111, count 9/539235
feature 112, count 101/539235
feature 113, count 81/539235
feature 114, count 9/539235
feature 115, count 674/539235
feature 116, count 415/539235
feature 117, count 527/539235
feature 118, count 18/539235
feature 119, count 65/539235
feature 120, count 305/539235
feature 121, count 81/539235
feature 122, count 1197/539235
feature 123, count 1071/539235
feature 124, count 434/539235
feature 125, count 374/539235
feature 126, count 410/539235
feature 127, count 157/539235
feature 128, count 136/539235
feature 129, count 161/539235
feature 130, count 1570/539235
feature 131, count 79/539235
feature 132, count 2125/539235
feature 133, count 337/539235
feature 134, count 1250/539235
feature 135, count 511/539235
feature 136, count 311/539235
feature 137, count 508/539235
feature 138, count 657/539235
feature 139, count 2074/539235
feature 140, count 210/539235
feature 141, count 997/539235
feature 142, count 728/539235
feature 143, count 208/539235
feature 144, count 485/539235
feature 145, count 2691/539235
feature 146, count 1321/539235
feature 147, count 3861/539235
feature 148, count 24/539235
feature 149, count 91/539235
feature 150, count 530/539235
feature 151, count 417/539235
feature 152, count 190/539235
feature 153, count 3335/539235
feature 154, count 238/539235
feature 155, count 196/539235
feature 156, count 197/539235
feature 157, count 979/539235
feature 158, count 986/539235
feature 159, count 1225/539235
feature 160, count 68/539235
feature 161, count 61/539235
feature 162, count 51/539235
feature 163, count 203/539235
feature 164, count 107/539235
feature 165, count 671/539235
feature 166, count 84/539235
feature 167, count 291/539235
feature 168, count 9/539235
feature 169, count 39/539235
feature 170, count 65/539235
feature 171, count 72/539235
feature 172, count 321/539235
feature 173, count 842/539235
feature 174, count 23/539235
feature 175, count 128/539235
feature 176, count 23/539235
feature 177, count 28/539235
feature 178, count 298/539235
feature 179, count 92/539235
feature 180, count 2924/539235
feature 181, count 96/539235
feature 182, count 91/539235
feature 183, count 95/539235
feature 184, count 231/539235
feature 185, count 129/539235
feature 186, count 37/539235
feature 187, count 44/539235
feature 188, count 21/539235
feature 189, count 41/539235
feature 190, count 74/539235
feature 191, count 69/539235
feature 192, count 986/539235
feature 193, count 64/539235
feature 194, count 35/539235
feature 195, count 386/539235
feature 196, count 56/539235
feature 197, count 24/539235
feature 198, count 16/539235
feature 199, count 239/539235
feature 200, count 24/539235
feature 201, count 260/539235
feature 202, count 178/539235
feature 203, count 119/539235
feature 204, count 52/539235
feature 205, count 54/539235
feature 206, count 25/539235
feature 207, count 303/539235
feature 208, count 35/539235
feature 209, count 416/539235
feature 210, count 57/539235
feature 211, count 133/539235
feature 212, count 106/539235
feature 213, count 353/539235
feature 214, count 29/539235
feature 215, count 65/539235
feature 216, count 33/539235
feature 217, count 65/539235
feature 218, count 20/539235
feature 219, count 22/539235
feature 220, count 50/539235
feature 221, count 47/539235
feature 222, count 94/539235
feature 223, count 14/539235
feature 224, count 27/539235
feature 225, count 20/539235
feature 226, count 5/539235
feature 227, count 16/539235
feature 228, count 135/539235
feature 229, count 338/539235
feature 230, count 186/539235
feature 231, count 22/539235
feature 232, count 64/539235
feature 233, count 53/539235
feature 234, count 58/539235
feature 235, count 116/539235
feature 236, count 58/539235
feature 237, count 99/539235
feature 238, count 19/539235
feature 239, count 17/539235
feature 240, count 47/539235
feature 241, count 9/539235
feature 242, count 11/539235
feature 243, count 33/539235
feature 244, count 11/539235
feature 245, count 17/539235
feature 246, count 23/539235
feature 247, count 174/539235
feature 248, count 22/539235
feature 249, count 23/539235
feature 250, count 43/539235
feature 251, count 62/539235
feature 252, count 83/539235
feature 253, count 44/539235
feature 254, count 35/539235
feature 255, count 14/539235
feature 256, count 36/539235
feature 257, count 40/539235
feature 258, count 53/539235
feature 259, count 113/539235
feature 260, count 120/539235
feature 261, count 55/539235
feature 262, count 65/539235
feature 263, count 27/539235
feature 264, count 166/539235
feature 265, count 49/539235
feature 266, count 337/539235
feature 267, count 24/539235
feature 268, count 19/539235
feature 269, count 44/539235
feature 270, count 40/539235
feature 271, count 20/539235
feature 272, count 29/539235
feature 273, count 37/539235
feature 274, count 92/539235
feature 275, count 104/539235
feature 276, count 72/539235
feature 277, count 30/539235
feature 278, count 21/539235
feature 279, count 101/539235
feature 280, count 93/539235
feature 281, count 197/539235
feature 282, count 54/539235
feature 283, count 43/539235
feature 284, count 126/539235
feature 285, count 216/539235
feature 286, count 18/539235
feature 287, count 10/539235
feature 288, count 31/539235
feature 289, count 31/539235
feature 290, count 36/539235
feature 291, count 33/539235
feature 292, count 108/539235
feature 293, count 17/539235
feature 294, count 8/539235
feature 295, count 27/539235
feature 296, count 96/539235
feature 297, count 21/539235
feature 298, count 36/539235
feature 299, count 40/539235
feature 300, count 29/539235
feature 301, count 56/539235
feature 302, count 46/539235
feature 303, count 38/539235
feature 304, count 1345/539235
feature 305, count 29/539235
feature 306, count 17/539235
feature 307, count 9/539235
feature 308, count 51/539235
feature 309, count 23/539235
feature 310, count 40/539235
feature 311, count 66/539235
feature 312, count 3354/539235
feature 313, count 12/539235
feature 314, count 139/539235
feature 315, count 45/539235
feature 316, count 93/539235
feature 317, count 168/539235
feature 318, count 183/539235
feature 319, count 27/539235
feature 320, count 99/539235
feature 321, count 57/539235
feature 322, count 49/539235
feature 323, count 70/539235
feature 324, count 718/539235
feature 325, count 413/539235
feature 326, count 64/539235
feature 327, count 688/539235
feature 328, count 2165/539235
feature 329, count 120/539235
feature 330, count 228/539235
feature 331, count 345/539235
feature 332, count 52/539235
feature 333, count 294/539235
feature 334, count 138/539235
feature 335, count 145/539235
feature 336, count 229/539235
feature 337, count 460/539235
feature 338, count 365/539235
feature 339, count 215/539235
feature 340, count 981/539235
feature 341, count 528/539235
feature 342, count 84/539235
feature 343, count 281/539235
feature 344, count 858/539235
feature 345, count 89/539235
feature 346, count 62/539235
feature 347, count 25/539235
feature 348, count 62/539235
feature 349, count 575/539235
feature 350, count 91/539235
feature 351, count 157/539235
feature 352, count 164/539235
feature 353, count 964/539235
feature 354, count 94/539235
feature 355, count 56/539235
feature 356, count 348/539235
feature 357, count 553/539235
feature 358, count 60/539235
feature 359, count 55/539235
feature 360, count 96/539235
feature 361, count 63/539235
feature 362, count 84/539235
feature 363, count 391/539235
feature 364, count 227/539235
feature 365, count 240/539235
feature 366, count 141/539235
feature 367, count 39/539235
feature 368, count 77/539235
feature 369, count 116/539235
feature 370, count 94/539235
feature 371, count 148/539235
feature 372, count 151/539235
feature 373, count 163/539235
feature 374, count 203/539235
feature 375, count 202/539235
feature 376, count 97/539235
feature 377, count 169/539235
feature 378, count 161/539235
feature 379, count 57/539235
feature 380, count 57/539235
feature 381, count 48/539235
feature 382, count 61/539235
feature 383, count 215/539235
feature 384, count 140/539235
feature 385, count 81/539235
feature 386, count 53/539235
feature 387, count 131/539235
feature 388, count 32/539235
feature 389, count 89/539235
feature 390, count 47/539235
feature 391, count 70/539235
feature 392, count 916/539235
feature 393, count 269/539235
feature 394, count 345/539235
feature 395, count 221/539235
feature 396, count 150/539235
feature 397, count 130/539235
feature 398, count 185/539235
feature 399, count 95/539235
feature 400, count 115/539235
feature 401, count 194/539235
feature 402, count 29/539235
feature 403, count 164/539235
feature 404, count 109/539235
feature 405, count 354/539235
feature 406, count 164/539235
feature 407, count 86/539235
feature 408, count 69/539235
feature 409, count 25/539235
feature 410, count 149/539235
feature 411, count 248/539235
feature 412, count 120/539235
feature 413, count 91/539235
feature 414, count 209/539235
feature 415, count 54/539235
feature 416, count 97/539235
feature 417, count 71/539235
feature 418, count 55/539235
feature 419, count 88/539235
feature 420, count 30/539235
feature 421, count 38/539235
feature 422, count 63/539235
feature 423, count 56/539235
feature 424, count 91/539235
feature 425, count 100/539235
feature 426, count 30/539235
feature 427, count 52/539235
feature 428, count 29/539235
feature 429, count 48/539235
feature 430, count 224/539235
feature 431, count 128/539235
feature 432, count 101/539235
feature 433, count 43/539235
feature 434, count 392/539235
feature 435, count 103/539235
feature 436, count 77/539235
feature 437, count 93/539235
feature 438, count 64/539235
feature 439, count 55/539235
feature 440, count 167/539235
feature 441, count 133/539235
feature 442, count 82/539235
feature 443, count 74/539235
feature 444, count 50/539235
feature 445, count 100/539235
feature 446, count 39/539235
feature 447, count 15/539235
feature 448, count 24/539235
feature 449, count 129/539235
feature 450, count 37/539235
feature 451, count 36/539235
feature 452, count 41/539235
feature 453, count 40/539235
feature 454, count 28/539235
feature 455, count 24/539235
feature 456, count 7/539235
feature 457, count 17/539235
feature 458, count 13/539235
feature 459, count 19/539235
feature 460, count 17/539235
feature 461, count 16/539235
feature 462, count 27/539235
feature 463, count 27/539235
feature 464, count 31/539235
feature 465, count 123/539235
feature 466, count 42/539235
feature 467, count 61/539235
feature 468, count 50/539235
feature 469, count 37/539235
feature 470, count 55/539235
feature 471, count 113/539235
feature 472, count 15/539235
feature 473, count 15/539235
feature 474, count 17/539235
feature 475, count 77/539235
feature 476, count 55/539235
feature 477, count 63/539235
feature 478, count 9/539235
feature 479, count 14/539235
feature 480, count 155/539235
feature 481, count 31/539235
feature 482, count 26/539235
feature 483, count 59/539235
feature 484, count 35/539235
feature 485, count 128/539235
feature 486, count 57/539235
feature 487, count 216/539235
feature 488, count 31/539235
feature 489, count 192/539235
feature 490, count 20/539235
feature 491, count 30/539235
feature 492, count 34/539235
feature 493, count 9/539235
feature 494, count 2/539235
feature 495, count 41/539235
feature 496, count 95/539235
feature 497, count 51/539235
feature 498, count 41/539235
feature 499, count 96/539235
feature 500, count 39/539235
feature 501, count 50/539235
feature 502, count 61/539235
feature 503, count 62/539235
feature 504, count 47/539235
feature 505, count 56/539235
feature 506, count 52/539235
feature 507, count 42/539235
feature 508, count 47/539235
feature 509, count 170/539235
feature 510, count 13/539235
feature 511, count 15/539235
feature 512, count 20/539235
feature 513, count 18/539235
feature 514, count 28/539235
feature 515, count 27/539235
feature 516, count 23/539235
feature 517, count 79/539235
feature 518, count 45/539235
feature 519, count 3/539235
feature 520, count 5/539235
feature 521, count 7/539235
feature 522, count 50/539235
feature 523, count 56/539235
feature 524, count 131/539235
feature 525, count 48/539235
feature 526, count 19/539235
feature 527, count 119/539235
feature 528, count 32/539235
feature 529, count 10/539235
feature 530, count 55/539235
feature 531, count 16/539235
feature 532, count 19/539235
feature 533, count 24/539235
feature 534, count 76/539235
feature 535, count 8/539235
feature 536, count 14/539235
feature 537, count 54/539235
feature 538, count 15/539235
feature 539, count 46/539235
feature 540, count 46/539235
feature 541, count 123/539235
feature 542, count 403/539235
feature 543, count 358/539235
feature 544, count 101/539235
feature 545, count 58/539235
feature 546, count 82/539235
feature 547, count 18/539235
feature 548, count 36/539235
feature 549, count 27/539235
feature 550, count 29/539235
feature 551, count 129/539235
feature 552, count 197/539235
feature 553, count 67/539235
feature 554, count 28/539235
feature 555, count 40/539235
feature 556, count 33/539235
feature 557, count 16/539235
feature 558, count 294/539235
feature 559, count 16/539235
feature 560, count 16/539235
feature 561, count 38/539235
feature 562, count 13/539235
feature 563, count 14/539235
feature 564, count 15/539235
feature 565, count 10/539235
feature 566, count 26/539235
feature 567, count 6/539235
feature 568, count 4/539235
feature 569, count 11/539235
feature 570, count 25/539235
feature 571, count 8477/539235
feature 572, count 24/539235
feature 573, count 23/539235
feature 574, count 7/539235
feature 575, count 29/539235
feature 576, count 33/539235
feature 577, count 94/539235
feature 578, count 36/539235
feature 579, count 70/539235
feature 580, count 31/539235
feature 581, count 77/539235
feature 582, count 16/539235
feature 583, count 3768/539235
feature 584, count 50/539235
feature 585, count 66/539235
feature 586, count 99/539235
feature 587, count 164/539235
feature 588, count 253/539235
feature 589, count 191/539235
feature 590, count 51/539235
feature 591, count 244/539235
feature 592, count 20/539235
feature 593, count 90/539235
feature 594, count 187/539235
feature 595, count 39/539235
feature 596, count 24/539235
feature 597, count 77/539235
feature 598, count 14/539235
feature 599, count 337/539235
feature 600, count 25/539235
feature 601, count 75/539235
feature 602, count 36/539235
feature 603, count 153/539235
feature 604, count 25/539235
feature 605, count 37/539235
feature 606, count 164/539235
feature 607, count 138/539235
feature 608, count 384/539235
feature 609, count 39/539235
feature 610, count 61/539235
feature 611, count 13/539235
feature 612, count 16/539235
feature 613, count 149/539235
feature 614, count 100/539235
feature 615, count 63/539235
feature 616, count 12/539235
feature 617, count 39/539235
feature 618, count 39/539235
feature 619, count 69/539235
feature 620, count 80/539235
feature 621, count 122/539235
feature 622, count 36/539235
feature 623, count 207/539235
feature 624, count 45/539235
feature 625, count 120/539235
feature 626, count 34/539235
feature 627, count 131/539235
feature 628, count 27/539235
feature 629, count 120/539235
feature 630, count 49/539235
feature 631, count 58/539235
feature 632, count 59/539235
feature 633, count 183/539235
feature 634, count 32/539235
feature 635, count 135/539235
feature 636, count 84/539235
feature 637, count 132/539235
feature 638, count 123/539235
feature 639, count 69/539235
feature 640, count 43/539235
feature 641, count 28/539235
feature 642, count 88/539235
feature 643, count 19/539235
feature 644, count 6/539235
feature 645, count 133/539235
feature 646, count 544/539235
feature 647, count 47/539235
feature 648, count 11/539235
feature 649, count 6/539235
feature 650, count 9/539235
feature 651, count 24/539235
feature 652, count 18/539235
feature 653, count 34/539235
feature 654, count 213/539235
feature 655, count 20/539235
feature 656, count 70/539235
feature 657, count 15/539235
feature 658, count 95/539235
feature 659, count 65/539235
feature 660, count 50/539235
feature 661, count 45/539235
feature 662, count 83/539235
feature 663, count 238/539235
feature 664, count 275/539235
feature 665, count 202/539235
feature 666, count 75/539235
feature 667, count 130/539235
feature 668, count 32/539235
feature 669, count 19/539235
feature 670, count 12/539235
feature 671, count 10/539235
feature 672, count 134/539235
feature 673, count 10/539235
feature 674, count 28/539235
feature 675, count 15/539235
feature 676, count 39/539235
feature 677, count 226/539235
feature 678, count 329/539235
feature 679, count 127/539235
feature 680, count 46/539235
feature 681, count 33/539235
feature 682, count 46/539235
feature 683, count 198/539235
feature 684, count 37/539235
feature 685, count 35/539235
feature 686, count 295/539235
feature 687, count 14/539235
feature 688, count 12/539235
feature 689, count 27/539235
feature 690, count 66/539235
feature 691, count 38/539235
feature 692, count 57/539235
feature 693, count 19/539235
feature 694, count 59/539235
feature 695, count 21/539235
feature 696, count 237/539235
feature 697, count 278/539235
feature 698, count 34/539235
feature 699, count 24/539235
feature 700, count 26/539235
feature 701, count 12/539235
feature 702, count 12/539235
feature 703, count 21/539235
feature 704, count 4/539235
feature 705, count 14/539235
feature 706, count 49/539235
feature 707, count 20/539235
feature 708, count 27/539235
feature 709, count 37/539235
feature 710, count 23/539235
feature 711, count 34/539235
feature 712, count 9/539235
feature 713, count 19/539235
feature 714, count 69/539235
feature 715, count 21/539235
feature 716, count 25/539235
feature 717, count 323/539235
feature 718, count 119/539235
feature 719, count 17/539235
feature 720, count 18/539235
feature 721, count 12/539235
feature 722, count 11/539235
feature 723, count 51/539235
feature 724, count 225/539235
feature 725, count 40/539235
feature 726, count 210/539235
feature 727, count 19/539235
feature 728, count 71/539235
feature 729, count 58/539235
feature 730, count 16/539235
feature 731, count 10/539235
feature 732, count 10/539235
feature 733, count 19/539235
feature 734, count 11/539235
feature 735, count 8/539235
feature 736, count 8/539235
feature 737, count 12/539235
feature 738, count 6/539235
feature 739, count 459/539235
feature 740, count 9/539235
feature 741, count 10/539235
feature 742, count 16/539235
feature 743, count 7/539235
feature 744, count 13/539235
feature 745, count 44/539235
feature 746, count 17/539235
feature 747, count 441/539235
feature 748, count 40/539235
feature 749, count 67/539235
feature 750, count 68/539235
feature 751, count 345/539235
feature 752, count 34/539235
feature 753, count 16/539235
feature 754, count 61/539235
feature 755, count 28/539235
feature 756, count 59/539235
feature 757, count 60/539235
feature 758, count 66/539235
feature 759, count 41/539235
feature 760, count 45/539235
feature 761, count 29/539235
feature 762, count 58/539235
feature 763, count 18/539235
feature 764, count 19/539235
feature 765, count 17/539235
feature 766, count 52/539235
feature 767, count 133/539235
feature 768, count 20/539235
feature 769, count 90/539235
feature 770, count 124/539235
feature 771, count 89/539235
feature 772, count 188/539235
feature 773, count 47/539235
feature 774, count 24/539235
feature 775, count 26/539235
feature 776, count 25/539235
feature 777, count 58/539235
feature 778, count 46/539235
feature 779, count 18/539235
feature 780, count 10/539235
feature 781, count 19/539235
feature 782, count 21/539235
feature 783, count 8/539235
feature 784, count 17/539235
feature 785, count 9/539235
feature 786, count 20/539235
feature 787, count 15/539235
feature 788, count 11/539235
feature 789, count 17/539235
feature 790, count 23/539235
feature 791, count 37/539235
feature 792, count 415/539235
feature 793, count 63/539235
feature 794, count 35/539235
feature 795, count 45/539235
feature 796, count 102/539235
feature 797, count 28/539235
feature 798, count 51/539235
feature 799, count 109/539235
feature 800, count 66/539235
feature 801, count 39/539235
feature 802, count 31/539235
feature 803, count 32/539235
feature 804, count 31/539235
feature 805, count 42/539235
feature 806, count 13/539235
feature 807, count 50/539235
feature 808, count 49/539235
feature 809, count 42/539235
feature 810, count 40/539235
feature 811, count 56/539235
feature 812, count 28/539235
feature 813, count 22/539235
feature 814, count 30/539235
feature 815, count 30/539235
feature 816, count 16/539235
feature 817, count 69/539235
feature 818, count 24/539235
feature 819, count 28/539235
feature 820, count 27/539235
feature 821, count 13/539235
feature 822, count 3/539235
feature 823, count 49/539235
feature 824, count 14/539235
feature 825, count 11/539235
feature 826, count 5/539235
feature 827, count 13/539235
feature 828, count 56/539235
feature 829, count 10/539235
feature 830, count 12/539235
feature 831, count 17/539235
feature 832, count 166/539235
feature 833, count 48/539235
feature 834, count 16/539235
feature 835, count 7/539235
feature 836, count 2/539235
feature 837, count 10/539235
feature 838, count 10/539235
feature 839, count 17/539235
feature 840, count 7/539235
feature 841, count 1/539235
feature 842, count 8/539235
feature 843, count 11/539235
feature 844, count 9/539235
feature 845, count 11/539235
feature 846, count 11/539235
feature 847, count 38/539235
feature 848, count 6/539235
feature 849, count 8/539235
feature 850, count 7/539235
feature 851, count 2/539235
feature 852, count 3/539235
feature 853, count 7/539235
feature 854, count 24/539235
feature 855, count 11/539235
feature 856, count 2/539235
feature 857, count 3/539235
feature 858, count 10/539235
feature 859, count 2/539235
feature 860, count 4/539235
feature 861, count 19/539235
feature 862, count 12/539235
feature 863, count 3/539235
feature 864, count 8/539235
feature 865, count 6/539235
feature 866, count 16/539235
feature 867, count 2/539235
feature 868, count 16/539235
feature 869, count 2/539235
feature 870, count 4/539235
feature 871, count 14/539235
feature 872, count 22/539235
feature 873, count 3/539235
feature 874, count 6/539235
feature 875, count 7/539235
feature 876, count 106/539235
feature 877, count 149/539235
feature 878, count 42/539235
feature 879, count 42/539235
feature 880, count 100/539235
feature 881, count 17/539235
feature 882, count 1/539235
feature 883, count 9/539235
feature 884, count 11/539235
feature 885, count 53/539235
feature 886, count 68/539235
feature 887, count 46/539235
feature 888, count 79/539235
feature 889, count 42/539235
feature 890, count 86/539235
feature 891, count 17/539235
feature 892, count 30/539235
feature 893, count 14/539235
feature 894, count 10/539235
feature 895, count 154/539235
feature 896, count 69/539235
feature 897, count 72/539235
feature 898, count 9/539235
feature 899, count 10/539235
feature 900, count 10/539235
feature 901, count 5/539235
feature 902, count 8/539235
feature 903, count 56/539235
feature 904, count 36/539235
feature 905, count 44/539235
feature 906, count 18/539235
feature 907, count 23/539235
feature 908, count 12/539235
feature 909, count 46/539235
feature 910, count 6/539235
feature 911, count 125/539235
feature 912, count 5/539235
feature 913, count 8/539235
feature 914, count 8/539235
feature 915, count 14/539235
feature 916, count 17/539235
feature 917, count 5/539235
feature 918, count 8/539235
feature 919, count 4/539235
feature 920, count 13/539235
feature 921, count 6/539235
feature 922, count 21/539235
feature 923, count 3/539235
feature 924, count 4/539235
feature 925, count 4/539235
feature 926, count 4/539235
feature 927, count 10/539235
feature 928, count 88/539235
feature 930, count 5/539235
feature 931, count 6/539235
feature 932, count 29/539235
feature 933, count 7/539235
feature 934, count 16/539235
feature 935, count 59/539235
feature 936, count 5/539235
feature 937, count 3/539235
feature 938, count 3/539235
feature 939, count 4/539235
feature 940, count 5/539235
feature 941, count 6/539235
feature 942, count 1/539235
feature 943, count 17/539235
feature 944, count 33/539235
feature 945, count 10/539235
feature 946, count 6/539235
feature 947, count 6/539235
feature 948, count 6/539235
feature 949, count 8/539235
feature 950, count 49/539235
feature 951, count 9/539235
feature 952, count 14/539235
feature 953, count 11/539235
feature 954, count 6/539235
feature 955, count 4/539235
feature 956, count 3/539235
feature 957, count 11/539235
feature 958, count 8/539235
feature 959, count 8/539235
feature 960, count 22/539235
feature 961, count 12/539235
feature 962, count 8/539235
feature 963, count 10/539235
feature 964, count 22/539235
feature 965, count 15/539235
feature 966, count 20/539235
feature 967, count 15/539235
feature 968, count 12/539235
feature 969, count 7/539235
feature 970, count 8/539235
feature 971, count 96/539235
feature 972, count 32/539235
feature 973, count 8/539235
feature 974, count 4/539235
feature 975, count 20/539235
feature 976, count 8/539235
feature 977, count 6/539235
feature 978, count 14/539235
feature 979, count 14/539235
feature 980, count 14/539235
feature 981, count 11/539235
feature 982, count 15/539235
feature 983, count 7/539235
feature 984, count 18/539235
feature 985, count 14/539235
feature 986, count 24/539235
feature 987, count 7/539235
feature 988, count 36/539235
feature 989, count 21/539235
feature 990, count 28/539235
feature 991, count 15/539235
feature 992, count 7/539235
feature 993, count 7/539235
feature 994, count 4/539235
feature 995, count 33/539235
feature 996, count 7/539235
feature 997, count 9/539235
feature 998, count 45/539235
feature 999, count 225/539235
feature 1000, count 7/539235
feature 1001, count 33/539235
feature 1002, count 23/539235
feature 1003, count 14/539235
feature 1004, count 21/539235
feature 1005, count 107/539235
feature 1006, count 17/539235
feature 1007, count 20/539235
feature 1008, count 16/539235
feature 1009, count 12/539235
feature 1010, count 18/539235
feature 1011, count 20/539235
feature 1012, count 11/539235
feature 1013, count 17/539235
feature 1014, count 31/539235
feature 1015, count 53/539235
feature 1016, count 71/539235
feature 1017, count 15/539235
feature 1018, count 4/539235
feature 1019, count 37/539235
feature 1020, count 11/539235
feature 1021, count 19/539235
feature 1022, count 29/539235
feature 1023, count 8/539235
feature 1024, count 5/539235
feature 1025, count 28/539235
feature 1026, count 14/539235
feature 1027, count 7/539235
feature 1028, count 15/539235
feature 1029, count 50/539235
feature 1030, count 20/539235
feature 1031, count 28/539235
feature 1032, count 33/539235
feature 1033, count 10/539235
feature 1034, count 16/539235
feature 1035, count 17/539235
feature 1036, count 17/539235
feature 1037, count 17/539235
feature 1038, count 23/539235
feature 1039, count 22/539235
feature 1040, count 44/539235
feature 1041, count 44/539235
feature 1042, count 34/539235
feature 1043, count 46/539235
feature 1044, count 38/539235
feature 1045, count 29/539235
feature 1046, count 31/539235
feature 1047, count 34/539235
feature 1048, count 30/539235
feature 1049, count 34/539235
feature 1050, count 31/539235
feature 1051, count 32/539235
feature 1052, count 22/539235
feature 1053, count 35/539235
feature 1054, count 23/539235
feature 1055, count 27/539235
feature 1056, count 11/539235
feature 1057, count 17/539235
feature 1058, count 21/539235
feature 1059, count 10/539235
feature 1060, count 10/539235
feature 1061, count 42/539235
feature 1062, count 67/539235
feature 1063, count 4/539235
feature 1064, count 8/539235
feature 1065, count 19/539235
feature 1066, count 33/539235
feature 1067, count 38/539235
feature 1068, count 44/539235
feature 1069, count 49/539235
feature 1070, count 41/539235
feature 1071, count 15/539235
feature 1072, count 14/539235
feature 1073, count 21/539235
feature 1074, count 14/539235
feature 1075, count 23/539235
feature 1076, count 104/539235
feature 1077, count 9/539235
feature 1078, count 39/539235
feature 1079, count 9/539235
feature 1080, count 17/539235
feature 1081, count 15/539235
feature 1082, count 13/539235
feature 1083, count 35/539235
feature 1084, count 14/539235
feature 1085, count 54/539235
feature 1086, count 10/539235
feature 1087, count 8/539235
feature 1088, count 22/539235
feature 1089, count 11/539235
feature 1090, count 6/539235
feature 1091, count 9/539235
feature 1092, count 27/539235
feature 1093, count 57/539235
feature 1094, count 65/539235
feature 1095, count 54/539235
feature 1096, count 36/539235
feature 1097, count 8/539235
feature 1098, count 36/539235
feature 1099, count 20/539235
feature 1100, count 54/539235
feature 1101, count 9/539235
feature 1102, count 11/539235
feature 1103, count 6/539235
feature 1104, count 6/539235
feature 1105, count 15/539235
feature 1106, count 9/539235
feature 1107, count 7/539235
feature 1108, count 20/539235
feature 1109, count 10/539235
feature 1110, count 12/539235
feature 1111, count 12/539235
feature 1112, count 14/539235
feature 1113, count 32/539235
feature 1114, count 101/539235
feature 1115, count 88/539235
feature 1116, count 9/539235
feature 1117, count 21/539235
feature 1118, count 23/539235
feature 1119, count 37/539235
feature 1120, count 22/539235
feature 1121, count 53/539235
feature 1122, count 9/539235
feature 1123, count 32/539235
feature 1124, count 11/539235
feature 1125, count 47/539235
feature 1126, count 68/539235
feature 1127, count 101/539235
feature 1128, count 27/539235
feature 1129, count 43/539235
feature 1130, count 10/539235
feature 1131, count 64/539235
feature 1132, count 22/539235
feature 1133, count 58/539235
feature 1134, count 11/539235
feature 1135, count 125/539235
feature 1136, count 81/539235
feature 1137, count 39/539235
feature 1138, count 23/539235
feature 1139, count 89/539235
feature 1140, count 17/539235
feature 1141, count 37/539235
feature 1142, count 8/539235
feature 1143, count 31/539235
feature 1144, count 32/539235
feature 1145, count 45/539235
feature 1146, count 6/539235
feature 1147, count 24/539235
feature 1148, count 11/539235
feature 1149, count 13/539235
feature 1150, count 4/539235
feature 1151, count 46/539235
feature 1152, count 3/539235
feature 1153, count 12/539235
feature 1154, count 1/539235
feature 1155, count 17/539235
feature 1156, count 17/539235
feature 1157, count 14/539235
feature 1158, count 4/539235
feature 1159, count 6/539235
feature 1160, count 12/539235
feature 1161, count 6/539235
feature 1162, count 10/539235
feature 1163, count 123/539235
feature 1164, count 24/539235
feature 1165, count 35/539235
feature 1166, count 332/539235
feature 1167, count 85/539235
feature 1168, count 8/539235
feature 1169, count 11/539235
feature 1170, count 9/539235
feature 1171, count 4/539235
feature 1172, count 20/539235
feature 1173, count 86/539235
feature 1174, count 12/539235
feature 1175, count 7/539235
feature 1176, count 18/539235
feature 1177, count 36/539235
feature 1178, count 3/539235
feature 1179, count 43/539235
feature 1180, count 27/539235
feature 1181, count 7/539235
feature 1182, count 12/539235
feature 1183, count 9/539235
feature 1184, count 2/539235
feature 1185, count 17/539235
feature 1186, count 9/539235
feature 1187, count 36/539235
feature 1188, count 4/539235
feature 1189, count 11/539235
feature 1190, count 10/539235
feature 1191, count 11/539235
feature 1192, count 8/539235
feature 1193, count 10/539235
feature 1194, count 15/539235
feature 1195, count 22/539235
feature 1196, count 21/539235
feature 1197, count 21/539235
feature 1198, count 16/539235
feature 1199, count 4/539235
feature 1200, count 17/539235
feature 1201, count 18/539235
feature 1202, count 60/539235
feature 1203, count 111/539235
feature 1204, count 23/539235
feature 1205, count 13/539235
feature 1206, count 12/539235
feature 1207, count 6/539235
feature 1208, count 17/539235
feature 1209, count 2/539235
feature 1210, count 9/539235
feature 1211, count 6/539235
feature 1212, count 44/539235
feature 1213, count 10/539235
feature 1214, count 12/539235
feature 1215, count 47/539235
feature 1216, count 18/539235
feature 1217, count 4/539235
feature 1218, count 11/539235
feature 1219, count 13/539235
feature 1220, count 19/539235
feature 1221, count 292/539235
feature 1222, count 14/539235
feature 1223, count 15/539235
feature 1224, count 348/539235
feature 1225, count 31/539235
feature 1226, count 15/539235
feature 1227, count 17/539235
feature 1228, count 21/539235
feature 1229, count 9/539235
feature 1230, count 1297/539235
feature 1231, count 37/539235
feature 1232, count 47/539235
feature 1233, count 80/539235
feature 1234, count 31/539235
feature 1235, count 19/539235
feature 1236, count 33/539235
feature 1237, count 54/539235
feature 1238, count 40/539235
feature 1239, count 47/539235
feature 1240, count 58/539235
feature 1241, count 22/539235
feature 1242, count 14/539235
feature 1243, count 29/539235
feature 1244, count 37/539235
feature 1245, count 1/539235
feature 1246, count 1268/539235
feature 1247, count 8/539235
feature 1248, count 2/539235
feature 1249, count 7/539235
feature 1250, count 51/539235
feature 1251, count 30/539235
feature 1252, count 4/539235
feature 1253, count 2/539235
feature 1254, count 134/539235
feature 1255, count 3/539235
feature 1256, count 2/539235
feature 1257, count 14/539235
feature 1258, count 11/539235
feature 1259, count 5/539235
feature 1260, count 20/539235
feature 1261, count 32/539235
feature 1262, count 7/539235
feature 1263, count 10/539235
feature 1264, count 19/539235
feature 1265, count 14/539235
feature 1266, count 744/539235
feature 1267, count 7/539235
feature 1268, count 7/539235
feature 1269, count 4/539235
feature 1270, count 108/539235
feature 1271, count 23/539235
feature 1272, count 19/539235
feature 1273, count 119/539235
feature 1274, count 23/539235
feature 1275, count 17/539235
feature 1276, count 48/539235
feature 1277, count 32/539235
feature 1278, count 32/539235
feature 1279, count 19/539235
feature 1280, count 11/539235
feature 1281, count 67/539235
feature 1282, count 23/539235
feature 1283, count 3/539235
feature 1284, count 6/539235
feature 1285, count 15/539235
feature 1286, count 4/539235
feature 1287, count 60/539235
feature 1288, count 41/539235
feature 1289, count 20/539235
feature 1290, count 6/539235
feature 1291, count 20/539235
feature 1292, count 48/539235
feature 1293, count 34/539235
feature 1294, count 901/539235
feature 1295, count 24/539235
feature 1296, count 9/539235
feature 1297, count 17/539235
feature 1298, count 9/539235
feature 1299, count 34/539235
feature 1300, count 4/539235
feature 1301, count 19/539235
feature 1302, count 21/539235
feature 1303, count 13/539235
feature 1304, count 5/539235
feature 1305, count 11/539235
feature 1306, count 10/539235
feature 1307, count 23/539235
feature 1308, count 9/539235
feature 1309, count 12/539235
feature 1310, count 30/539235
feature 1311, count 8/539235
feature 1312, count 19/539235
feature 1313, count 12/539235
feature 1314, count 11/539235
feature 1315, count 422/539235
feature 1316, count 15/539235
feature 1317, count 12/539235
feature 1318, count 24/539235
feature 1319, count 56/539235
feature 1320, count 7/539235
feature 1321, count 27/539235
feature 1322, count 1/539235
feature 1323, count 19/539235
feature 1324, count 13/539235
feature 1325, count 11/539235
feature 1326, count 200/539235
feature 1327, count 5/539235
feature 1328, count 10/539235
feature 1329, count 8/539235
feature 1330, count 9/539235
feature 1331, count 6/539235
feature 1332, count 6/539235
feature 1333, count 7/539235
feature 1334, count 114/539235
feature 1335, count 48/539235
feature 1336, count 17/539235
feature 1337, count 17/539235
feature 1338, count 9/539235
feature 1339, count 44/539235
feature 1340, count 15/539235
feature 1341, count 56/539235
feature 1342, count 17/539235
feature 1343, count 10/539235
feature 1344, count 43/539235
feature 1345, count 8/539235
feature 1346, count 13/539235
feature 1347, count 23/539235
feature 1348, count 8/539235
feature 1349, count 7/539235
feature 1350, count 43/539235
feature 1351, count 13/539235
feature 1352, count 67/539235
feature 1353, count 107/539235
feature 1354, count 28/539235
feature 1355, count 72/539235
feature 1356, count 19/539235
feature 1357, count 7/539235
feature 1358, count 126/539235
feature 1359, count 17/539235
feature 1360, count 31/539235
feature 1361, count 29/539235
feature 1362, count 22/539235
feature 1363, count 17/539235
feature 1364, count 5/539235
feature 1365, count 16/539235
feature 1366, count 108/539235
feature 1367, count 12/539235
feature 1368, count 5/539235
feature 1369, count 13/539235
feature 1370, count 100/539235
feature 1371, count 12/539235
feature 1372, count 20/539235
feature 1373, count 9/539235
feature 1374, count 24/539235
feature 1375, count 3/539235
feature 1376, count 23/539235
feature 1377, count 3/539235
feature 1378, count 22/539235
feature 1379, count 2/539235
feature 1380, count 4/539235
feature 1381, count 4/539235
feature 1382, count 4/539235
feature 1383, count 32/539235
feature 1384, count 30/539235
feature 1385, count 28/539235
feature 1386, count 14/539235
feature 1387, count 18/539235
feature 1388, count 24/539235
feature 1389, count 35/539235
feature 1390, count 52/539235
feature 1391, count 251/539235
feature 1392, count 4/539235
feature 1394, count 2/539235
feature 1395, count 21/539235
feature 1396, count 5/539235
feature 1397, count 219/539235
feature 1398, count 504/539235
feature 1399, count 6/539235
feature 1400, count 10/539235
feature 1401, count 8/539235
feature 1402, count 10/539235
feature 1403, count 7/539235
feature 1404, count 29/539235
feature 1405, count 18/539235
feature 1406, count 25/539235
feature 1407, count 27/539235
feature 1408, count 16/539235
feature 1409, count 28/539235
feature 1410, count 14/539235
feature 1411, count 11/539235
feature 1412, count 15/539235
feature 1413, count 5/539235
feature 1414, count 533/539235
feature 1415, count 27/539235
feature 1416, count 32/539235
feature 1417, count 14/539235
feature 1418, count 18/539235
feature 1419, count 60/539235
feature 1420, count 85/539235
feature 1421, count 14/539235
feature 1422, count 15/539235
feature 1423, count 45/539235
feature 1424, count 93/539235
feature 1425, count 34/539235
feature 1426, count 41/539235
feature 1427, count 22/539235
feature 1428, count 13/539235
feature 1429, count 53/539235
feature 1430, count 6/539235
feature 1431, count 54/539235
feature 1432, count 7/539235
feature 1433, count 4/539235
feature 1434, count 2/539235
feature 1435, count 6/539235
feature 1436, count 709/539235
feature 1437, count 16/539235
feature 1438, count 4/539235
feature 1439, count 6/539235
feature 1440, count 38/539235
feature 1441, count 62/539235
feature 1442, count 30/539235
feature 1443, count 10/539235
feature 1444, count 123/539235
feature 1445, count 177/539235
feature 1446, count 50/539235
feature 1447, count 4/539235
feature 1448, count 3/539235
feature 1449, count 28/539235
feature 1450, count 5/539235
feature 1451, count 1/539235
feature 1452, count 2/539235
feature 1453, count 1/539235
feature 1454, count 22/539235
feature 1455, count 6/539235
feature 1456, count 2/539235
feature 1457, count 3/539235
feature 1458, count 4/539235
feature 1459, count 7/539235
feature 1460, count 4/539235
feature 1461, count 10/539235
feature 1462, count 2/539235
feature 1463, count 49/539235
feature 1464, count 192/539235
feature 1465, count 69/539235
feature 1466, count 13173/539235
feature 1467, count 5/539235
feature 1468, count 8/539235
feature 1469, count 21/539235
feature 1470, count 36/539235
feature 1471, count 8/539235
feature 1472, count 28/539235
feature 1473, count 34/539235
feature 1474, count 4842/539235
feature 1475, count 312/539235
feature 1476, count 69/539235
feature 1477, count 35/539235
feature 1478, count 9/539235
feature 1479, count 2/539235
feature 1480, count 16/539235
feature 1481, count 14/539235
feature 1482, count 6/539235
feature 1483, count 140/539235
feature 1484, count 2201/539235
feature 1485, count 19/539235
feature 1486, count 3430/539235
feature 1487, count 12/539235
feature 1488, count 9/539235
feature 1489, count 22/539235
feature 1490, count 4913/539235
feature 1491, count 33/539235
feature 1492, count 92/539235
feature 1493, count 6/539235
feature 1494, count 1635/539235
feature 1495, count 38/539235
feature 1496, count 5/539235
feature 1497, count 30/539235
feature 1498, count 76/539235
feature 1499, count 145/539235
feature 1500, count 33/539235
feature 1501, count 11/539235
feature 1502, count 10/539235
feature 1503, count 15/539235
feature 1504, count 20/539235
feature 1505, count 6/539235
feature 1506, count 9347/539235
feature 1507, count 13/539235
feature 1508, count 5/539235
feature 1509, count 10/539235
feature 1510, count 1333/539235
feature 1511, count 8/539235
feature 1512, count 55/539235
feature 1513, count 94/539235
feature 1514, count 99021/539235
Loading data completed
processing graph data
Processing graphs:   0%|          | 0/20708 [00:00<?, ?graph/s]Processing graphs:   0%|          | 7/20708 [00:00<05:04, 68.04graph/s]Processing graphs:   0%|          | 14/20708 [00:00<05:06, 67.53graph/s]Processing graphs:   0%|          | 21/20708 [00:00<05:05, 67.79graph/s]Processing graphs:   0%|          | 28/20708 [00:00<05:04, 67.97graph/s]Processing graphs:   0%|          | 36/20708 [00:00<04:53, 70.48graph/s]Processing graphs:   0%|          | 44/20708 [00:00<04:51, 70.77graph/s]Processing graphs:   0%|          | 52/20708 [00:00<04:47, 71.91graph/s]Processing graphs:   0%|          | 60/20708 [00:00<04:37, 74.35graph/s]Processing graphs:   0%|          | 68/20708 [00:00<04:39, 73.75graph/s]Processing graphs:   0%|          | 76/20708 [00:01<04:38, 73.95graph/s]Processing graphs:   0%|          | 84/20708 [00:01<04:42, 73.07graph/s]Processing graphs:   0%|          | 92/20708 [00:01<04:44, 72.37graph/s]Processing graphs:   0%|          | 100/20708 [00:01<04:48, 71.53graph/s]Processing graphs:   1%|          | 108/20708 [00:01<04:52, 70.32graph/s]Processing graphs:   1%|          | 116/20708 [00:01<04:44, 72.41graph/s]Processing graphs:   1%|          | 124/20708 [00:01<04:43, 72.61graph/s]Processing graphs:   1%|          | 132/20708 [00:01<04:49, 71.06graph/s]Processing graphs:   1%|          | 140/20708 [00:01<04:50, 70.91graph/s]Processing graphs:   1%|          | 148/20708 [00:02<04:49, 71.09graph/s]Processing graphs:   1%|          | 156/20708 [00:02<04:48, 71.25graph/s]Processing graphs:   1%|          | 164/20708 [00:02<04:48, 71.16graph/s]Processing graphs:   1%|          | 172/20708 [00:02<04:47, 71.38graph/s]Processing graphs:   1%|          | 180/20708 [00:02<04:41, 72.89graph/s]Processing graphs:   1%|          | 188/20708 [00:02<04:46, 71.56graph/s]Processing graphs:   1%|          | 196/20708 [00:02<04:50, 70.65graph/s]Processing graphs:   1%|          | 204/20708 [00:02<04:41, 72.92graph/s]Processing graphs:   1%|          | 212/20708 [00:02<04:42, 72.55graph/s]Processing graphs:   1%|          | 220/20708 [00:03<04:43, 72.31graph/s]Processing graphs:   1%|          | 228/20708 [00:03<04:40, 73.05graph/s]Processing graphs:   1%|          | 236/20708 [00:03<04:45, 71.75graph/s]Processing graphs:   1%|          | 244/20708 [00:03<04:44, 71.88graph/s]Processing graphs:   1%|          | 252/20708 [00:03<04:40, 72.96graph/s]Processing graphs:   1%|▏         | 260/20708 [00:03<04:43, 72.18graph/s]Processing graphs:   1%|▏         | 268/20708 [00:03<04:50, 70.36graph/s]Processing graphs:   1%|▏         | 276/20708 [00:03<04:45, 71.68graph/s]Processing graphs:   1%|▏         | 284/20708 [00:03<04:44, 71.87graph/s]Processing graphs:   1%|▏         | 292/20708 [00:04<04:46, 71.27graph/s]Processing graphs:   1%|▏         | 300/20708 [00:04<04:53, 69.48graph/s]Processing graphs:   1%|▏         | 308/20708 [00:04<04:53, 69.51graph/s]Processing graphs:   2%|▏         | 316/20708 [00:04<04:49, 70.38graph/s]Processing graphs:   2%|▏         | 324/20708 [00:04<04:45, 71.51graph/s]Processing graphs:   2%|▏         | 332/20708 [00:04<04:53, 69.52graph/s]Processing graphs:   2%|▏         | 340/20708 [00:04<04:41, 72.35graph/s]Processing graphs:   2%|▏         | 348/20708 [00:04<04:41, 72.38graph/s]Processing graphs:   2%|▏         | 356/20708 [00:04<04:48, 70.65graph/s]Processing graphs:   2%|▏         | 364/20708 [00:05<04:52, 69.66graph/s]Processing graphs:   2%|▏         | 372/20708 [00:05<04:49, 70.28graph/s]Processing graphs:   2%|▏         | 380/20708 [00:05<04:42, 71.88graph/s]Processing graphs:   2%|▏         | 388/20708 [00:05<04:36, 73.50graph/s]Processing graphs:   2%|▏         | 396/20708 [00:05<04:40, 72.38graph/s]Processing graphs:   2%|▏         | 404/20708 [00:05<04:46, 70.78graph/s]Processing graphs:   2%|▏         | 412/20708 [00:05<04:55, 68.79graph/s]Processing graphs:   2%|▏         | 419/20708 [00:05<04:54, 68.89graph/s]Processing graphs:   2%|▏         | 427/20708 [00:05<04:52, 69.33graph/s]Processing graphs:   2%|▏         | 435/20708 [00:06<04:49, 70.10graph/s]Processing graphs:   2%|▏         | 443/20708 [00:06<04:40, 72.14graph/s]Processing graphs:   2%|▏         | 451/20708 [00:06<04:45, 70.92graph/s]Processing graphs:   2%|▏         | 459/20708 [00:06<04:42, 71.61graph/s]Processing graphs:   2%|▏         | 467/20708 [00:06<04:47, 70.39graph/s]Processing graphs:   2%|▏         | 475/20708 [00:06<04:49, 69.95graph/s]Processing graphs:   2%|▏         | 483/20708 [00:06<04:52, 69.25graph/s]Processing graphs:   2%|▏         | 491/20708 [00:06<04:43, 71.27graph/s]Processing graphs:   2%|▏         | 499/20708 [00:07<04:47, 70.41graph/s]Processing graphs:   2%|▏         | 507/20708 [00:07<04:46, 70.61graph/s]Processing graphs:   2%|▏         | 515/20708 [00:07<04:47, 70.20graph/s]Processing graphs:   3%|▎         | 523/20708 [00:07<04:37, 72.65graph/s]Processing graphs:   3%|▎         | 531/20708 [00:07<04:42, 71.49graph/s]Processing graphs:   3%|▎         | 539/20708 [00:07<04:36, 73.05graph/s]Processing graphs:   3%|▎         | 547/20708 [00:07<04:36, 72.80graph/s]Processing graphs:   3%|▎         | 555/20708 [00:07<04:32, 73.82graph/s]Processing graphs:   3%|▎         | 563/20708 [00:07<04:28, 75.11graph/s]Processing graphs:   3%|▎         | 571/20708 [00:07<04:33, 73.68graph/s]Processing graphs:   3%|▎         | 579/20708 [00:08<04:34, 73.20graph/s]Processing graphs:   3%|▎         | 587/20708 [00:08<04:30, 74.32graph/s]Processing graphs:   3%|▎         | 595/20708 [00:08<04:35, 73.01graph/s]Processing graphs:   3%|▎         | 603/20708 [00:08<04:38, 72.31graph/s]Processing graphs:   3%|▎         | 611/20708 [00:08<04:40, 71.55graph/s]Processing graphs:   3%|▎         | 619/20708 [00:08<04:43, 70.76graph/s]Processing graphs:   3%|▎         | 627/20708 [00:08<04:47, 69.86graph/s]Processing graphs:   3%|▎         | 635/20708 [00:08<04:45, 70.22graph/s]Processing graphs:   3%|▎         | 643/20708 [00:08<04:40, 71.55graph/s]Processing graphs:   3%|▎         | 651/20708 [00:09<04:48, 69.53graph/s]Processing graphs:   3%|▎         | 658/20708 [00:09<04:49, 69.29graph/s]Processing graphs:   3%|▎         | 665/20708 [00:09<04:49, 69.12graph/s]Processing graphs:   3%|▎         | 673/20708 [00:09<04:45, 70.14graph/s]Processing graphs:   3%|▎         | 681/20708 [00:09<04:53, 68.29graph/s]Processing graphs:   3%|▎         | 688/20708 [00:09<04:51, 68.65graph/s]Processing graphs:   3%|▎         | 696/20708 [00:09<04:49, 69.03graph/s]Processing graphs:   3%|▎         | 704/20708 [00:09<04:46, 69.79graph/s]Processing graphs:   3%|▎         | 711/20708 [00:09<04:49, 69.16graph/s]Processing graphs:   3%|▎         | 718/20708 [00:10<04:49, 69.02graph/s]Processing graphs:   4%|▎         | 725/20708 [00:10<04:51, 68.58graph/s]Processing graphs:   4%|▎         | 732/20708 [00:10<04:52, 68.27graph/s]Processing graphs:   4%|▎         | 739/20708 [00:10<04:51, 68.60graph/s]Processing graphs:   4%|▎         | 746/20708 [00:10<04:53, 67.94graph/s]Processing graphs:   4%|▎         | 753/20708 [00:10<04:52, 68.16graph/s]Processing graphs:   4%|▎         | 761/20708 [00:10<04:42, 70.69graph/s]Processing graphs:   4%|▎         | 769/20708 [00:10<04:43, 70.38graph/s]Processing graphs:   4%|▍         | 777/20708 [00:10<04:45, 69.91graph/s]Processing graphs:   4%|▍         | 785/20708 [00:11<04:38, 71.61graph/s]Processing graphs:   4%|▍         | 793/20708 [00:11<04:36, 72.11graph/s]Processing graphs:   4%|▍         | 801/20708 [00:11<04:36, 71.95graph/s]Processing graphs:   4%|▍         | 809/20708 [00:11<04:37, 71.78graph/s]Processing graphs:   4%|▍         | 817/20708 [00:11<04:32, 72.92graph/s]Processing graphs:   4%|▍         | 825/20708 [00:11<04:40, 70.82graph/s]Processing graphs:   4%|▍         | 833/20708 [00:11<04:39, 71.11graph/s]Processing graphs:   4%|▍         | 841/20708 [00:11<04:35, 72.24graph/s]Processing graphs:   4%|▍         | 849/20708 [00:11<04:34, 72.23graph/s]Processing graphs:   4%|▍         | 857/20708 [00:12<04:37, 71.54graph/s]Processing graphs:   4%|▍         | 865/20708 [00:12<04:39, 71.00graph/s]Processing graphs:   4%|▍         | 873/20708 [00:12<04:38, 71.31graph/s]Processing graphs:   4%|▍         | 881/20708 [00:12<04:39, 70.99graph/s]Processing graphs:   4%|▍         | 889/20708 [00:12<04:42, 70.20graph/s]Processing graphs:   4%|▍         | 897/20708 [00:12<04:44, 69.67graph/s]Processing graphs:   4%|▍         | 905/20708 [00:12<04:36, 71.58graph/s]Processing graphs:   4%|▍         | 913/20708 [00:12<04:40, 70.61graph/s]Processing graphs:   4%|▍         | 921/20708 [00:12<04:40, 70.54graph/s]Processing graphs:   4%|▍         | 929/20708 [00:13<04:38, 70.95graph/s]Processing graphs:   5%|▍         | 937/20708 [00:13<04:39, 70.65graph/s]Processing graphs:   5%|▍         | 945/20708 [00:13<04:39, 70.79graph/s]Processing graphs:   5%|▍         | 953/20708 [00:13<04:37, 71.24graph/s]Processing graphs:   5%|▍         | 961/20708 [00:13<04:39, 70.67graph/s]Processing graphs:   5%|▍         | 969/20708 [00:13<04:29, 73.11graph/s]Processing graphs:   5%|▍         | 977/20708 [00:13<04:32, 72.30graph/s]Processing graphs:   5%|▍         | 985/20708 [00:13<04:39, 70.64graph/s]Processing graphs:   5%|▍         | 993/20708 [00:13<04:40, 70.26graph/s]Processing graphs:   5%|▍         | 1001/20708 [00:14<04:34, 71.80graph/s]Processing graphs:   5%|▍         | 1009/20708 [00:14<04:36, 71.29graph/s]Processing graphs:   5%|▍         | 1017/20708 [00:14<04:33, 72.08graph/s]Processing graphs:   5%|▍         | 1025/20708 [00:14<04:35, 71.56graph/s]Processing graphs:   5%|▍         | 1033/20708 [00:14<04:37, 70.90graph/s]Processing graphs:   5%|▌         | 1041/20708 [00:14<04:32, 72.09graph/s]Processing graphs:   5%|▌         | 1049/20708 [00:14<04:32, 72.15graph/s]Processing graphs:   5%|▌         | 1057/20708 [00:14<04:27, 73.58graph/s]Processing graphs:   5%|▌         | 1065/20708 [00:14<04:27, 73.54graph/s]Processing graphs:   5%|▌         | 1073/20708 [00:15<04:29, 72.83graph/s]Processing graphs:   5%|▌         | 1081/20708 [00:15<04:29, 72.84graph/s]Processing graphs:   5%|▌         | 1089/20708 [00:15<04:36, 70.95graph/s]Processing graphs:   5%|▌         | 1097/20708 [00:15<04:37, 70.78graph/s]Processing graphs:   5%|▌         | 1105/20708 [00:15<04:35, 71.06graph/s]Processing graphs:   5%|▌         | 1114/20708 [00:15<04:24, 74.17graph/s]Processing graphs:   5%|▌         | 1122/20708 [00:15<04:26, 73.37graph/s]Processing graphs:   5%|▌         | 1130/20708 [00:15<04:29, 72.64graph/s]Processing graphs:   5%|▌         | 1138/20708 [00:15<04:28, 72.77graph/s]Processing graphs:   6%|▌         | 1146/20708 [00:16<04:33, 71.63graph/s]Processing graphs:   6%|▌         | 1154/20708 [00:16<04:35, 71.04graph/s]Processing graphs:   6%|▌         | 1162/20708 [00:16<04:36, 70.75graph/s]Processing graphs:   6%|▌         | 1170/20708 [00:16<04:42, 69.05graph/s]Processing graphs:   6%|▌         | 1177/20708 [00:16<04:43, 68.83graph/s]Processing graphs:   6%|▌         | 1184/20708 [00:16<04:44, 68.74graph/s]Processing graphs:   6%|▌         | 1192/20708 [00:16<04:41, 69.24graph/s]Processing graphs:   6%|▌         | 1200/20708 [00:16<04:39, 69.86graph/s]Processing graphs:   6%|▌         | 1208/20708 [00:16<04:34, 71.03graph/s]Processing graphs:   6%|▌         | 1216/20708 [00:17<04:35, 70.79graph/s]Processing graphs:   6%|▌         | 1224/20708 [00:17<04:27, 72.95graph/s]Processing graphs:   6%|▌         | 1232/20708 [00:17<04:24, 73.59graph/s]Processing graphs:   6%|▌         | 1240/20708 [00:17<04:23, 73.77graph/s]Processing graphs:   6%|▌         | 1248/20708 [00:17<04:20, 74.72graph/s]Processing graphs:   6%|▌         | 1256/20708 [00:17<04:28, 72.38graph/s]Processing graphs:   6%|▌         | 1264/20708 [00:17<04:23, 73.65graph/s]Processing graphs:   6%|▌         | 1272/20708 [00:17<04:27, 72.65graph/s]Processing graphs:   6%|▌         | 1280/20708 [00:17<04:30, 71.82graph/s]Processing graphs:   6%|▌         | 1288/20708 [00:18<04:30, 71.82graph/s]Processing graphs:   6%|▋         | 1297/20708 [00:18<04:22, 73.95graph/s]Processing graphs:   6%|▋         | 1305/20708 [00:18<04:27, 72.66graph/s]Processing graphs:   6%|▋         | 1313/20708 [00:18<04:37, 70.01graph/s]Processing graphs:   6%|▋         | 1321/20708 [00:18<04:36, 70.21graph/s]Processing graphs:   6%|▋         | 1329/20708 [00:18<04:34, 70.49graph/s]Processing graphs:   6%|▋         | 1337/20708 [00:18<04:32, 71.09graph/s]Processing graphs:   6%|▋         | 1345/20708 [00:18<04:35, 70.16graph/s]Processing graphs:   7%|▋         | 1353/20708 [00:18<04:36, 69.94graph/s]Processing graphs:   7%|▋         | 1361/20708 [00:19<04:26, 72.59graph/s]Processing graphs:   7%|▋         | 1369/20708 [00:19<04:29, 71.89graph/s]Processing graphs:   7%|▋         | 1377/20708 [00:19<04:27, 72.34graph/s]Processing graphs:   7%|▋         | 1385/20708 [00:19<04:26, 72.53graph/s]Processing graphs:   7%|▋         | 1393/20708 [00:19<04:24, 73.04graph/s]Processing graphs:   7%|▋         | 1401/20708 [00:19<04:23, 73.18graph/s]Processing graphs:   7%|▋         | 1409/20708 [00:19<04:26, 72.47graph/s]Processing graphs:   7%|▋         | 1417/20708 [00:19<04:24, 72.86graph/s]Processing graphs:   7%|▋         | 1425/20708 [00:19<04:29, 71.51graph/s]Processing graphs:   7%|▋         | 1433/20708 [00:20<04:28, 71.70graph/s]Processing graphs:   7%|▋         | 1441/20708 [00:20<04:26, 72.18graph/s]Processing graphs:   7%|▋         | 1450/20708 [00:20<04:15, 75.49graph/s]Processing graphs:   7%|▋         | 1458/20708 [00:20<04:18, 74.48graph/s]Processing graphs:   7%|▋         | 1466/20708 [00:20<04:14, 75.70graph/s]Processing graphs:   7%|▋         | 1474/20708 [00:20<04:18, 74.48graph/s]Processing graphs:   7%|▋         | 1482/20708 [00:20<04:17, 74.60graph/s]Processing graphs:   7%|▋         | 1490/20708 [00:20<04:19, 74.02graph/s]Processing graphs:   7%|▋         | 1498/20708 [00:20<04:20, 73.72graph/s]Processing graphs:   7%|▋         | 1506/20708 [00:21<04:20, 73.64graph/s]Processing graphs:   7%|▋         | 1514/20708 [00:21<04:20, 73.56graph/s]Processing graphs:   7%|▋         | 1523/20708 [00:21<04:14, 75.36graph/s]Processing graphs:   7%|▋         | 1531/20708 [00:21<04:23, 72.75graph/s]Processing graphs:   7%|▋         | 1539/20708 [00:21<04:26, 71.97graph/s]Processing graphs:   7%|▋         | 1547/20708 [00:21<04:28, 71.38graph/s]Processing graphs:   8%|▊         | 1555/20708 [00:21<04:28, 71.30graph/s]Processing graphs:   8%|▊         | 1563/20708 [00:21<04:33, 69.96graph/s]Processing graphs:   8%|▊         | 1571/20708 [00:21<04:35, 69.36graph/s]Processing graphs:   8%|▊         | 1579/20708 [00:22<04:27, 71.50graph/s]Processing graphs:   8%|▊         | 1587/20708 [00:22<04:26, 71.82graph/s]Processing graphs:   8%|▊         | 1595/20708 [00:22<04:24, 72.28graph/s]Processing graphs:   8%|▊         | 1603/20708 [00:22<04:18, 73.94graph/s]Processing graphs:   8%|▊         | 1611/20708 [00:22<04:19, 73.71graph/s]Processing graphs:   8%|▊         | 1619/20708 [00:22<04:18, 73.98graph/s]Processing graphs:   8%|▊         | 1627/20708 [00:22<04:15, 74.76graph/s]Processing graphs:   8%|▊         | 1635/20708 [00:22<04:13, 75.23graph/s]Processing graphs:   8%|▊         | 1643/20708 [00:22<04:16, 74.33graph/s]Processing graphs:   8%|▊         | 1651/20708 [00:23<04:22, 72.68graph/s]Processing graphs:   8%|▊         | 1659/20708 [00:23<04:17, 73.97graph/s]Processing graphs:   8%|▊         | 1667/20708 [00:23<04:13, 75.23graph/s]Processing graphs:   8%|▊         | 1675/20708 [00:23<04:20, 73.11graph/s]Processing graphs:   8%|▊         | 1683/20708 [00:23<04:17, 73.77graph/s]Processing graphs:   8%|▊         | 1691/20708 [00:23<04:22, 72.49graph/s]Processing graphs:   8%|▊         | 1699/20708 [00:23<04:25, 71.67graph/s]Processing graphs:   8%|▊         | 1707/20708 [00:23<04:25, 71.50graph/s]Processing graphs:   8%|▊         | 1715/20708 [00:23<04:30, 70.10graph/s]Processing graphs:   8%|▊         | 1723/20708 [00:24<04:29, 70.55graph/s]Processing graphs:   8%|▊         | 1731/20708 [00:24<04:28, 70.80graph/s]Processing graphs:   8%|▊         | 1739/20708 [00:24<04:32, 69.60graph/s]Processing graphs:   8%|▊         | 1747/20708 [00:24<04:30, 70.02graph/s]Processing graphs:   8%|▊         | 1755/20708 [00:24<04:35, 68.83graph/s]Processing graphs:   9%|▊         | 1762/20708 [00:24<04:37, 68.22graph/s]Processing graphs:   9%|▊         | 1770/20708 [00:24<04:33, 69.30graph/s]Processing graphs:   9%|▊         | 1778/20708 [00:24<04:28, 70.56graph/s]Processing graphs:   9%|▊         | 1786/20708 [00:24<04:26, 70.93graph/s]Processing graphs:   9%|▊         | 1794/20708 [00:25<04:19, 72.88graph/s]Processing graphs:   9%|▊         | 1802/20708 [00:25<04:26, 70.82graph/s]Processing graphs:   9%|▊         | 1810/20708 [00:25<04:22, 72.09graph/s]Processing graphs:   9%|▉         | 1818/20708 [00:25<04:19, 72.72graph/s]Processing graphs:   9%|▉         | 1826/20708 [00:25<04:20, 72.58graph/s]Processing graphs:   9%|▉         | 1834/20708 [00:25<04:23, 71.68graph/s]Processing graphs:   9%|▉         | 1842/20708 [00:25<04:15, 73.72graph/s]Processing graphs:   9%|▉         | 1850/20708 [00:25<04:20, 72.48graph/s]Processing graphs:   9%|▉         | 1858/20708 [00:25<04:21, 71.96graph/s]Processing graphs:   9%|▉         | 1866/20708 [00:26<04:20, 72.22graph/s]Processing graphs:   9%|▉         | 1874/20708 [00:26<04:13, 74.16graph/s]Processing graphs:   9%|▉         | 1882/20708 [00:26<04:14, 74.09graph/s]Processing graphs:   9%|▉         | 1890/20708 [00:26<04:11, 74.96graph/s]Processing graphs:   9%|▉         | 1898/20708 [00:26<04:14, 73.89graph/s]Processing graphs:   9%|▉         | 1906/20708 [00:26<04:13, 74.08graph/s]Processing graphs:   9%|▉         | 1914/20708 [00:26<04:20, 72.02graph/s]Processing graphs:   9%|▉         | 1922/20708 [00:26<04:18, 72.64graph/s]Processing graphs:   9%|▉         | 1930/20708 [00:26<04:24, 71.00graph/s]Processing graphs:   9%|▉         | 1938/20708 [00:27<04:25, 70.57graph/s]Processing graphs:   9%|▉         | 1946/20708 [00:27<04:16, 73.04graph/s]Processing graphs:   9%|▉         | 1954/20708 [00:27<04:15, 73.41graph/s]Processing graphs:   9%|▉         | 1962/20708 [00:27<04:16, 73.14graph/s]Processing graphs:  10%|▉         | 1970/20708 [00:27<04:20, 71.92graph/s]Processing graphs:  10%|▉         | 1978/20708 [00:27<04:16, 72.93graph/s]Processing graphs:  10%|▉         | 1986/20708 [00:27<04:14, 73.43graph/s]Processing graphs:  10%|▉         | 1994/20708 [00:27<04:24, 70.67graph/s]Processing graphs:  10%|▉         | 2002/20708 [00:27<04:17, 72.62graph/s]Processing graphs:  10%|▉         | 2010/20708 [00:28<04:10, 74.63graph/s]Processing graphs:  10%|▉         | 2018/20708 [00:28<04:17, 72.71graph/s]Processing graphs:  10%|▉         | 2026/20708 [00:28<04:14, 73.44graph/s]Processing graphs:  10%|▉         | 2034/20708 [00:28<04:18, 72.37graph/s]Processing graphs:  10%|▉         | 2042/20708 [00:28<04:15, 73.11graph/s]Processing graphs:  10%|▉         | 2050/20708 [00:28<04:12, 73.91graph/s]Processing graphs:  10%|▉         | 2058/20708 [00:28<04:12, 73.99graph/s]Processing graphs:  10%|▉         | 2066/20708 [00:28<04:11, 74.07graph/s]Processing graphs:  10%|█         | 2074/20708 [00:28<04:13, 73.62graph/s]Processing graphs:  10%|█         | 2082/20708 [00:29<04:11, 73.92graph/s]Processing graphs:  10%|█         | 2090/20708 [00:29<04:19, 71.73graph/s]Processing graphs:  10%|█         | 2098/20708 [00:29<04:26, 69.81graph/s]Processing graphs:  10%|█         | 2106/20708 [00:29<04:20, 71.39graph/s]Processing graphs:  10%|█         | 2114/20708 [00:29<04:25, 69.95graph/s]Processing graphs:  10%|█         | 2122/20708 [00:29<04:18, 72.00graph/s]Processing graphs:  10%|█         | 2130/20708 [00:29<04:11, 73.83graph/s]Processing graphs:  10%|█         | 2138/20708 [00:29<04:15, 72.79graph/s]Processing graphs:  10%|█         | 2146/20708 [00:29<04:17, 72.15graph/s]Processing graphs:  10%|█         | 2154/20708 [00:30<04:11, 73.88graph/s]Processing graphs:  10%|█         | 2162/20708 [00:30<04:09, 74.47graph/s]Processing graphs:  10%|█         | 2170/20708 [00:30<04:14, 72.76graph/s]Processing graphs:  11%|█         | 2178/20708 [00:30<04:19, 71.43graph/s]Processing graphs:  11%|█         | 2186/20708 [00:30<04:19, 71.36graph/s]Processing graphs:  11%|█         | 2194/20708 [00:30<04:15, 72.36graph/s]Processing graphs:  11%|█         | 2202/20708 [00:30<04:14, 72.64graph/s]Processing graphs:  11%|█         | 2210/20708 [00:30<04:17, 71.94graph/s]Processing graphs:  11%|█         | 2218/20708 [00:30<04:17, 71.73graph/s]Processing graphs:  11%|█         | 2226/20708 [00:31<04:13, 72.95graph/s]Processing graphs:  11%|█         | 2234/20708 [00:31<04:10, 73.80graph/s]Processing graphs:  11%|█         | 2242/20708 [00:31<04:15, 72.15graph/s]Processing graphs:  11%|█         | 2250/20708 [00:31<04:12, 73.17graph/s]Processing graphs:  11%|█         | 2258/20708 [00:31<04:13, 72.90graph/s]Processing graphs:  11%|█         | 2266/20708 [00:31<04:09, 73.95graph/s]Processing graphs:  11%|█         | 2274/20708 [00:31<04:06, 74.90graph/s]Processing graphs:  11%|█         | 2282/20708 [00:31<04:08, 74.23graph/s]Processing graphs:  11%|█         | 2290/20708 [00:31<04:12, 72.96graph/s]Processing graphs:  11%|█         | 2298/20708 [00:32<04:13, 72.65graph/s]Processing graphs:  11%|█         | 2306/20708 [00:32<04:09, 73.75graph/s]Processing graphs:  11%|█         | 2314/20708 [00:32<04:15, 72.04graph/s]Processing graphs:  11%|█         | 2322/20708 [00:32<04:16, 71.75graph/s]Processing graphs:  11%|█▏        | 2330/20708 [00:32<04:20, 70.49graph/s]Processing graphs:  11%|█▏        | 2338/20708 [00:32<04:19, 70.75graph/s]Processing graphs:  11%|█▏        | 2346/20708 [00:32<04:16, 71.69graph/s]Processing graphs:  11%|█▏        | 2354/20708 [00:32<04:17, 71.35graph/s]Processing graphs:  11%|█▏        | 2362/20708 [00:32<04:08, 73.69graph/s]Processing graphs:  11%|█▏        | 2370/20708 [00:32<04:09, 73.46graph/s]Processing graphs:  11%|█▏        | 2378/20708 [00:33<04:14, 72.04graph/s]Processing graphs:  12%|█▏        | 2386/20708 [00:33<04:11, 72.90graph/s]Processing graphs:  12%|█▏        | 2394/20708 [00:33<04:17, 71.06graph/s]Processing graphs:  12%|█▏        | 2402/20708 [00:33<04:12, 72.62graph/s]Processing graphs:  12%|█▏        | 2410/20708 [00:33<04:06, 74.17graph/s]Processing graphs:  12%|█▏        | 2418/20708 [00:33<04:06, 74.16graph/s]Processing graphs:  12%|█▏        | 2426/20708 [00:33<04:09, 73.32graph/s]Processing graphs:  12%|█▏        | 2434/20708 [00:33<04:09, 73.16graph/s]Processing graphs:  12%|█▏        | 2442/20708 [00:33<04:05, 74.32graph/s]Processing graphs:  12%|█▏        | 2450/20708 [00:34<04:08, 73.42graph/s]Processing graphs:  12%|█▏        | 2458/20708 [00:34<04:07, 73.87graph/s]Processing graphs:  12%|█▏        | 2466/20708 [00:34<04:08, 73.53graph/s]Processing graphs:  12%|█▏        | 2474/20708 [00:34<04:15, 71.24graph/s]Processing graphs:  12%|█▏        | 2483/20708 [00:34<04:06, 74.03graph/s]Processing graphs:  12%|█▏        | 2491/20708 [00:34<04:08, 73.16graph/s]Processing graphs:  12%|█▏        | 2499/20708 [00:34<04:03, 74.63graph/s]Processing graphs:  12%|█▏        | 2507/20708 [00:34<04:01, 75.38graph/s]Processing graphs:  12%|█▏        | 2515/20708 [00:34<04:07, 73.43graph/s]Processing graphs:  12%|█▏        | 2523/20708 [00:35<04:08, 73.13graph/s]Processing graphs:  12%|█▏        | 2531/20708 [00:35<04:11, 72.21graph/s]Processing graphs:  12%|█▏        | 2539/20708 [00:35<04:12, 72.09graph/s]Processing graphs:  12%|█▏        | 2547/20708 [00:35<04:08, 73.20graph/s]Processing graphs:  12%|█▏        | 2555/20708 [00:35<04:05, 73.86graph/s]Processing graphs:  12%|█▏        | 2563/20708 [00:35<04:02, 74.94graph/s]Processing graphs:  12%|█▏        | 2572/20708 [00:35<03:57, 76.20graph/s]Processing graphs:  12%|█▏        | 2580/20708 [00:35<04:04, 74.20graph/s]Processing graphs:  12%|█▏        | 2588/20708 [00:35<04:07, 73.16graph/s]Processing graphs:  13%|█▎        | 2596/20708 [00:36<04:08, 72.86graph/s]Processing graphs:  13%|█▎        | 2604/20708 [00:36<04:02, 74.71graph/s]Processing graphs:  13%|█▎        | 2612/20708 [00:36<04:03, 74.45graph/s]Processing graphs:  13%|█▎        | 2620/20708 [00:36<04:04, 74.11graph/s]Processing graphs:  13%|█▎        | 2628/20708 [00:36<04:05, 73.65graph/s]Processing graphs:  13%|█▎        | 2636/20708 [00:36<04:01, 74.92graph/s]Processing graphs:  13%|█▎        | 2644/20708 [00:36<04:06, 73.37graph/s]Processing graphs:  13%|█▎        | 2652/20708 [00:36<04:08, 72.79graph/s]Processing graphs:  13%|█▎        | 2660/20708 [00:36<04:08, 72.66graph/s]Processing graphs:  13%|█▎        | 2668/20708 [00:37<04:10, 71.91graph/s]Processing graphs:  13%|█▎        | 2676/20708 [00:37<04:11, 71.71graph/s]Processing graphs:  13%|█▎        | 2684/20708 [00:37<04:10, 72.03graph/s]Processing graphs:  13%|█▎        | 2692/20708 [00:37<04:05, 73.30graph/s]Processing graphs:  13%|█▎        | 2700/20708 [00:37<04:02, 74.21graph/s]Processing graphs:  13%|█▎        | 2708/20708 [00:37<04:01, 74.43graph/s]Processing graphs:  13%|█▎        | 2716/20708 [00:37<04:07, 72.69graph/s]Processing graphs:  13%|█▎        | 2724/20708 [00:37<04:03, 73.77graph/s]Processing graphs:  13%|█▎        | 2732/20708 [00:37<04:05, 73.22graph/s]Processing graphs:  13%|█▎        | 2740/20708 [00:38<04:02, 74.11graph/s]Processing graphs:  13%|█▎        | 2748/20708 [00:38<04:03, 73.74graph/s]Processing graphs:  13%|█▎        | 2756/20708 [00:38<04:10, 71.75graph/s]Processing graphs:  13%|█▎        | 2764/20708 [00:38<04:08, 72.13graph/s]Processing graphs:  13%|█▎        | 2772/20708 [00:38<04:05, 73.13graph/s]Processing graphs:  13%|█▎        | 2780/20708 [00:38<04:03, 73.71graph/s]Processing graphs:  13%|█▎        | 2788/20708 [00:38<03:57, 75.41graph/s]Processing graphs:  14%|█▎        | 2796/20708 [00:38<03:57, 75.26graph/s]Processing graphs:  14%|█▎        | 2804/20708 [00:38<03:56, 75.58graph/s]Processing graphs:  14%|█▎        | 2812/20708 [00:38<03:55, 76.04graph/s]Processing graphs:  14%|█▎        | 2820/20708 [00:39<03:52, 76.84graph/s]Processing graphs:  14%|█▎        | 2828/20708 [00:39<03:58, 74.98graph/s]Processing graphs:  14%|█▎        | 2836/20708 [00:39<03:55, 75.83graph/s]Processing graphs:  14%|█▎        | 2844/20708 [00:39<03:56, 75.69graph/s]Processing graphs:  14%|█▍        | 2852/20708 [00:39<03:54, 76.17graph/s]Processing graphs:  14%|█▍        | 2860/20708 [00:39<04:01, 73.87graph/s]Processing graphs:  14%|█▍        | 2868/20708 [00:39<03:57, 75.01graph/s]Processing graphs:  14%|█▍        | 2876/20708 [00:39<04:04, 72.93graph/s]Processing graphs:  14%|█▍        | 2884/20708 [00:39<04:00, 74.12graph/s]Processing graphs:  14%|█▍        | 2892/20708 [00:40<04:03, 73.09graph/s]Processing graphs:  14%|█▍        | 2900/20708 [00:40<04:02, 73.54graph/s]Processing graphs:  14%|█▍        | 2908/20708 [00:40<04:00, 74.02graph/s]Processing graphs:  14%|█▍        | 2916/20708 [00:40<04:00, 74.06graph/s]Processing graphs:  14%|█▍        | 2924/20708 [00:40<04:02, 73.32graph/s]Processing graphs:  14%|█▍        | 2932/20708 [00:40<04:02, 73.36graph/s]Processing graphs:  14%|█▍        | 2940/20708 [00:40<03:57, 74.81graph/s]Processing graphs:  14%|█▍        | 2949/20708 [00:40<03:47, 78.15graph/s]Processing graphs:  14%|█▍        | 2957/20708 [00:40<03:54, 75.72graph/s]Processing graphs:  14%|█▍        | 2965/20708 [00:41<03:54, 75.53graph/s]Processing graphs:  14%|█▍        | 2973/20708 [00:41<03:51, 76.67graph/s]Processing graphs:  14%|█▍        | 2981/20708 [00:41<03:54, 75.49graph/s]Processing graphs:  14%|█▍        | 2989/20708 [00:41<03:54, 75.69graph/s]Processing graphs:  14%|█▍        | 2997/20708 [00:41<03:56, 74.92graph/s]Processing graphs:  15%|█▍        | 3006/20708 [00:41<03:48, 77.41graph/s]Processing graphs:  15%|█▍        | 3014/20708 [00:41<03:48, 77.43graph/s]Processing graphs:  15%|█▍        | 3022/20708 [00:41<03:54, 75.38graph/s]Processing graphs:  15%|█▍        | 3030/20708 [00:41<03:51, 76.52graph/s]Processing graphs:  15%|█▍        | 3038/20708 [00:41<03:48, 77.27graph/s]Processing graphs:  15%|█▍        | 3046/20708 [00:42<03:47, 77.63graph/s]Processing graphs:  15%|█▍        | 3054/20708 [00:42<03:47, 77.55graph/s]Processing graphs:  15%|█▍        | 3062/20708 [00:42<03:46, 77.90graph/s]Processing graphs:  15%|█▍        | 3071/20708 [00:42<03:39, 80.20graph/s]Processing graphs:  15%|█▍        | 3080/20708 [00:42<03:47, 77.39graph/s]Processing graphs:  15%|█▍        | 3088/20708 [00:42<03:50, 76.38graph/s]Processing graphs:  15%|█▍        | 3096/20708 [00:42<03:50, 76.56graph/s]Processing graphs:  15%|█▍        | 3104/20708 [00:42<03:53, 75.35graph/s]Processing graphs:  15%|█▌        | 3112/20708 [00:42<04:01, 72.95graph/s]Processing graphs:  15%|█▌        | 3120/20708 [00:43<04:01, 72.86graph/s]Processing graphs:  15%|█▌        | 3128/20708 [00:43<03:59, 73.53graph/s]Processing graphs:  15%|█▌        | 3136/20708 [00:43<03:56, 74.15graph/s]Processing graphs:  15%|█▌        | 3144/20708 [00:43<04:00, 72.90graph/s]Processing graphs:  15%|█▌        | 3152/20708 [00:43<03:55, 74.53graph/s]Processing graphs:  15%|█▌        | 3160/20708 [00:43<04:01, 72.74graph/s]Processing graphs:  15%|█▌        | 3168/20708 [00:43<03:56, 74.15graph/s]Processing graphs:  15%|█▌        | 3176/20708 [00:43<03:51, 75.64graph/s]Processing graphs:  15%|█▌        | 3184/20708 [00:43<03:51, 75.63graph/s]Processing graphs:  15%|█▌        | 3192/20708 [00:44<03:57, 73.69graph/s]Processing graphs:  15%|█▌        | 3200/20708 [00:44<03:57, 73.60graph/s]Processing graphs:  15%|█▌        | 3208/20708 [00:44<03:55, 74.42graph/s]Processing graphs:  16%|█▌        | 3216/20708 [00:44<03:53, 74.93graph/s]Processing graphs:  16%|█▌        | 3224/20708 [00:44<03:50, 76.00graph/s]Processing graphs:  16%|█▌        | 3232/20708 [00:44<03:54, 74.62graph/s]Processing graphs:  16%|█▌        | 3240/20708 [00:44<03:52, 74.99graph/s]Processing graphs:  16%|█▌        | 3248/20708 [00:44<03:52, 75.00graph/s]Processing graphs:  16%|█▌        | 3256/20708 [00:44<03:59, 72.83graph/s]Processing graphs:  16%|█▌        | 3264/20708 [00:45<03:55, 73.95graph/s]Processing graphs:  16%|█▌        | 3272/20708 [00:45<03:53, 74.81graph/s]Processing graphs:  16%|█▌        | 3280/20708 [00:45<03:54, 74.37graph/s]Processing graphs:  16%|█▌        | 3289/20708 [00:45<03:48, 76.39graph/s]Processing graphs:  16%|█▌        | 3297/20708 [00:45<03:55, 74.02graph/s]Processing graphs:  16%|█▌        | 3305/20708 [00:45<03:54, 74.12graph/s]Processing graphs:  16%|█▌        | 3314/20708 [00:45<03:45, 77.05graph/s]Processing graphs:  16%|█▌        | 3323/20708 [00:45<03:42, 78.04graph/s]Processing graphs:  16%|█▌        | 3331/20708 [00:45<03:47, 76.46graph/s]Processing graphs:  16%|█▌        | 3339/20708 [00:46<03:47, 76.37graph/s]Processing graphs:  16%|█▌        | 3347/20708 [00:46<03:45, 77.05graph/s]Processing graphs:  16%|█▌        | 3355/20708 [00:46<03:47, 76.16graph/s]Processing graphs:  16%|█▌        | 3363/20708 [00:46<03:50, 75.39graph/s]Processing graphs:  16%|█▋        | 3371/20708 [00:46<03:52, 74.52graph/s]Processing graphs:  16%|█▋        | 3379/20708 [00:46<03:59, 72.30graph/s]Processing graphs:  16%|█▋        | 3387/20708 [00:46<03:55, 73.45graph/s]Processing graphs:  16%|█▋        | 3396/20708 [00:46<03:45, 76.81graph/s]Processing graphs:  16%|█▋        | 3404/20708 [00:46<03:53, 74.23graph/s]Processing graphs:  16%|█▋        | 3412/20708 [00:46<03:52, 74.52graph/s]Processing graphs:  17%|█▋        | 3420/20708 [00:47<03:53, 73.91graph/s]Processing graphs:  17%|█▋        | 3428/20708 [00:47<03:49, 75.20graph/s]Processing graphs:  17%|█▋        | 3436/20708 [00:47<03:47, 75.78graph/s]Processing graphs:  17%|█▋        | 3444/20708 [00:47<03:53, 73.88graph/s]Processing graphs:  17%|█▋        | 3452/20708 [00:47<03:50, 74.94graph/s]Processing graphs:  17%|█▋        | 3460/20708 [00:47<03:57, 72.49graph/s]Processing graphs:  17%|█▋        | 3468/20708 [00:47<03:55, 73.08graph/s]Processing graphs:  17%|█▋        | 3476/20708 [00:47<03:56, 72.88graph/s]Processing graphs:  17%|█▋        | 3485/20708 [00:47<03:50, 74.80graph/s]Processing graphs:  17%|█▋        | 3493/20708 [00:48<03:45, 76.19graph/s]Processing graphs:  17%|█▋        | 3501/20708 [00:48<03:51, 74.41graph/s]Processing graphs:  17%|█▋        | 3509/20708 [00:48<03:53, 73.80graph/s]Processing graphs:  17%|█▋        | 3518/20708 [00:48<03:45, 76.34graph/s]Processing graphs:  17%|█▋        | 3526/20708 [00:48<03:44, 76.38graph/s]Processing graphs:  17%|█▋        | 3534/20708 [00:48<03:50, 74.58graph/s]Processing graphs:  17%|█▋        | 3542/20708 [00:48<03:53, 73.37graph/s]Processing graphs:  17%|█▋        | 3550/20708 [00:48<03:53, 73.62graph/s]Processing graphs:  17%|█▋        | 3559/20708 [00:48<03:40, 77.94graph/s]Processing graphs:  17%|█▋        | 3567/20708 [00:49<03:44, 76.52graph/s]Processing graphs:  17%|█▋        | 3575/20708 [00:49<03:51, 74.07graph/s]Processing graphs:  17%|█▋        | 3583/20708 [00:49<03:55, 72.65graph/s]Processing graphs:  17%|█▋        | 3591/20708 [00:49<03:58, 71.82graph/s]Processing graphs:  17%|█▋        | 3600/20708 [00:49<03:50, 74.15graph/s]Processing graphs:  17%|█▋        | 3608/20708 [00:49<03:48, 74.85graph/s]Processing graphs:  17%|█▋        | 3616/20708 [00:49<03:46, 75.43graph/s]Processing graphs:  18%|█▊        | 3625/20708 [00:49<03:42, 76.63graph/s]Processing graphs:  18%|█▊        | 3633/20708 [00:49<03:43, 76.41graph/s]Processing graphs:  18%|█▊        | 3641/20708 [00:50<03:41, 77.05graph/s]Processing graphs:  18%|█▊        | 3650/20708 [00:50<03:39, 77.64graph/s]Processing graphs:  18%|█▊        | 3658/20708 [00:50<03:41, 76.97graph/s]Processing graphs:  18%|█▊        | 3666/20708 [00:50<03:44, 75.82graph/s]Processing graphs:  18%|█▊        | 3674/20708 [00:50<03:48, 74.56graph/s]Processing graphs:  18%|█▊        | 3682/20708 [00:50<03:47, 74.89graph/s]Processing graphs:  18%|█▊        | 3690/20708 [00:50<03:54, 72.59graph/s]Processing graphs:  18%|█▊        | 3698/20708 [00:50<03:49, 74.03graph/s]Processing graphs:  18%|█▊        | 3706/20708 [00:50<03:47, 74.70graph/s]Processing graphs:  18%|█▊        | 3714/20708 [00:51<03:48, 74.46graph/s]Processing graphs:  18%|█▊        | 3722/20708 [00:51<03:45, 75.38graph/s]Processing graphs:  18%|█▊        | 3730/20708 [00:51<03:50, 73.60graph/s]Processing graphs:  18%|█▊        | 3739/20708 [00:51<03:42, 76.36graph/s]Processing graphs:  18%|█▊        | 3747/20708 [00:51<03:42, 76.30graph/s]Processing graphs:  18%|█▊        | 3755/20708 [00:51<03:40, 76.76graph/s]Processing graphs:  18%|█▊        | 3763/20708 [00:51<03:44, 75.35graph/s]Processing graphs:  18%|█▊        | 3771/20708 [00:51<03:43, 75.62graph/s]Processing graphs:  18%|█▊        | 3779/20708 [00:51<03:47, 74.44graph/s]Processing graphs:  18%|█▊        | 3788/20708 [00:51<03:37, 77.95graph/s]Processing graphs:  18%|█▊        | 3796/20708 [00:52<03:45, 75.08graph/s]Processing graphs:  18%|█▊        | 3804/20708 [00:52<03:46, 74.69graph/s]Processing graphs:  18%|█▊        | 3813/20708 [00:52<03:39, 76.83graph/s]Processing graphs:  18%|█▊        | 3821/20708 [00:52<03:39, 77.01graph/s]Processing graphs:  18%|█▊        | 3829/20708 [00:52<03:39, 76.91graph/s]Processing graphs:  19%|█▊        | 3837/20708 [00:52<03:41, 76.14graph/s]Processing graphs:  19%|█▊        | 3845/20708 [00:52<03:45, 74.86graph/s]Processing graphs:  19%|█▊        | 3853/20708 [00:52<03:48, 73.78graph/s]Processing graphs:  19%|█▊        | 3861/20708 [00:52<03:47, 73.90graph/s]Processing graphs:  19%|█▊        | 3869/20708 [00:53<03:44, 74.89graph/s]Processing graphs:  19%|█▊        | 3877/20708 [00:53<03:43, 75.32graph/s]Processing graphs:  19%|█▉        | 3885/20708 [00:53<03:45, 74.47graph/s]Processing graphs:  19%|█▉        | 3893/20708 [00:53<03:47, 73.98graph/s]Processing graphs:  19%|█▉        | 3902/20708 [00:53<03:36, 77.53graph/s]Processing graphs:  19%|█▉        | 3910/20708 [00:53<03:41, 75.67graph/s]Processing graphs:  19%|█▉        | 3918/20708 [00:53<03:46, 74.13graph/s]Processing graphs:  19%|█▉        | 3927/20708 [00:53<03:34, 78.18graph/s]Processing graphs:  19%|█▉        | 3935/20708 [00:53<03:35, 77.77graph/s]Processing graphs:  19%|█▉        | 3944/20708 [00:54<03:35, 77.88graph/s]Processing graphs:  19%|█▉        | 3952/20708 [00:54<03:35, 77.81graph/s]Processing graphs:  19%|█▉        | 3960/20708 [00:54<03:34, 78.05graph/s]Processing graphs:  19%|█▉        | 3968/20708 [00:54<03:37, 77.06graph/s]Processing graphs:  19%|█▉        | 3976/20708 [00:54<03:35, 77.63graph/s]Processing graphs:  19%|█▉        | 3985/20708 [00:54<03:30, 79.42graph/s]Processing graphs:  19%|█▉        | 3993/20708 [00:54<03:37, 76.86graph/s]Processing graphs:  19%|█▉        | 4001/20708 [00:54<03:43, 74.61graph/s]Processing graphs:  19%|█▉        | 4009/20708 [00:54<03:44, 74.47graph/s]Processing graphs:  19%|█▉        | 4018/20708 [00:55<03:37, 76.76graph/s]Processing graphs:  19%|█▉        | 4026/20708 [00:55<03:37, 76.67graph/s]Processing graphs:  19%|█▉        | 4035/20708 [00:55<03:32, 78.36graph/s]Processing graphs:  20%|█▉        | 4043/20708 [00:55<03:35, 77.44graph/s]Processing graphs:  20%|█▉        | 4051/20708 [00:55<03:38, 76.40graph/s]Processing graphs:  20%|█▉        | 4060/20708 [00:55<03:35, 77.34graph/s]Processing graphs:  20%|█▉        | 4068/20708 [00:55<03:38, 76.30graph/s]Processing graphs:  20%|█▉        | 4076/20708 [00:55<03:36, 76.84graph/s]Processing graphs:  20%|█▉        | 4085/20708 [00:55<03:31, 78.42graph/s]Processing graphs:  20%|█▉        | 4094/20708 [00:55<03:31, 78.68graph/s]Processing graphs:  20%|█▉        | 4102/20708 [00:56<03:30, 79.00graph/s]Processing graphs:  20%|█▉        | 4110/20708 [00:56<03:34, 77.54graph/s]Processing graphs:  20%|█▉        | 4118/20708 [00:56<03:43, 74.20graph/s]Processing graphs:  20%|█▉        | 4126/20708 [00:56<03:39, 75.52graph/s]Processing graphs:  20%|█▉        | 4134/20708 [00:56<03:37, 76.16graph/s]Processing graphs:  20%|██        | 4142/20708 [00:56<03:34, 77.11graph/s]Processing graphs:  20%|██        | 4150/20708 [00:56<03:36, 76.41graph/s]Processing graphs:  20%|██        | 4158/20708 [00:56<03:40, 75.00graph/s]Processing graphs:  20%|██        | 4166/20708 [00:56<03:40, 74.96graph/s]Processing graphs:  20%|██        | 4174/20708 [00:57<03:43, 74.08graph/s]Processing graphs:  20%|██        | 4182/20708 [00:57<03:47, 72.63graph/s]Processing graphs:  20%|██        | 4190/20708 [00:57<03:46, 72.99graph/s]Processing graphs:  20%|██        | 4198/20708 [00:57<03:44, 73.49graph/s]Processing graphs:  20%|██        | 4206/20708 [00:57<03:40, 74.87graph/s]Processing graphs:  20%|██        | 4215/20708 [00:57<03:34, 76.89graph/s]Processing graphs:  20%|██        | 4223/20708 [00:57<03:34, 76.96graph/s]Processing graphs:  20%|██        | 4231/20708 [00:57<03:35, 76.47graph/s]Processing graphs:  20%|██        | 4239/20708 [00:57<03:35, 76.25graph/s]Processing graphs:  21%|██        | 4247/20708 [00:58<03:36, 76.00graph/s]Processing graphs:  21%|██        | 4255/20708 [00:58<03:38, 75.40graph/s]Processing graphs:  21%|██        | 4263/20708 [00:58<03:36, 76.08graph/s]Processing graphs:  21%|██        | 4271/20708 [00:58<03:39, 74.91graph/s]Processing graphs:  21%|██        | 4279/20708 [00:58<03:44, 73.04graph/s]Processing graphs:  21%|██        | 4287/20708 [00:58<03:39, 74.78graph/s]Processing graphs:  21%|██        | 4295/20708 [00:58<03:38, 75.25graph/s]Processing graphs:  21%|██        | 4304/20708 [00:58<03:31, 77.48graph/s]Processing graphs:  21%|██        | 4312/20708 [00:58<03:32, 77.05graph/s]Processing graphs:  21%|██        | 4320/20708 [00:58<03:34, 76.42graph/s]Processing graphs:  21%|██        | 4328/20708 [00:59<03:34, 76.20graph/s]Processing graphs:  21%|██        | 4336/20708 [00:59<03:34, 76.42graph/s]Processing graphs:  21%|██        | 4344/20708 [00:59<03:36, 75.44graph/s]Processing graphs:  21%|██        | 4352/20708 [00:59<03:38, 74.91graph/s]Processing graphs:  21%|██        | 4360/20708 [00:59<03:37, 75.06graph/s]Processing graphs:  21%|██        | 4368/20708 [00:59<03:39, 74.61graph/s]Processing graphs:  21%|██        | 4376/20708 [00:59<03:37, 75.03graph/s]Processing graphs:  21%|██        | 4384/20708 [00:59<03:38, 74.61graph/s]Processing graphs:  21%|██        | 4392/20708 [00:59<03:37, 75.01graph/s]Processing graphs:  21%|██▏       | 4401/20708 [01:00<03:30, 77.49graph/s]Processing graphs:  21%|██▏       | 4409/20708 [01:00<03:42, 73.27graph/s]Processing graphs:  21%|██▏       | 4417/20708 [01:00<03:40, 73.99graph/s]Processing graphs:  21%|██▏       | 4426/20708 [01:00<03:30, 77.44graph/s]Processing graphs:  21%|██▏       | 4435/20708 [01:00<03:26, 78.80graph/s]Processing graphs:  21%|██▏       | 4443/20708 [01:00<03:29, 77.64graph/s]Processing graphs:  21%|██▏       | 4451/20708 [01:00<03:27, 78.17graph/s]Processing graphs:  22%|██▏       | 4460/20708 [01:00<03:27, 78.31graph/s]Processing graphs:  22%|██▏       | 4469/20708 [01:00<03:25, 79.00graph/s]Processing graphs:  22%|██▏       | 4477/20708 [01:01<03:30, 76.97graph/s]Processing graphs:  22%|██▏       | 4485/20708 [01:01<03:35, 75.44graph/s]Processing graphs:  22%|██▏       | 4494/20708 [01:01<03:24, 79.10graph/s]Processing graphs:  22%|██▏       | 4502/20708 [01:01<03:26, 78.57graph/s]Processing graphs:  22%|██▏       | 4510/20708 [01:01<03:30, 77.09graph/s]Processing graphs:  22%|██▏       | 4518/20708 [01:01<03:30, 76.89graph/s]Processing graphs:  22%|██▏       | 4526/20708 [01:01<03:35, 75.08graph/s]Processing graphs:  22%|██▏       | 4534/20708 [01:01<03:36, 74.80graph/s]Processing graphs:  22%|██▏       | 4542/20708 [01:01<03:32, 76.24graph/s]Processing graphs:  22%|██▏       | 4550/20708 [01:01<03:32, 76.07graph/s]Processing graphs:  22%|██▏       | 4558/20708 [01:02<03:32, 75.87graph/s]Processing graphs:  22%|██▏       | 4566/20708 [01:02<03:32, 75.94graph/s]Processing graphs:  22%|██▏       | 4574/20708 [01:02<03:35, 74.72graph/s]Processing graphs:  22%|██▏       | 4582/20708 [01:02<03:31, 76.21graph/s]Processing graphs:  22%|██▏       | 4590/20708 [01:02<03:34, 75.17graph/s]Processing graphs:  22%|██▏       | 4598/20708 [01:02<03:43, 72.00graph/s]Processing graphs:  22%|██▏       | 4606/20708 [01:02<03:42, 72.29graph/s]Processing graphs:  22%|██▏       | 4614/20708 [01:02<03:40, 72.90graph/s]Processing graphs:  22%|██▏       | 4623/20708 [01:02<03:32, 75.57graph/s]Processing graphs:  22%|██▏       | 4631/20708 [01:03<03:36, 74.29graph/s]Processing graphs:  22%|██▏       | 4639/20708 [01:03<03:32, 75.79graph/s]Processing graphs:  22%|██▏       | 4647/20708 [01:03<03:39, 73.15graph/s]Processing graphs:  22%|██▏       | 4656/20708 [01:03<03:32, 75.60graph/s]Processing graphs:  23%|██▎       | 4664/20708 [01:03<03:30, 76.14graph/s]Processing graphs:  23%|██▎       | 4672/20708 [01:03<03:31, 75.81graph/s]Processing graphs:  23%|██▎       | 4681/20708 [01:03<03:26, 77.76graph/s]Processing graphs:  23%|██▎       | 4689/20708 [01:03<03:29, 76.47graph/s]Processing graphs:  23%|██▎       | 4698/20708 [01:03<03:22, 79.01graph/s]Processing graphs:  23%|██▎       | 4707/20708 [01:04<03:19, 80.18graph/s]Processing graphs:  23%|██▎       | 4716/20708 [01:04<03:20, 79.80graph/s]Processing graphs:  23%|██▎       | 4724/20708 [01:04<03:22, 78.93graph/s]Processing graphs:  23%|██▎       | 4732/20708 [01:04<03:25, 77.66graph/s]Processing graphs:  23%|██▎       | 4740/20708 [01:04<03:32, 75.12graph/s]Processing graphs:  23%|██▎       | 4748/20708 [01:04<03:30, 75.68graph/s]Processing graphs:  23%|██▎       | 4756/20708 [01:04<03:34, 74.30graph/s]Processing graphs:  23%|██▎       | 4764/20708 [01:04<03:30, 75.84graph/s]Processing graphs:  23%|██▎       | 4772/20708 [01:04<03:34, 74.38graph/s]Processing graphs:  23%|██▎       | 4780/20708 [01:05<03:32, 74.93graph/s]Processing graphs:  23%|██▎       | 4788/20708 [01:05<03:35, 73.95graph/s]Processing graphs:  23%|██▎       | 4796/20708 [01:05<03:34, 74.15graph/s]Processing graphs:  23%|██▎       | 4804/20708 [01:05<03:31, 75.20graph/s]Processing graphs:  23%|██▎       | 4812/20708 [01:05<03:31, 75.17graph/s]Processing graphs:  23%|██▎       | 4820/20708 [01:05<03:32, 74.68graph/s]Processing graphs:  23%|██▎       | 4829/20708 [01:05<03:25, 77.31graph/s]Processing graphs:  23%|██▎       | 4837/20708 [01:05<03:26, 76.76graph/s]Processing graphs:  23%|██▎       | 4845/20708 [01:05<03:24, 77.40graph/s]Processing graphs:  23%|██▎       | 4853/20708 [01:05<03:25, 77.18graph/s]Processing graphs:  23%|██▎       | 4861/20708 [01:06<03:29, 75.49graph/s]Processing graphs:  24%|██▎       | 4869/20708 [01:06<03:32, 74.36graph/s]Processing graphs:  24%|██▎       | 4877/20708 [01:06<03:36, 73.04graph/s]Processing graphs:  24%|██▎       | 4885/20708 [01:06<03:32, 74.56graph/s]Processing graphs:  24%|██▎       | 4893/20708 [01:06<03:35, 73.45graph/s]Processing graphs:  24%|██▎       | 4901/20708 [01:06<03:32, 74.42graph/s]Processing graphs:  24%|██▎       | 4909/20708 [01:06<03:28, 75.85graph/s]Processing graphs:  24%|██▎       | 4917/20708 [01:06<03:32, 74.28graph/s]Processing graphs:  24%|██▍       | 4925/20708 [01:06<03:28, 75.52graph/s]Processing graphs:  24%|██▍       | 4933/20708 [01:07<03:27, 76.00graph/s]Processing graphs:  24%|██▍       | 4941/20708 [01:07<03:31, 74.38graph/s]Processing graphs:  24%|██▍       | 4949/20708 [01:07<03:28, 75.43graph/s]Processing graphs:  24%|██▍       | 4957/20708 [01:07<03:30, 74.72graph/s]Processing graphs:  24%|██▍       | 4965/20708 [01:07<03:32, 74.21graph/s]Processing graphs:  24%|██▍       | 4973/20708 [01:07<03:28, 75.65graph/s]Processing graphs:  24%|██▍       | 4981/20708 [01:07<03:31, 74.30graph/s]Processing graphs:  24%|██▍       | 4990/20708 [01:07<03:19, 78.76graph/s]Processing graphs:  24%|██▍       | 4999/20708 [01:07<03:17, 79.60graph/s]Processing graphs:  24%|██▍       | 5007/20708 [01:08<03:21, 78.00graph/s]Processing graphs:  24%|██▍       | 5015/20708 [01:08<03:21, 77.88graph/s]Processing graphs:  24%|██▍       | 5023/20708 [01:08<03:28, 75.34graph/s]Processing graphs:  24%|██▍       | 5033/20708 [01:08<03:18, 79.07graph/s]Processing graphs:  24%|██▍       | 5041/20708 [01:08<03:19, 78.70graph/s]Processing graphs:  24%|██▍       | 5049/20708 [01:08<03:26, 75.83graph/s]Processing graphs:  24%|██▍       | 5058/20708 [01:08<03:21, 77.59graph/s]Processing graphs:  24%|██▍       | 5066/20708 [01:08<03:20, 77.95graph/s]Processing graphs:  25%|██▍       | 5076/20708 [01:08<03:09, 82.63graph/s]Processing graphs:  25%|██▍       | 5085/20708 [01:09<03:12, 81.17graph/s]Processing graphs:  25%|██▍       | 5094/20708 [01:09<03:19, 78.46graph/s]Processing graphs:  25%|██▍       | 5102/20708 [01:09<03:18, 78.71graph/s]Processing graphs:  25%|██▍       | 5111/20708 [01:09<03:13, 80.72graph/s]Processing graphs:  25%|██▍       | 5121/20708 [01:09<03:04, 84.53graph/s]Processing graphs:  25%|██▍       | 5130/20708 [01:09<03:14, 79.92graph/s]Processing graphs:  25%|██▍       | 5139/20708 [01:09<03:08, 82.39graph/s]Processing graphs:  25%|██▍       | 5148/20708 [01:09<03:09, 82.16graph/s]Processing graphs:  25%|██▍       | 5157/20708 [01:09<03:13, 80.28graph/s]Processing graphs:  25%|██▍       | 5167/20708 [01:10<03:02, 85.18graph/s]Processing graphs:  25%|██▍       | 5176/20708 [01:10<03:00, 86.19graph/s]Processing graphs:  25%|██▌       | 5185/20708 [01:10<03:01, 85.67graph/s]Processing graphs:  25%|██▌       | 5194/20708 [01:10<03:07, 82.90graph/s]Processing graphs:  25%|██▌       | 5203/20708 [01:10<03:04, 83.84graph/s]Processing graphs:  25%|██▌       | 5212/20708 [01:10<03:06, 83.29graph/s]Processing graphs:  25%|██▌       | 5221/20708 [01:10<03:12, 80.57graph/s]Processing graphs:  25%|██▌       | 5230/20708 [01:10<03:19, 77.74graph/s]Processing graphs:  25%|██▌       | 5238/20708 [01:10<03:19, 77.38graph/s]Processing graphs:  25%|██▌       | 5247/20708 [01:11<03:17, 78.20graph/s]Processing graphs:  25%|██▌       | 5255/20708 [01:11<03:19, 77.27graph/s]Processing graphs:  25%|██▌       | 5263/20708 [01:11<03:27, 74.30graph/s]Processing graphs:  25%|██▌       | 5271/20708 [01:11<03:28, 74.17graph/s]Processing graphs:  26%|██▌       | 5281/20708 [01:11<03:10, 81.01graph/s]Processing graphs:  26%|██▌       | 5290/20708 [01:11<03:08, 81.73graph/s]Processing graphs:  26%|██▌       | 5300/20708 [01:11<03:00, 85.57graph/s]Processing graphs:  26%|██▌       | 5309/20708 [01:11<02:57, 86.69graph/s]Processing graphs:  26%|██▌       | 5318/20708 [01:11<03:04, 83.51graph/s]Processing graphs:  26%|██▌       | 5327/20708 [01:11<03:06, 82.61graph/s]Processing graphs:  26%|██▌       | 5336/20708 [01:12<03:05, 82.66graph/s]Processing graphs:  26%|██▌       | 5345/20708 [01:12<03:06, 82.58graph/s]Processing graphs:  26%|██▌       | 5355/20708 [01:12<02:57, 86.37graph/s]Processing graphs:  26%|██▌       | 5364/20708 [01:12<03:06, 82.35graph/s]Processing graphs:  26%|██▌       | 5374/20708 [01:12<02:59, 85.52graph/s]Processing graphs:  26%|██▌       | 5384/20708 [01:12<02:54, 88.01graph/s]Processing graphs:  26%|██▌       | 5393/20708 [01:12<02:55, 87.31graph/s]Processing graphs:  26%|██▌       | 5402/20708 [01:12<02:58, 85.52graph/s]Processing graphs:  26%|██▌       | 5411/20708 [01:12<03:00, 84.88graph/s]Processing graphs:  26%|██▌       | 5420/20708 [01:13<03:04, 83.08graph/s]Processing graphs:  26%|██▌       | 5429/20708 [01:13<03:03, 83.24graph/s]Processing graphs:  26%|██▋       | 5439/20708 [01:13<02:59, 85.22graph/s]Processing graphs:  26%|██▋       | 5448/20708 [01:13<03:08, 80.99graph/s]Processing graphs:  26%|██▋       | 5457/20708 [01:13<03:10, 79.93graph/s]Processing graphs:  26%|██▋       | 5466/20708 [01:13<03:13, 78.59graph/s]Processing graphs:  26%|██▋       | 5475/20708 [01:13<03:09, 80.35graph/s]Processing graphs:  26%|██▋       | 5484/20708 [01:13<03:07, 81.01graph/s]Processing graphs:  27%|██▋       | 5493/20708 [01:14<03:14, 78.20graph/s]Processing graphs:  27%|██▋       | 5501/20708 [01:14<03:14, 78.32graph/s]Processing graphs:  27%|██▋       | 5509/20708 [01:14<03:17, 77.13graph/s]Processing graphs:  27%|██▋       | 5517/20708 [01:14<03:18, 76.52graph/s]Processing graphs:  27%|██▋       | 5525/20708 [01:14<03:20, 75.87graph/s]Processing graphs:  27%|██▋       | 5534/20708 [01:14<03:15, 77.72graph/s]Processing graphs:  27%|██▋       | 5543/20708 [01:14<03:07, 80.84graph/s]Processing graphs:  27%|██▋       | 5552/20708 [01:14<03:17, 76.57graph/s]Processing graphs:  27%|██▋       | 5561/20708 [01:14<03:13, 78.36graph/s]Processing graphs:  27%|██▋       | 5569/20708 [01:14<03:16, 77.01graph/s]Processing graphs:  27%|██▋       | 5577/20708 [01:15<03:17, 76.69graph/s]Processing graphs:  27%|██▋       | 5585/20708 [01:15<03:17, 76.48graph/s]Processing graphs:  27%|██▋       | 5594/20708 [01:15<03:11, 79.05graph/s]Processing graphs:  27%|██▋       | 5603/20708 [01:15<03:07, 80.76graph/s]Processing graphs:  27%|██▋       | 5612/20708 [01:15<03:05, 81.59graph/s]Processing graphs:  27%|██▋       | 5621/20708 [01:15<03:04, 81.69graph/s]Processing graphs:  27%|██▋       | 5630/20708 [01:15<03:02, 82.84graph/s]Processing graphs:  27%|██▋       | 5640/20708 [01:15<02:55, 86.02graph/s]Processing graphs:  27%|██▋       | 5649/20708 [01:15<03:02, 82.37graph/s]Processing graphs:  27%|██▋       | 5658/20708 [01:16<03:07, 80.20graph/s]Processing graphs:  27%|██▋       | 5667/20708 [01:16<03:06, 80.61graph/s]Processing graphs:  27%|██▋       | 5676/20708 [01:16<03:08, 79.94graph/s]Processing graphs:  27%|██▋       | 5685/20708 [01:16<03:06, 80.61graph/s]Processing graphs:  27%|██▋       | 5694/20708 [01:16<03:02, 82.16graph/s]Processing graphs:  28%|██▊       | 5703/20708 [01:16<03:01, 82.76graph/s]Processing graphs:  28%|██▊       | 5712/20708 [01:16<02:59, 83.73graph/s]Processing graphs:  28%|██▊       | 5722/20708 [01:16<02:53, 86.47graph/s]Processing graphs:  28%|██▊       | 5732/20708 [01:16<02:48, 88.89graph/s]Processing graphs:  28%|██▊       | 5741/20708 [01:17<02:53, 86.05graph/s]Processing graphs:  28%|██▊       | 5751/20708 [01:17<02:52, 86.87graph/s]Processing graphs:  28%|██▊       | 5761/20708 [01:17<02:50, 87.56graph/s]Processing graphs:  28%|██▊       | 5771/20708 [01:17<02:47, 89.26graph/s]Processing graphs:  28%|██▊       | 5780/20708 [01:17<02:55, 85.00graph/s]Processing graphs:  28%|██▊       | 5789/20708 [01:17<02:53, 85.79graph/s]Processing graphs:  28%|██▊       | 5798/20708 [01:17<02:55, 85.14graph/s]Processing graphs:  28%|██▊       | 5807/20708 [01:17<02:55, 85.03graph/s]Processing graphs:  28%|██▊       | 5816/20708 [01:17<02:52, 86.20graph/s]Processing graphs:  28%|██▊       | 5826/20708 [01:18<02:48, 88.10graph/s]Processing graphs:  28%|██▊       | 5835/20708 [01:18<02:55, 84.84graph/s]Processing graphs:  28%|██▊       | 5844/20708 [01:18<02:55, 84.60graph/s]Processing graphs:  28%|██▊       | 5853/20708 [01:18<03:01, 81.63graph/s]Processing graphs:  28%|██▊       | 5863/20708 [01:18<02:58, 83.15graph/s]Processing graphs:  28%|██▊       | 5872/20708 [01:18<02:59, 82.81graph/s]Processing graphs:  28%|██▊       | 5881/20708 [01:18<02:58, 83.04graph/s]Processing graphs:  28%|██▊       | 5891/20708 [01:18<02:51, 86.64graph/s]Processing graphs:  28%|██▊       | 5900/20708 [01:18<02:52, 85.77graph/s]Processing graphs:  29%|██▊       | 5909/20708 [01:19<02:51, 86.31graph/s]Processing graphs:  29%|██▊       | 5920/20708 [01:19<02:44, 89.86graph/s]Processing graphs:  29%|██▊       | 5929/20708 [01:19<02:50, 86.84graph/s]Processing graphs:  29%|██▊       | 5938/20708 [01:19<02:57, 83.23graph/s]Processing graphs:  29%|██▊       | 5947/20708 [01:19<03:02, 80.79graph/s]Processing graphs:  29%|██▉       | 5956/20708 [01:19<03:03, 80.53graph/s]Processing graphs:  29%|██▉       | 5965/20708 [01:19<03:08, 78.09graph/s]Processing graphs:  29%|██▉       | 5973/20708 [01:19<03:09, 77.92graph/s]Processing graphs:  29%|██▉       | 5982/20708 [01:19<03:03, 80.16graph/s]Processing graphs:  29%|██▉       | 5991/20708 [01:20<03:10, 77.21graph/s]Processing graphs:  29%|██▉       | 5999/20708 [01:20<03:11, 76.96graph/s]Processing graphs:  29%|██▉       | 6007/20708 [01:20<03:15, 75.09graph/s]Processing graphs:  29%|██▉       | 6015/20708 [01:20<03:14, 75.52graph/s]Processing graphs:  29%|██▉       | 6024/20708 [01:20<03:08, 77.78graph/s]Processing graphs:  29%|██▉       | 6032/20708 [01:20<03:17, 74.32graph/s]Processing graphs:  29%|██▉       | 6041/20708 [01:20<03:13, 75.91graph/s]Processing graphs:  29%|██▉       | 6050/20708 [01:20<03:05, 78.88graph/s]Processing graphs:  29%|██▉       | 6059/20708 [01:20<03:04, 79.20graph/s]Processing graphs:  29%|██▉       | 6067/20708 [01:21<03:10, 77.06graph/s]Processing graphs:  29%|██▉       | 6075/20708 [01:21<03:10, 76.89graph/s]Processing graphs:  29%|██▉       | 6085/20708 [01:21<02:57, 82.25graph/s]Processing graphs:  29%|██▉       | 6094/20708 [01:21<02:55, 83.27graph/s]Processing graphs:  29%|██▉       | 6103/20708 [01:21<02:54, 83.76graph/s]Processing graphs:  30%|██▉       | 6112/20708 [01:21<02:57, 82.18graph/s]Processing graphs:  30%|██▉       | 6121/20708 [01:21<03:00, 80.62graph/s]Processing graphs:  30%|██▉       | 6130/20708 [01:21<02:58, 81.49graph/s]Processing graphs:  30%|██▉       | 6139/20708 [01:21<03:03, 79.36graph/s]Processing graphs:  30%|██▉       | 6148/20708 [01:22<03:00, 80.82graph/s]Processing graphs:  30%|██▉       | 6157/20708 [01:22<02:59, 81.12graph/s]Processing graphs:  30%|██▉       | 6166/20708 [01:22<03:04, 78.76graph/s]Processing graphs:  30%|██▉       | 6176/20708 [01:22<02:57, 81.85graph/s]Processing graphs:  30%|██▉       | 6185/20708 [01:22<03:08, 76.94graph/s]Processing graphs:  30%|██▉       | 6195/20708 [01:22<02:58, 81.21graph/s]Processing graphs:  30%|██▉       | 6204/20708 [01:22<02:55, 82.81graph/s]Processing graphs:  30%|███       | 6213/20708 [01:22<02:51, 84.29graph/s]Processing graphs:  30%|███       | 6222/20708 [01:22<02:58, 80.97graph/s]Processing graphs:  30%|███       | 6231/20708 [01:23<03:05, 77.94graph/s]Processing graphs:  30%|███       | 6239/20708 [01:23<03:05, 77.94graph/s]Processing graphs:  30%|███       | 6247/20708 [01:23<03:11, 75.59graph/s]Processing graphs:  30%|███       | 6256/20708 [01:23<03:04, 78.23graph/s]Processing graphs:  30%|███       | 6265/20708 [01:23<03:01, 79.37graph/s]Processing graphs:  30%|███       | 6273/20708 [01:23<03:06, 77.36graph/s]Processing graphs:  30%|███       | 6281/20708 [01:23<03:13, 74.45graph/s]Processing graphs:  30%|███       | 6289/20708 [01:23<03:22, 71.14graph/s]Processing graphs:  30%|███       | 6298/20708 [01:23<03:15, 73.66graph/s]Processing graphs:  30%|███       | 6307/20708 [01:24<03:05, 77.67graph/s]Processing graphs:  30%|███       | 6315/20708 [01:24<03:07, 76.86graph/s]Processing graphs:  31%|███       | 6323/20708 [01:24<03:05, 77.64graph/s]Processing graphs:  31%|███       | 6331/20708 [01:24<03:07, 76.67graph/s]Processing graphs:  31%|███       | 6339/20708 [01:24<03:11, 74.93graph/s]Processing graphs:  31%|███       | 6349/20708 [01:24<03:00, 79.53graph/s]Processing graphs:  31%|███       | 6359/20708 [01:24<02:50, 84.32graph/s]Processing graphs:  31%|███       | 6368/20708 [01:24<02:57, 81.01graph/s]Processing graphs:  31%|███       | 6377/20708 [01:24<03:06, 77.01graph/s]Processing graphs:  31%|███       | 6386/20708 [01:25<03:04, 77.79graph/s]Processing graphs:  31%|███       | 6394/20708 [01:25<03:07, 76.39graph/s]Processing graphs:  31%|███       | 6403/20708 [01:25<03:05, 77.29graph/s]Processing graphs:  31%|███       | 6413/20708 [01:25<02:56, 80.95graph/s]Processing graphs:  31%|███       | 6422/20708 [01:25<03:02, 78.41graph/s]Processing graphs:  31%|███       | 6430/20708 [01:25<03:07, 76.14graph/s]Processing graphs:  31%|███       | 6439/20708 [01:25<03:00, 79.27graph/s]Processing graphs:  31%|███       | 6447/20708 [01:25<03:04, 77.14graph/s]Processing graphs:  31%|███       | 6455/20708 [01:25<03:09, 75.24graph/s]Processing graphs:  31%|███       | 6463/20708 [01:26<03:06, 76.41graph/s]Processing graphs:  31%|███▏      | 6472/20708 [01:26<02:59, 79.19graph/s]Processing graphs:  31%|███▏      | 6480/20708 [01:26<03:08, 75.45graph/s]Processing graphs:  31%|███▏      | 6488/20708 [01:26<03:08, 75.61graph/s]Processing graphs:  31%|███▏      | 6496/20708 [01:26<03:10, 74.60graph/s]Processing graphs:  31%|███▏      | 6504/20708 [01:26<03:12, 73.90graph/s]Processing graphs:  31%|███▏      | 6514/20708 [01:26<03:00, 78.57graph/s]Processing graphs:  31%|███▏      | 6523/20708 [01:26<02:55, 80.70graph/s]Processing graphs:  32%|███▏      | 6532/20708 [01:26<02:52, 82.37graph/s]Processing graphs:  32%|███▏      | 6541/20708 [01:27<02:56, 80.33graph/s]Processing graphs:  32%|███▏      | 6550/20708 [01:27<02:53, 81.58graph/s]Processing graphs:  32%|███▏      | 6560/20708 [01:27<02:46, 85.16graph/s]Processing graphs:  32%|███▏      | 6569/20708 [01:27<02:50, 83.09graph/s]Processing graphs:  32%|███▏      | 6578/20708 [01:27<02:55, 80.37graph/s]Processing graphs:  32%|███▏      | 6587/20708 [01:27<02:51, 82.47graph/s]Processing graphs:  32%|███▏      | 6596/20708 [01:27<02:51, 82.46graph/s]Processing graphs:  32%|███▏      | 6605/20708 [01:27<02:52, 81.74graph/s]Processing graphs:  32%|███▏      | 6614/20708 [01:27<02:59, 78.64graph/s]Processing graphs:  32%|███▏      | 6624/20708 [01:28<02:52, 81.46graph/s]Processing graphs:  32%|███▏      | 6633/20708 [01:28<02:59, 78.55graph/s]Processing graphs:  32%|███▏      | 6642/20708 [01:28<02:57, 79.13graph/s]Processing graphs:  32%|███▏      | 6652/20708 [01:28<02:47, 83.79graph/s]Processing graphs:  32%|███▏      | 6661/20708 [01:28<02:56, 79.70graph/s]Processing graphs:  32%|███▏      | 6670/20708 [01:28<02:59, 78.39graph/s]Processing graphs:  32%|███▏      | 6679/20708 [01:28<02:56, 79.49graph/s]Processing graphs:  32%|███▏      | 6689/20708 [01:28<02:47, 83.73graph/s]Processing graphs:  32%|███▏      | 6698/20708 [01:29<02:55, 80.04graph/s]Processing graphs:  32%|███▏      | 6707/20708 [01:29<02:53, 80.58graph/s]Processing graphs:  32%|███▏      | 6716/20708 [01:29<02:50, 82.21graph/s]Processing graphs:  32%|███▏      | 6725/20708 [01:29<02:51, 81.46graph/s]Processing graphs:  33%|███▎      | 6734/20708 [01:29<02:52, 81.03graph/s]Processing graphs:  33%|███▎      | 6743/20708 [01:29<02:56, 79.23graph/s]Processing graphs:  33%|███▎      | 6751/20708 [01:29<02:55, 79.38graph/s]Processing graphs:  33%|███▎      | 6759/20708 [01:29<02:59, 77.83graph/s]Processing graphs:  33%|███▎      | 6767/20708 [01:29<02:59, 77.63graph/s]Processing graphs:  33%|███▎      | 6775/20708 [01:29<02:59, 77.68graph/s]Processing graphs:  33%|███▎      | 6784/20708 [01:30<02:57, 78.30graph/s]Processing graphs:  33%|███▎      | 6792/20708 [01:30<03:01, 76.81graph/s]Processing graphs:  33%|███▎      | 6801/20708 [01:30<02:54, 79.55graph/s]Processing graphs:  33%|███▎      | 6810/20708 [01:30<02:49, 81.78graph/s]Processing graphs:  33%|███▎      | 6819/20708 [01:30<02:54, 79.43graph/s]Processing graphs:  33%|███▎      | 6828/20708 [01:30<02:51, 81.16graph/s]Processing graphs:  33%|███▎      | 6837/20708 [01:30<02:57, 78.16graph/s]Processing graphs:  33%|███▎      | 6847/20708 [01:30<02:46, 83.38graph/s]Processing graphs:  33%|███▎      | 6856/20708 [01:30<02:48, 82.38graph/s]Processing graphs:  33%|███▎      | 6865/20708 [01:31<02:53, 79.91graph/s]Processing graphs:  33%|███▎      | 6875/20708 [01:31<02:45, 83.34graph/s]Processing graphs:  33%|███▎      | 6884/20708 [01:31<02:47, 82.74graph/s]Processing graphs:  33%|███▎      | 6893/20708 [01:31<02:55, 78.64graph/s]Processing graphs:  33%|███▎      | 6901/20708 [01:31<02:58, 77.52graph/s]Processing graphs:  33%|███▎      | 6910/20708 [01:31<02:55, 78.59graph/s]Processing graphs:  33%|███▎      | 6919/20708 [01:31<02:53, 79.61graph/s]Processing graphs:  33%|███▎      | 6928/20708 [01:31<02:50, 81.01graph/s]Processing graphs:  34%|███▎      | 6938/20708 [01:31<02:40, 85.59graph/s]Processing graphs:  34%|███▎      | 6947/20708 [01:32<02:47, 81.99graph/s]Processing graphs:  34%|███▎      | 6956/20708 [01:32<02:58, 76.90graph/s]Processing graphs:  34%|███▎      | 6966/20708 [01:32<02:51, 80.10graph/s]Processing graphs:  34%|███▎      | 6975/20708 [01:32<02:49, 80.79graph/s]Processing graphs:  34%|███▎      | 6984/20708 [01:32<02:53, 79.23graph/s]Processing graphs:  34%|███▍      | 6993/20708 [01:32<02:52, 79.30graph/s]Processing graphs:  34%|███▍      | 7001/20708 [01:32<02:59, 76.37graph/s]Processing graphs:  34%|███▍      | 7010/20708 [01:32<02:55, 78.04graph/s]Processing graphs:  34%|███▍      | 7019/20708 [01:33<02:52, 79.16graph/s]Processing graphs:  34%|███▍      | 7027/20708 [01:33<02:53, 78.66graph/s]Processing graphs:  34%|███▍      | 7036/20708 [01:33<02:49, 80.48graph/s]Processing graphs:  34%|███▍      | 7045/20708 [01:33<02:50, 80.34graph/s]Processing graphs:  34%|███▍      | 7054/20708 [01:33<02:54, 78.44graph/s]Processing graphs:  34%|███▍      | 7063/20708 [01:33<02:51, 79.37graph/s]Processing graphs:  34%|███▍      | 7072/20708 [01:33<02:49, 80.53graph/s]Processing graphs:  34%|███▍      | 7081/20708 [01:33<02:53, 78.45graph/s]Processing graphs:  34%|███▍      | 7090/20708 [01:33<02:49, 80.32graph/s]Processing graphs:  34%|███▍      | 7099/20708 [01:34<02:46, 81.75graph/s]Processing graphs:  34%|███▍      | 7108/20708 [01:34<02:46, 81.47graph/s]Processing graphs:  34%|███▍      | 7117/20708 [01:34<02:52, 79.01graph/s]Processing graphs:  34%|███▍      | 7126/20708 [01:34<02:46, 81.68graph/s]Processing graphs:  34%|███▍      | 7135/20708 [01:34<02:46, 81.71graph/s]Processing graphs:  34%|███▍      | 7144/20708 [01:34<02:50, 79.36graph/s]Processing graphs:  35%|███▍      | 7152/20708 [01:34<02:55, 77.34graph/s]Processing graphs:  35%|███▍      | 7160/20708 [01:34<02:54, 77.74graph/s]Processing graphs:  35%|███▍      | 7168/20708 [01:34<02:59, 75.24graph/s]Processing graphs:  35%|███▍      | 7176/20708 [01:35<02:57, 76.28graph/s]Processing graphs:  35%|███▍      | 7184/20708 [01:35<02:57, 76.17graph/s]Processing graphs:  35%|███▍      | 7192/20708 [01:35<02:55, 76.94graph/s]Processing graphs:  35%|███▍      | 7201/20708 [01:35<02:50, 79.03graph/s]Processing graphs:  35%|███▍      | 7209/20708 [01:35<02:51, 78.86graph/s]Processing graphs:  35%|███▍      | 7218/20708 [01:35<02:50, 79.04graph/s]Processing graphs:  35%|███▍      | 7227/20708 [01:35<02:46, 80.82graph/s]Processing graphs:  35%|███▍      | 7236/20708 [01:35<02:51, 78.73graph/s]Processing graphs:  35%|███▍      | 7244/20708 [01:35<02:52, 78.15graph/s]Processing graphs:  35%|███▌      | 7252/20708 [01:35<02:51, 78.27graph/s]Processing graphs:  35%|███▌      | 7261/20708 [01:36<02:47, 80.28graph/s]Processing graphs:  35%|███▌      | 7270/20708 [01:36<02:44, 81.51graph/s]Processing graphs:  35%|███▌      | 7279/20708 [01:36<02:47, 80.41graph/s]Processing graphs:  35%|███▌      | 7288/20708 [01:36<02:48, 79.44graph/s]Processing graphs:  35%|███▌      | 7296/20708 [01:36<02:52, 77.79graph/s]Processing graphs:  35%|███▌      | 7305/20708 [01:36<02:48, 79.45graph/s]Processing graphs:  35%|███▌      | 7314/20708 [01:36<02:45, 80.93graph/s]Processing graphs:  35%|███▌      | 7323/20708 [01:36<02:51, 78.12graph/s]Processing graphs:  35%|███▌      | 7332/20708 [01:36<02:44, 81.30graph/s]Processing graphs:  35%|███▌      | 7341/20708 [01:37<02:44, 81.22graph/s]Processing graphs:  35%|███▌      | 7350/20708 [01:37<02:48, 79.39graph/s]Processing graphs:  36%|███▌      | 7359/20708 [01:37<02:45, 80.42graph/s]Processing graphs:  36%|███▌      | 7368/20708 [01:37<02:48, 79.16graph/s]Processing graphs:  36%|███▌      | 7377/20708 [01:37<02:45, 80.39graph/s]Processing graphs:  36%|███▌      | 7386/20708 [01:37<02:45, 80.29graph/s]Processing graphs:  36%|███▌      | 7395/20708 [01:37<02:50, 78.11graph/s]Processing graphs:  36%|███▌      | 7403/20708 [01:37<02:53, 76.70graph/s]Processing graphs:  36%|███▌      | 7411/20708 [01:37<02:52, 76.92graph/s]Processing graphs:  36%|███▌      | 7420/20708 [01:38<02:45, 80.42graph/s]Processing graphs:  36%|███▌      | 7429/20708 [01:38<02:50, 77.94graph/s]Processing graphs:  36%|███▌      | 7437/20708 [01:38<02:55, 75.74graph/s]Processing graphs:  36%|███▌      | 7446/20708 [01:38<02:48, 78.75graph/s]Processing graphs:  36%|███▌      | 7454/20708 [01:38<02:52, 77.02graph/s]Processing graphs:  36%|███▌      | 7462/20708 [01:38<02:52, 76.86graph/s]Processing graphs:  36%|███▌      | 7470/20708 [01:38<02:51, 77.40graph/s]Processing graphs:  36%|███▌      | 7479/20708 [01:38<02:47, 79.07graph/s]Processing graphs:  36%|███▌      | 7487/20708 [01:38<02:49, 78.02graph/s]Processing graphs:  36%|███▌      | 7496/20708 [01:39<02:45, 79.72graph/s]Processing graphs:  36%|███▌      | 7506/20708 [01:39<02:39, 82.61graph/s]Processing graphs:  36%|███▋      | 7515/20708 [01:39<02:38, 83.33graph/s]Processing graphs:  36%|███▋      | 7524/20708 [01:39<02:44, 80.12graph/s]Processing graphs:  36%|███▋      | 7533/20708 [01:39<02:50, 77.08graph/s]Processing graphs:  36%|███▋      | 7542/20708 [01:39<02:47, 78.73graph/s]Processing graphs:  36%|███▋      | 7550/20708 [01:39<02:48, 77.99graph/s]Processing graphs:  37%|███▋      | 7559/20708 [01:39<02:48, 78.17graph/s]Processing graphs:  37%|███▋      | 7568/20708 [01:39<02:42, 80.80graph/s]Processing graphs:  37%|███▋      | 7577/20708 [01:40<02:43, 80.13graph/s]Processing graphs:  37%|███▋      | 7586/20708 [01:40<02:44, 79.74graph/s]Processing graphs:  37%|███▋      | 7594/20708 [01:40<02:44, 79.69graph/s]Processing graphs:  37%|███▋      | 7603/20708 [01:40<02:44, 79.63graph/s]Processing graphs:  37%|███▋      | 7612/20708 [01:40<02:43, 80.29graph/s]Processing graphs:  37%|███▋      | 7621/20708 [01:40<02:48, 77.69graph/s]Processing graphs:  37%|███▋      | 7629/20708 [01:40<02:47, 77.97graph/s]Processing graphs:  37%|███▋      | 7638/20708 [01:40<02:41, 80.74graph/s]Processing graphs:  37%|███▋      | 7647/20708 [01:40<02:43, 80.00graph/s]Processing graphs:  37%|███▋      | 7656/20708 [01:41<02:44, 79.49graph/s]Processing graphs:  37%|███▋      | 7665/20708 [01:41<02:42, 80.39graph/s]Processing graphs:  37%|███▋      | 7674/20708 [01:41<02:41, 80.48graph/s]Processing graphs:  37%|███▋      | 7683/20708 [01:41<02:41, 80.70graph/s]Processing graphs:  37%|███▋      | 7692/20708 [01:41<02:43, 79.59graph/s]Processing graphs:  37%|███▋      | 7701/20708 [01:41<02:40, 81.05graph/s]Processing graphs:  37%|███▋      | 7710/20708 [01:41<02:38, 82.05graph/s]Processing graphs:  37%|███▋      | 7719/20708 [01:41<02:38, 82.05graph/s]Processing graphs:  37%|███▋      | 7728/20708 [01:41<02:42, 79.86graph/s]Processing graphs:  37%|███▋      | 7737/20708 [01:42<02:40, 81.06graph/s]Processing graphs:  37%|███▋      | 7746/20708 [01:42<02:42, 79.98graph/s]Processing graphs:  37%|███▋      | 7755/20708 [01:42<02:40, 80.66graph/s]Processing graphs:  37%|███▋      | 7764/20708 [01:42<02:43, 79.15graph/s]Processing graphs:  38%|███▊      | 7773/20708 [01:42<02:42, 79.37graph/s]Processing graphs:  38%|███▊      | 7782/20708 [01:42<02:37, 82.15graph/s]Processing graphs:  38%|███▊      | 7791/20708 [01:42<02:34, 83.76graph/s]Processing graphs:  38%|███▊      | 7800/20708 [01:42<02:35, 83.22graph/s]Processing graphs:  38%|███▊      | 7809/20708 [01:42<02:37, 81.78graph/s]Processing graphs:  38%|███▊      | 7818/20708 [01:43<02:45, 77.83graph/s]Processing graphs:  38%|███▊      | 7827/20708 [01:43<02:44, 78.36graph/s]Processing graphs:  38%|███▊      | 7835/20708 [01:43<02:46, 77.35graph/s]Processing graphs:  38%|███▊      | 7843/20708 [01:43<02:50, 75.25graph/s]Processing graphs:  38%|███▊      | 7851/20708 [01:43<02:53, 73.94graph/s]Processing graphs:  38%|███▊      | 7860/20708 [01:43<02:45, 77.77graph/s]Processing graphs:  38%|███▊      | 7869/20708 [01:43<02:40, 79.93graph/s]Processing graphs:  38%|███▊      | 7878/20708 [01:43<02:42, 79.14graph/s]Processing graphs:  38%|███▊      | 7886/20708 [01:43<02:45, 77.58graph/s]Processing graphs:  38%|███▊      | 7895/20708 [01:44<02:41, 79.10graph/s]Processing graphs:  38%|███▊      | 7903/20708 [01:44<02:45, 77.35graph/s]Processing graphs:  38%|███▊      | 7912/20708 [01:44<02:42, 78.67graph/s]Processing graphs:  38%|███▊      | 7920/20708 [01:44<02:46, 76.99graph/s]Processing graphs:  38%|███▊      | 7929/20708 [01:44<02:43, 78.07graph/s]Processing graphs:  38%|███▊      | 7937/20708 [01:44<02:44, 77.66graph/s]Processing graphs:  38%|███▊      | 7945/20708 [01:44<02:44, 77.50graph/s]Processing graphs:  38%|███▊      | 7953/20708 [01:44<02:46, 76.58graph/s]Processing graphs:  38%|███▊      | 7961/20708 [01:44<02:47, 76.27graph/s]Processing graphs:  38%|███▊      | 7970/20708 [01:45<02:44, 77.49graph/s]Processing graphs:  39%|███▊      | 7978/20708 [01:45<02:47, 76.03graph/s]Processing graphs:  39%|███▊      | 7986/20708 [01:45<02:45, 76.92graph/s]Processing graphs:  39%|███▊      | 7996/20708 [01:45<02:38, 80.37graph/s]Processing graphs:  39%|███▊      | 8005/20708 [01:45<02:36, 81.10graph/s]Processing graphs:  39%|███▊      | 8014/20708 [01:45<02:43, 77.70graph/s]Processing graphs:  39%|███▊      | 8022/20708 [01:45<02:50, 74.22graph/s]Processing graphs:  39%|███▉      | 8030/20708 [01:45<02:51, 74.09graph/s]Processing graphs:  39%|███▉      | 8038/20708 [01:45<02:49, 74.95graph/s]Processing graphs:  39%|███▉      | 8047/20708 [01:46<02:45, 76.71graph/s]Processing graphs:  39%|███▉      | 8055/20708 [01:46<02:50, 74.37graph/s]Processing graphs:  39%|███▉      | 8063/20708 [01:46<02:51, 73.93graph/s]Processing graphs:  39%|███▉      | 8071/20708 [01:46<02:47, 75.40graph/s]Processing graphs:  39%|███▉      | 8079/20708 [01:46<02:47, 75.30graph/s]Processing graphs:  39%|███▉      | 8087/20708 [01:46<02:53, 72.81graph/s]Processing graphs:  39%|███▉      | 8095/20708 [01:46<02:53, 72.78graph/s]Processing graphs:  39%|███▉      | 8103/20708 [01:46<02:54, 72.12graph/s]Processing graphs:  39%|███▉      | 8113/20708 [01:46<02:42, 77.58graph/s]Processing graphs:  39%|███▉      | 8121/20708 [01:47<02:45, 75.95graph/s]Processing graphs:  39%|███▉      | 8129/20708 [01:47<02:44, 76.30graph/s]Processing graphs:  39%|███▉      | 8137/20708 [01:47<02:48, 74.73graph/s]Processing graphs:  39%|███▉      | 8145/20708 [01:47<02:47, 75.16graph/s]Processing graphs:  39%|███▉      | 8153/20708 [01:47<02:48, 74.59graph/s]Processing graphs:  39%|███▉      | 8161/20708 [01:47<02:47, 74.83graph/s]Processing graphs:  39%|███▉      | 8170/20708 [01:47<02:44, 76.17graph/s]Processing graphs:  39%|███▉      | 8178/20708 [01:47<02:42, 77.15graph/s]Processing graphs:  40%|███▉      | 8186/20708 [01:47<02:44, 76.09graph/s]Processing graphs:  40%|███▉      | 8194/20708 [01:48<02:42, 76.99graph/s]Processing graphs:  40%|███▉      | 8202/20708 [01:48<02:44, 76.20graph/s]Processing graphs:  40%|███▉      | 8211/20708 [01:48<02:39, 78.52graph/s]Processing graphs:  40%|███▉      | 8220/20708 [01:48<02:35, 80.40graph/s]Processing graphs:  40%|███▉      | 8229/20708 [01:48<02:36, 79.56graph/s]Processing graphs:  40%|███▉      | 8238/20708 [01:48<02:35, 80.39graph/s]Processing graphs:  40%|███▉      | 8247/20708 [01:48<02:39, 78.31graph/s]Processing graphs:  40%|███▉      | 8256/20708 [01:48<02:36, 79.48graph/s]Processing graphs:  40%|███▉      | 8265/20708 [01:48<02:33, 81.27graph/s]Processing graphs:  40%|███▉      | 8274/20708 [01:49<02:38, 78.53graph/s]Processing graphs:  40%|███▉      | 8282/20708 [01:49<02:40, 77.57graph/s]Processing graphs:  40%|████      | 8290/20708 [01:49<02:41, 76.79graph/s]Processing graphs:  40%|████      | 8299/20708 [01:49<02:41, 77.06graph/s]Processing graphs:  40%|████      | 8307/20708 [01:49<02:46, 74.56graph/s]Processing graphs:  40%|████      | 8316/20708 [01:49<02:38, 78.11graph/s]Processing graphs:  40%|████      | 8325/20708 [01:49<02:35, 79.38graph/s]Processing graphs:  40%|████      | 8334/20708 [01:49<02:36, 79.20graph/s]Processing graphs:  40%|████      | 8342/20708 [01:49<02:35, 79.40graph/s]Processing graphs:  40%|████      | 8351/20708 [01:49<02:32, 81.01graph/s]Processing graphs:  40%|████      | 8360/20708 [01:50<02:37, 78.28graph/s]Processing graphs:  40%|████      | 8368/20708 [01:50<02:42, 75.89graph/s]Processing graphs:  40%|████      | 8376/20708 [01:50<02:43, 75.20graph/s]Processing graphs:  40%|████      | 8384/20708 [01:50<02:45, 74.51graph/s]Processing graphs:  41%|████      | 8392/20708 [01:50<02:42, 75.67graph/s]Processing graphs:  41%|████      | 8400/20708 [01:50<02:42, 75.81graph/s]Processing graphs:  41%|████      | 8408/20708 [01:50<02:46, 74.07graph/s]Processing graphs:  41%|████      | 8417/20708 [01:50<02:43, 75.24graph/s]Processing graphs:  41%|████      | 8425/20708 [01:50<02:42, 75.82graph/s]Processing graphs:  41%|████      | 8433/20708 [01:51<02:43, 75.23graph/s]Processing graphs:  41%|████      | 8441/20708 [01:51<02:45, 74.26graph/s]Processing graphs:  41%|████      | 8450/20708 [01:51<02:37, 77.63graph/s]Processing graphs:  41%|████      | 8458/20708 [01:51<02:42, 75.60graph/s]Processing graphs:  41%|████      | 8466/20708 [01:51<02:45, 74.09graph/s]Processing graphs:  41%|████      | 8475/20708 [01:51<02:40, 76.40graph/s]Processing graphs:  41%|████      | 8483/20708 [01:51<02:42, 75.13graph/s]Processing graphs:  41%|████      | 8493/20708 [01:51<02:33, 79.32graph/s]Processing graphs:  41%|████      | 8501/20708 [01:51<02:38, 77.23graph/s]Processing graphs:  41%|████      | 8509/20708 [01:52<02:36, 77.86graph/s]Processing graphs:  41%|████      | 8517/20708 [01:52<02:39, 76.37graph/s]Processing graphs:  41%|████      | 8525/20708 [01:52<02:41, 75.41graph/s]Processing graphs:  41%|████      | 8533/20708 [01:52<02:47, 72.70graph/s]Processing graphs:  41%|████      | 8541/20708 [01:52<02:51, 70.85graph/s]Processing graphs:  41%|████▏     | 8549/20708 [01:52<02:45, 73.34graph/s]Processing graphs:  41%|████▏     | 8557/20708 [01:52<02:45, 73.23graph/s]Processing graphs:  41%|████▏     | 8565/20708 [01:52<02:49, 71.79graph/s]Processing graphs:  41%|████▏     | 8573/20708 [01:52<02:52, 70.35graph/s]Processing graphs:  41%|████▏     | 8582/20708 [01:53<02:44, 73.89graph/s]Processing graphs:  41%|████▏     | 8590/20708 [01:53<02:50, 70.90graph/s]Processing graphs:  42%|████▏     | 8598/20708 [01:53<02:46, 72.87graph/s]Processing graphs:  42%|████▏     | 8606/20708 [01:53<02:41, 74.78graph/s]Processing graphs:  42%|████▏     | 8616/20708 [01:53<02:32, 79.33graph/s]Processing graphs:  42%|████▏     | 8624/20708 [01:53<02:32, 79.16graph/s]Processing graphs:  42%|████▏     | 8632/20708 [01:53<02:36, 77.18graph/s]Processing graphs:  42%|████▏     | 8640/20708 [01:53<02:35, 77.45graph/s]Processing graphs:  42%|████▏     | 8649/20708 [01:53<02:30, 79.93graph/s]Processing graphs:  42%|████▏     | 8658/20708 [01:54<02:32, 79.21graph/s]Processing graphs:  42%|████▏     | 8668/20708 [01:54<02:24, 83.50graph/s]Processing graphs:  42%|████▏     | 8677/20708 [01:54<02:31, 79.32graph/s]Processing graphs:  42%|████▏     | 8685/20708 [01:54<02:37, 76.10graph/s]Processing graphs:  42%|████▏     | 8694/20708 [01:54<02:34, 77.98graph/s]Processing graphs:  42%|████▏     | 8702/20708 [01:54<02:38, 75.70graph/s]Processing graphs:  42%|████▏     | 8710/20708 [01:54<02:42, 73.99graph/s]Processing graphs:  42%|████▏     | 8718/20708 [01:54<02:45, 72.38graph/s]Processing graphs:  42%|████▏     | 8727/20708 [01:54<02:37, 75.96graph/s]Processing graphs:  42%|████▏     | 8736/20708 [01:55<02:30, 79.44graph/s]Processing graphs:  42%|████▏     | 8744/20708 [01:55<02:35, 76.84graph/s]Processing graphs:  42%|████▏     | 8752/20708 [01:55<02:36, 76.52graph/s]Processing graphs:  42%|████▏     | 8760/20708 [01:55<02:37, 75.97graph/s]Processing graphs:  42%|████▏     | 8769/20708 [01:55<02:33, 77.82graph/s]Processing graphs:  42%|████▏     | 8777/20708 [01:55<02:32, 78.36graph/s]Processing graphs:  42%|████▏     | 8785/20708 [01:55<02:42, 73.30graph/s]Processing graphs:  42%|████▏     | 8793/20708 [01:55<02:42, 73.43graph/s]Processing graphs:  43%|████▎     | 8801/20708 [01:55<02:45, 71.75graph/s]Processing graphs:  43%|████▎     | 8809/20708 [01:56<02:49, 70.01graph/s]Processing graphs:  43%|████▎     | 8817/20708 [01:56<02:48, 70.41graph/s]Processing graphs:  43%|████▎     | 8825/20708 [01:56<02:45, 71.70graph/s]Processing graphs:  43%|████▎     | 8833/20708 [01:56<02:42, 72.90graph/s]Processing graphs:  43%|████▎     | 8841/20708 [01:56<02:38, 74.64graph/s]Processing graphs:  43%|████▎     | 8849/20708 [01:56<02:38, 74.76graph/s]Processing graphs:  43%|████▎     | 8857/20708 [01:56<02:36, 75.92graph/s]Processing graphs:  43%|████▎     | 8866/20708 [01:56<02:31, 77.93graph/s]Processing graphs:  43%|████▎     | 8875/20708 [01:56<02:29, 78.93graph/s]Processing graphs:  43%|████▎     | 8883/20708 [01:57<02:33, 77.12graph/s]Processing graphs:  43%|████▎     | 8891/20708 [01:57<02:35, 76.08graph/s]Processing graphs:  43%|████▎     | 8899/20708 [01:57<02:33, 76.81graph/s]Processing graphs:  43%|████▎     | 8908/20708 [01:57<02:30, 78.31graph/s]Processing graphs:  43%|████▎     | 8917/20708 [01:57<02:25, 81.29graph/s]Processing graphs:  43%|████▎     | 8926/20708 [01:57<02:23, 82.25graph/s]Processing graphs:  43%|████▎     | 8935/20708 [01:57<02:30, 78.48graph/s]Processing graphs:  43%|████▎     | 8943/20708 [01:57<02:37, 74.83graph/s]Processing graphs:  43%|████▎     | 8951/20708 [01:57<02:36, 75.35graph/s]Processing graphs:  43%|████▎     | 8959/20708 [01:58<02:38, 74.13graph/s]Processing graphs:  43%|████▎     | 8967/20708 [01:58<02:40, 73.14graph/s]Processing graphs:  43%|████▎     | 8975/20708 [01:58<02:38, 74.06graph/s]Processing graphs:  43%|████▎     | 8983/20708 [01:58<02:40, 72.89graph/s]Processing graphs:  43%|████▎     | 8991/20708 [01:58<02:39, 73.35graph/s]Processing graphs:  43%|████▎     | 8999/20708 [01:58<02:37, 74.29graph/s]Processing graphs:  43%|████▎     | 9007/20708 [01:58<02:39, 73.44graph/s]Processing graphs:  44%|████▎     | 9017/20708 [01:58<02:29, 78.44graph/s]Processing graphs:  44%|████▎     | 9027/20708 [01:58<02:21, 82.48graph/s]Processing graphs:  44%|████▎     | 9036/20708 [01:59<02:18, 84.09graph/s]Processing graphs:  44%|████▎     | 9045/20708 [01:59<02:19, 83.48graph/s]Processing graphs:  44%|████▎     | 9054/20708 [01:59<02:18, 83.93graph/s]Processing graphs:  44%|████▍     | 9063/20708 [01:59<02:23, 81.06graph/s]Processing graphs:  44%|████▍     | 9073/20708 [01:59<02:17, 84.49graph/s]Processing graphs:  44%|████▍     | 9082/20708 [01:59<02:18, 84.17graph/s]Processing graphs:  44%|████▍     | 9091/20708 [01:59<02:24, 80.52graph/s]Processing graphs:  44%|████▍     | 9101/20708 [01:59<02:16, 85.00graph/s]Processing graphs:  44%|████▍     | 9110/20708 [01:59<02:16, 84.99graph/s]Processing graphs:  44%|████▍     | 9119/20708 [02:00<02:23, 81.03graph/s]Processing graphs:  44%|████▍     | 9128/20708 [02:00<02:19, 82.92graph/s]Processing graphs:  44%|████▍     | 9137/20708 [02:00<02:16, 84.54graph/s]Processing graphs:  44%|████▍     | 9146/20708 [02:00<02:23, 80.45graph/s]Processing graphs:  44%|████▍     | 9155/20708 [02:00<02:23, 80.49graph/s]Processing graphs:  44%|████▍     | 9164/20708 [02:00<02:20, 82.23graph/s]Processing graphs:  44%|████▍     | 9173/20708 [02:00<02:21, 81.48graph/s]Processing graphs:  44%|████▍     | 9183/20708 [02:00<02:15, 85.13graph/s]Processing graphs:  44%|████▍     | 9193/20708 [02:00<02:11, 87.27graph/s]Processing graphs:  44%|████▍     | 9204/20708 [02:01<02:07, 90.58graph/s]Processing graphs:  44%|████▍     | 9214/20708 [02:01<02:07, 90.16graph/s]Processing graphs:  45%|████▍     | 9224/20708 [02:01<02:09, 88.90graph/s]Processing graphs:  45%|████▍     | 9233/20708 [02:01<02:14, 85.41graph/s]Processing graphs:  45%|████▍     | 9242/20708 [02:01<02:13, 85.79graph/s]Processing graphs:  45%|████▍     | 9252/20708 [02:01<02:07, 89.51graph/s]Processing graphs:  45%|████▍     | 9261/20708 [02:01<02:08, 89.36graph/s]Processing graphs:  45%|████▍     | 9271/20708 [02:01<02:07, 89.37graph/s]Processing graphs:  45%|████▍     | 9281/20708 [02:01<02:03, 92.24graph/s]Processing graphs:  45%|████▍     | 9291/20708 [02:01<02:10, 87.32graph/s]Processing graphs:  45%|████▍     | 9301/20708 [02:02<02:05, 90.60graph/s]Processing graphs:  45%|████▍     | 9311/20708 [02:02<02:11, 86.95graph/s]Processing graphs:  45%|████▌     | 9320/20708 [02:02<02:10, 87.45graph/s]Processing graphs:  45%|████▌     | 9329/20708 [02:02<02:10, 86.99graph/s]Processing graphs:  45%|████▌     | 9338/20708 [02:02<02:10, 87.08graph/s]Processing graphs:  45%|████▌     | 9347/20708 [02:02<02:10, 87.30graph/s]Processing graphs:  45%|████▌     | 9357/20708 [02:02<02:08, 88.19graph/s]Processing graphs:  45%|████▌     | 9366/20708 [02:02<02:09, 87.73graph/s]Processing graphs:  45%|████▌     | 9376/20708 [02:02<02:05, 90.29graph/s]Processing graphs:  45%|████▌     | 9386/20708 [02:03<02:06, 89.42graph/s]Processing graphs:  45%|████▌     | 9396/20708 [02:03<02:03, 91.72graph/s]Processing graphs:  45%|████▌     | 9406/20708 [02:03<02:04, 90.81graph/s]Processing graphs:  45%|████▌     | 9416/20708 [02:03<02:02, 92.24graph/s]Processing graphs:  46%|████▌     | 9426/20708 [02:03<02:10, 86.43graph/s]Processing graphs:  46%|████▌     | 9435/20708 [02:03<02:10, 86.10graph/s]Processing graphs:  46%|████▌     | 9444/20708 [02:03<02:11, 85.83graph/s]Processing graphs:  46%|████▌     | 9453/20708 [02:03<02:11, 85.84graph/s]Processing graphs:  46%|████▌     | 9463/20708 [02:03<02:06, 88.68graph/s]Processing graphs:  46%|████▌     | 9472/20708 [02:04<02:10, 86.27graph/s]Processing graphs:  46%|████▌     | 9481/20708 [02:04<02:11, 85.55graph/s]Processing graphs:  46%|████▌     | 9491/20708 [02:04<02:07, 87.76graph/s]Processing graphs:  46%|████▌     | 9500/20708 [02:04<02:07, 87.93graph/s]Processing graphs:  46%|████▌     | 9509/20708 [02:04<02:10, 85.72graph/s]Processing graphs:  46%|████▌     | 9518/20708 [02:04<02:11, 85.40graph/s]Processing graphs:  46%|████▌     | 9527/20708 [02:04<02:11, 85.32graph/s]Processing graphs:  46%|████▌     | 9536/20708 [02:04<02:12, 84.56graph/s]Processing graphs:  46%|████▌     | 9545/20708 [02:04<02:12, 84.12graph/s]Processing graphs:  46%|████▌     | 9554/20708 [02:05<02:16, 81.53graph/s]Processing graphs:  46%|████▌     | 9563/20708 [02:05<02:14, 82.59graph/s]Processing graphs:  46%|████▌     | 9573/20708 [02:05<02:09, 86.06graph/s]Processing graphs:  46%|████▋     | 9583/20708 [02:05<02:08, 86.90graph/s]Processing graphs:  46%|████▋     | 9594/20708 [02:05<02:03, 90.14graph/s]Processing graphs:  46%|████▋     | 9604/20708 [02:05<02:02, 90.60graph/s]Processing graphs:  46%|████▋     | 9614/20708 [02:05<02:09, 85.74graph/s]Processing graphs:  46%|████▋     | 9623/20708 [02:05<02:09, 85.82graph/s]Processing graphs:  47%|████▋     | 9632/20708 [02:05<02:08, 86.15graph/s]Processing graphs:  47%|████▋     | 9642/20708 [02:06<02:05, 88.31graph/s]Processing graphs:  47%|████▋     | 9651/20708 [02:06<02:05, 88.38graph/s]Processing graphs:  47%|████▋     | 9660/20708 [02:06<02:10, 84.53graph/s]Processing graphs:  47%|████▋     | 9669/20708 [02:06<02:12, 83.59graph/s]Processing graphs:  47%|████▋     | 9678/20708 [02:06<02:14, 82.14graph/s]Processing graphs:  47%|████▋     | 9688/20708 [02:06<02:08, 85.86graph/s]Processing graphs:  47%|████▋     | 9698/20708 [02:06<02:05, 87.39graph/s]Processing graphs:  47%|████▋     | 9707/20708 [02:06<02:07, 86.45graph/s]Processing graphs:  47%|████▋     | 9717/20708 [02:06<02:05, 87.50graph/s]Processing graphs:  47%|████▋     | 9727/20708 [02:07<02:03, 89.06graph/s]Processing graphs:  47%|████▋     | 9736/20708 [02:07<02:06, 86.66graph/s]Processing graphs:  47%|████▋     | 9745/20708 [02:07<02:05, 87.06graph/s]Processing graphs:  47%|████▋     | 9754/20708 [02:07<02:11, 83.53graph/s]Processing graphs:  47%|████▋     | 9764/20708 [02:07<02:07, 85.68graph/s]Processing graphs:  47%|████▋     | 9774/20708 [02:07<02:05, 87.11graph/s]Processing graphs:  47%|████▋     | 9784/20708 [02:07<02:02, 89.50graph/s]Processing graphs:  47%|████▋     | 9794/20708 [02:07<02:01, 89.51graph/s]Processing graphs:  47%|████▋     | 9805/20708 [02:07<01:58, 91.86graph/s]Processing graphs:  47%|████▋     | 9815/20708 [02:08<02:00, 90.15graph/s]Processing graphs:  47%|████▋     | 9825/20708 [02:08<02:01, 89.32graph/s]Processing graphs:  47%|████▋     | 9834/20708 [02:08<02:02, 88.89graph/s]Processing graphs:  48%|████▊     | 9843/20708 [02:08<02:06, 85.94graph/s]Processing graphs:  48%|████▊     | 9852/20708 [02:08<02:09, 83.91graph/s]Processing graphs:  48%|████▊     | 9861/20708 [02:08<02:06, 85.50graph/s]Processing graphs:  48%|████▊     | 9870/20708 [02:08<02:04, 86.72graph/s]Processing graphs:  48%|████▊     | 9880/20708 [02:08<02:00, 89.91graph/s]Processing graphs:  48%|████▊     | 9890/20708 [02:08<02:03, 87.29graph/s]Processing graphs:  48%|████▊     | 9899/20708 [02:08<02:04, 86.50graph/s]Processing graphs:  48%|████▊     | 9910/20708 [02:09<01:56, 92.36graph/s]Processing graphs:  48%|████▊     | 9920/20708 [02:09<01:59, 90.44graph/s]Processing graphs:  48%|████▊     | 9930/20708 [02:09<02:01, 88.72graph/s]Processing graphs:  48%|████▊     | 9939/20708 [02:09<02:06, 85.40graph/s]Processing graphs:  48%|████▊     | 9949/20708 [02:09<02:01, 88.43graph/s]Processing graphs:  48%|████▊     | 9958/20708 [02:09<02:04, 86.05graph/s]Processing graphs:  48%|████▊     | 9967/20708 [02:09<02:03, 86.73graph/s]Processing graphs:  48%|████▊     | 9977/20708 [02:09<02:02, 87.37graph/s]Processing graphs:  48%|████▊     | 9988/20708 [02:09<01:57, 91.21graph/s]Processing graphs:  48%|████▊     | 9998/20708 [02:10<01:59, 89.97graph/s]Processing graphs:  48%|████▊     | 10008/20708 [02:10<01:59, 89.62graph/s]Processing graphs:  48%|████▊     | 10018/20708 [02:10<01:58, 90.03graph/s]Processing graphs:  48%|████▊     | 10028/20708 [02:10<02:00, 88.47graph/s]Processing graphs:  48%|████▊     | 10038/20708 [02:10<01:57, 91.08graph/s]Processing graphs:  49%|████▊     | 10048/20708 [02:10<01:55, 92.15graph/s]Processing graphs:  49%|████▊     | 10059/20708 [02:10<01:52, 95.08graph/s]Processing graphs:  49%|████▊     | 10070/20708 [02:10<01:49, 96.94graph/s]Processing graphs:  49%|████▊     | 10080/20708 [02:10<01:52, 94.58graph/s]Processing graphs:  49%|████▊     | 10090/20708 [02:11<01:52, 94.37graph/s]Processing graphs:  49%|████▉     | 10100/20708 [02:11<01:53, 93.79graph/s]Processing graphs:  49%|████▉     | 10110/20708 [02:11<01:53, 93.00graph/s]Processing graphs:  49%|████▉     | 10120/20708 [02:11<01:52, 94.13graph/s]Processing graphs:  49%|████▉     | 10130/20708 [02:11<01:50, 95.32graph/s]Processing graphs:  49%|████▉     | 10142/20708 [02:11<01:45, 100.00graph/s]Processing graphs:  49%|████▉     | 10152/20708 [02:11<01:46, 99.09graph/s] Processing graphs:  49%|████▉     | 10162/20708 [02:11<01:54, 92.22graph/s]Processing graphs:  49%|████▉     | 10172/20708 [02:11<01:54, 91.94graph/s]Processing graphs:  49%|████▉     | 10183/20708 [02:12<01:50, 95.17graph/s]Processing graphs:  49%|████▉     | 10193/20708 [02:12<01:51, 93.98graph/s]Processing graphs:  49%|████▉     | 10204/20708 [02:12<01:49, 96.00graph/s]Processing graphs:  49%|████▉     | 10214/20708 [02:12<01:48, 96.66graph/s]Processing graphs:  49%|████▉     | 10224/20708 [02:12<01:57, 89.20graph/s]Processing graphs:  49%|████▉     | 10234/20708 [02:12<01:57, 89.43graph/s]Processing graphs:  49%|████▉     | 10244/20708 [02:12<01:56, 90.11graph/s]Processing graphs:  50%|████▉     | 10254/20708 [02:12<01:54, 91.64graph/s]Processing graphs:  50%|████▉     | 10265/20708 [02:12<01:48, 96.17graph/s]Processing graphs:  50%|████▉     | 10275/20708 [02:13<01:53, 91.66graph/s]Processing graphs:  50%|████▉     | 10286/20708 [02:13<01:50, 94.41graph/s]Processing graphs:  50%|████▉     | 10296/20708 [02:13<01:49, 95.37graph/s]Processing graphs:  50%|████▉     | 10307/20708 [02:13<01:46, 97.64graph/s]Processing graphs:  50%|████▉     | 10317/20708 [02:13<01:48, 95.70graph/s]Processing graphs:  50%|████▉     | 10327/20708 [02:13<01:51, 92.86graph/s]Processing graphs:  50%|████▉     | 10338/20708 [02:13<01:47, 96.12graph/s]Processing graphs:  50%|████▉     | 10348/20708 [02:13<01:50, 93.84graph/s]Processing graphs:  50%|█████     | 10359/20708 [02:13<01:46, 97.62graph/s]Processing graphs:  50%|█████     | 10370/20708 [02:14<01:44, 98.56graph/s]Processing graphs:  50%|█████     | 10380/20708 [02:14<01:48, 95.23graph/s]Processing graphs:  50%|█████     | 10390/20708 [02:14<01:50, 93.69graph/s]Processing graphs:  50%|█████     | 10400/20708 [02:14<01:49, 93.91graph/s]Processing graphs:  50%|█████     | 10411/20708 [02:14<01:47, 95.35graph/s]Processing graphs:  50%|█████     | 10421/20708 [02:14<01:50, 92.87graph/s]Processing graphs:  50%|█████     | 10432/20708 [02:14<01:46, 96.42graph/s]Processing graphs:  50%|█████     | 10442/20708 [02:14<01:46, 96.24graph/s]Processing graphs:  50%|█████     | 10452/20708 [02:14<01:50, 92.44graph/s]Processing graphs:  51%|█████     | 10462/20708 [02:15<01:50, 92.54graph/s]Processing graphs:  51%|█████     | 10473/20708 [02:15<01:47, 95.12graph/s]Processing graphs:  51%|█████     | 10484/20708 [02:15<01:46, 96.41graph/s]Processing graphs:  51%|█████     | 10494/20708 [02:15<01:48, 94.19graph/s]Processing graphs:  51%|█████     | 10504/20708 [02:15<01:51, 91.22graph/s]Processing graphs:  51%|█████     | 10514/20708 [02:15<01:52, 90.31graph/s]Processing graphs:  51%|█████     | 10524/20708 [02:15<01:51, 91.38graph/s]Processing graphs:  51%|█████     | 10534/20708 [02:15<01:50, 91.99graph/s]Processing graphs:  51%|█████     | 10544/20708 [02:15<01:50, 92.30graph/s]Processing graphs:  51%|█████     | 10554/20708 [02:16<01:50, 91.59graph/s]Processing graphs:  51%|█████     | 10564/20708 [02:16<01:51, 91.14graph/s]Processing graphs:  51%|█████     | 10574/20708 [02:16<01:49, 92.15graph/s]Processing graphs:  51%|█████     | 10584/20708 [02:16<01:51, 90.67graph/s]Processing graphs:  51%|█████     | 10594/20708 [02:16<01:53, 89.27graph/s]Processing graphs:  51%|█████     | 10603/20708 [02:16<01:55, 87.30graph/s]Processing graphs:  51%|█████▏    | 10614/20708 [02:16<01:51, 90.84graph/s]Processing graphs:  51%|█████▏    | 10624/20708 [02:16<01:53, 89.16graph/s]Processing graphs:  51%|█████▏    | 10635/20708 [02:16<01:48, 93.13graph/s]Processing graphs:  51%|█████▏    | 10645/20708 [02:17<01:48, 92.38graph/s]Processing graphs:  51%|█████▏    | 10655/20708 [02:17<01:51, 89.77graph/s]Processing graphs:  52%|█████▏    | 10665/20708 [02:17<01:50, 90.49graph/s]Processing graphs:  52%|█████▏    | 10675/20708 [02:17<01:49, 91.65graph/s]Processing graphs:  52%|█████▏    | 10685/20708 [02:17<01:50, 90.48graph/s]Processing graphs:  52%|█████▏    | 10695/20708 [02:17<01:53, 88.59graph/s]Processing graphs:  52%|█████▏    | 10705/20708 [02:17<01:49, 91.14graph/s]Processing graphs:  52%|█████▏    | 10715/20708 [02:17<01:48, 92.47graph/s]Processing graphs:  52%|█████▏    | 10725/20708 [02:17<01:50, 90.64graph/s]Processing graphs:  52%|█████▏    | 10736/20708 [02:18<01:45, 94.26graph/s]Processing graphs:  52%|█████▏    | 10746/20708 [02:18<01:48, 91.63graph/s]Processing graphs:  52%|█████▏    | 10756/20708 [02:18<01:46, 93.23graph/s]Processing graphs:  52%|█████▏    | 10766/20708 [02:18<01:51, 88.82graph/s]Processing graphs:  52%|█████▏    | 10777/20708 [02:18<01:45, 94.19graph/s]Processing graphs:  52%|█████▏    | 10788/20708 [02:18<01:42, 97.19graph/s]Processing graphs:  52%|█████▏    | 10798/20708 [02:18<01:44, 94.92graph/s]Processing graphs:  52%|█████▏    | 10809/20708 [02:18<01:42, 96.24graph/s]Processing graphs:  52%|█████▏    | 10819/20708 [02:18<01:42, 96.42graph/s]Processing graphs:  52%|█████▏    | 10829/20708 [02:19<01:49, 89.96graph/s]Processing graphs:  52%|█████▏    | 10839/20708 [02:19<01:46, 92.53graph/s]Processing graphs:  52%|█████▏    | 10849/20708 [02:19<01:46, 92.70graph/s]Processing graphs:  52%|█████▏    | 10859/20708 [02:19<01:46, 92.73graph/s]Processing graphs:  52%|█████▏    | 10869/20708 [02:19<01:44, 93.79graph/s]Processing graphs:  53%|█████▎    | 10879/20708 [02:19<01:47, 91.74graph/s]Processing graphs:  53%|█████▎    | 10889/20708 [02:19<01:46, 92.36graph/s]Processing graphs:  53%|█████▎    | 10899/20708 [02:19<01:47, 91.51graph/s]Processing graphs:  53%|█████▎    | 10909/20708 [02:19<01:49, 89.16graph/s]Processing graphs:  53%|█████▎    | 10919/20708 [02:19<01:47, 90.66graph/s]Processing graphs:  53%|█████▎    | 10929/20708 [02:20<01:47, 91.28graph/s]Processing graphs:  53%|█████▎    | 10939/20708 [02:20<01:47, 90.93graph/s]Processing graphs:  53%|█████▎    | 10949/20708 [02:20<01:47, 91.18graph/s]Processing graphs:  53%|█████▎    | 10959/20708 [02:20<01:45, 92.77graph/s]Processing graphs:  53%|█████▎    | 10969/20708 [02:20<01:46, 91.14graph/s]Processing graphs:  53%|█████▎    | 10979/20708 [02:20<01:48, 89.86graph/s]Processing graphs:  53%|█████▎    | 10989/20708 [02:20<01:46, 91.68graph/s]Processing graphs:  53%|█████▎    | 11000/20708 [02:20<01:42, 94.73graph/s]Processing graphs:  53%|█████▎    | 11010/20708 [02:21<01:55, 83.69graph/s]Processing graphs:  53%|█████▎    | 11019/20708 [02:21<02:04, 77.58graph/s]Processing graphs:  53%|█████▎    | 11028/20708 [02:21<02:03, 78.49graph/s]Processing graphs:  53%|█████▎    | 11037/20708 [02:21<02:05, 77.22graph/s]Processing graphs:  53%|█████▎    | 11045/20708 [02:21<02:07, 75.54graph/s]Processing graphs:  53%|█████▎    | 11053/20708 [02:21<02:08, 75.08graph/s]Processing graphs:  53%|█████▎    | 11061/20708 [02:21<02:09, 74.25graph/s]Processing graphs:  53%|█████▎    | 11069/20708 [02:21<02:09, 74.51graph/s]Processing graphs:  53%|█████▎    | 11077/20708 [02:21<02:07, 75.61graph/s]Processing graphs:  54%|█████▎    | 11085/20708 [02:22<02:10, 73.94graph/s]Processing graphs:  54%|█████▎    | 11093/20708 [02:22<02:10, 73.53graph/s]Processing graphs:  54%|█████▎    | 11101/20708 [02:22<02:12, 72.70graph/s]Processing graphs:  54%|█████▎    | 11109/20708 [02:22<02:10, 73.63graph/s]Processing graphs:  54%|█████▎    | 11117/20708 [02:22<02:14, 71.19graph/s]Processing graphs:  54%|█████▎    | 11125/20708 [02:22<02:12, 72.16graph/s]Processing graphs:  54%|█████▍    | 11133/20708 [02:22<02:09, 73.79graph/s]Processing graphs:  54%|█████▍    | 11141/20708 [02:22<02:08, 74.34graph/s]Processing graphs:  54%|█████▍    | 11149/20708 [02:22<02:09, 73.62graph/s]Processing graphs:  54%|█████▍    | 11157/20708 [02:23<02:09, 73.75graph/s]Processing graphs:  54%|█████▍    | 11165/20708 [02:23<02:06, 75.46graph/s]Processing graphs:  54%|█████▍    | 11173/20708 [02:23<02:09, 73.76graph/s]Processing graphs:  54%|█████▍    | 11182/20708 [02:23<02:05, 76.20graph/s]Processing graphs:  54%|█████▍    | 11191/20708 [02:23<02:02, 77.93graph/s]Processing graphs:  54%|█████▍    | 11199/20708 [02:23<02:04, 76.31graph/s]Processing graphs:  54%|█████▍    | 11207/20708 [02:23<02:06, 75.30graph/s]Processing graphs:  54%|█████▍    | 11215/20708 [02:23<02:09, 73.44graph/s]Processing graphs:  54%|█████▍    | 11223/20708 [02:23<02:07, 74.34graph/s]Processing graphs:  54%|█████▍    | 11231/20708 [02:23<02:05, 75.42graph/s]Processing graphs:  54%|█████▍    | 11239/20708 [02:24<02:08, 73.66graph/s]Processing graphs:  54%|█████▍    | 11247/20708 [02:24<02:08, 73.45graph/s]Processing graphs:  54%|█████▍    | 11255/20708 [02:24<02:06, 74.62graph/s]Processing graphs:  54%|█████▍    | 11263/20708 [02:24<02:07, 74.08graph/s]Processing graphs:  54%|█████▍    | 11271/20708 [02:24<02:09, 72.65graph/s]Processing graphs:  54%|█████▍    | 11279/20708 [02:24<02:06, 74.52graph/s]Processing graphs:  55%|█████▍    | 11287/20708 [02:24<02:07, 73.73graph/s]Processing graphs:  55%|█████▍    | 11295/20708 [02:24<02:10, 72.17graph/s]Processing graphs:  55%|█████▍    | 11303/20708 [02:24<02:10, 72.13graph/s]Processing graphs:  55%|█████▍    | 11311/20708 [02:25<02:07, 73.56graph/s]Processing graphs:  55%|█████▍    | 11319/20708 [02:25<02:05, 74.58graph/s]Processing graphs:  55%|█████▍    | 11327/20708 [02:25<02:05, 74.79graph/s]Processing graphs:  55%|█████▍    | 11335/20708 [02:25<02:06, 74.38graph/s]Processing graphs:  55%|█████▍    | 11343/20708 [02:25<02:04, 75.34graph/s]Processing graphs:  55%|█████▍    | 11351/20708 [02:25<02:02, 76.09graph/s]Processing graphs:  55%|█████▍    | 11359/20708 [02:25<02:06, 74.00graph/s]Processing graphs:  55%|█████▍    | 11367/20708 [02:25<02:08, 72.82graph/s]Processing graphs:  55%|█████▍    | 11375/20708 [02:25<02:08, 72.50graph/s]Processing graphs:  55%|█████▍    | 11383/20708 [02:26<02:10, 71.48graph/s]Processing graphs:  55%|█████▌    | 11391/20708 [02:26<02:08, 72.74graph/s]Processing graphs:  55%|█████▌    | 11399/20708 [02:26<02:05, 74.43graph/s]Processing graphs:  55%|█████▌    | 11407/20708 [02:26<02:08, 72.50graph/s]Processing graphs:  55%|█████▌    | 11415/20708 [02:26<02:12, 70.30graph/s]Processing graphs:  55%|█████▌    | 11424/20708 [02:26<02:04, 74.76graph/s]Processing graphs:  55%|█████▌    | 11433/20708 [02:26<02:02, 75.65graph/s]Processing graphs:  55%|█████▌    | 11441/20708 [02:26<02:07, 72.89graph/s]Processing graphs:  55%|█████▌    | 11449/20708 [02:26<02:09, 71.55graph/s]Processing graphs:  55%|█████▌    | 11457/20708 [02:27<02:12, 69.58graph/s]Processing graphs:  55%|█████▌    | 11465/20708 [02:27<02:10, 71.01graph/s]Processing graphs:  55%|█████▌    | 11473/20708 [02:27<02:10, 71.01graph/s]Processing graphs:  55%|█████▌    | 11482/20708 [02:27<02:02, 75.47graph/s]Processing graphs:  55%|█████▌    | 11490/20708 [02:27<02:04, 74.32graph/s]Processing graphs:  56%|█████▌    | 11498/20708 [02:27<02:07, 72.38graph/s]Processing graphs:  56%|█████▌    | 11506/20708 [02:27<02:04, 73.65graph/s]Processing graphs:  56%|█████▌    | 11514/20708 [02:27<02:07, 72.05graph/s]Processing graphs:  56%|█████▌    | 11523/20708 [02:27<02:03, 74.30graph/s]Processing graphs:  56%|█████▌    | 11531/20708 [02:28<02:05, 72.91graph/s]Processing graphs:  56%|█████▌    | 11539/20708 [02:28<02:03, 74.39graph/s]Processing graphs:  56%|█████▌    | 11547/20708 [02:28<02:02, 74.80graph/s]Processing graphs:  56%|█████▌    | 11555/20708 [02:28<02:00, 76.18graph/s]Processing graphs:  56%|█████▌    | 11563/20708 [02:28<02:00, 75.86graph/s]Processing graphs:  56%|█████▌    | 11571/20708 [02:28<02:07, 71.86graph/s]Processing graphs:  56%|█████▌    | 11579/20708 [02:28<02:05, 72.57graph/s]Processing graphs:  56%|█████▌    | 11587/20708 [02:28<02:09, 70.17graph/s]Processing graphs:  56%|█████▌    | 11595/20708 [02:28<02:08, 71.17graph/s]Processing graphs:  56%|█████▌    | 11603/20708 [02:29<02:04, 72.85graph/s]Processing graphs:  56%|█████▌    | 11611/20708 [02:29<02:03, 73.49graph/s]Processing graphs:  56%|█████▌    | 11619/20708 [02:29<02:08, 70.99graph/s]Processing graphs:  56%|█████▌    | 11627/20708 [02:29<02:10, 69.53graph/s]Processing graphs:  56%|█████▌    | 11635/20708 [02:29<02:06, 71.87graph/s]Processing graphs:  56%|█████▌    | 11643/20708 [02:29<02:08, 70.73graph/s]Processing graphs:  56%|█████▋    | 11651/20708 [02:29<02:06, 71.85graph/s]Processing graphs:  56%|█████▋    | 11659/20708 [02:29<02:05, 71.98graph/s]Processing graphs:  56%|█████▋    | 11667/20708 [02:29<02:02, 73.74graph/s]Processing graphs:  56%|█████▋    | 11675/20708 [02:30<02:05, 72.19graph/s]Processing graphs:  56%|█████▋    | 11683/20708 [02:30<02:02, 73.61graph/s]Processing graphs:  56%|█████▋    | 11691/20708 [02:30<02:02, 73.86graph/s]Processing graphs:  56%|█████▋    | 11699/20708 [02:30<02:03, 73.01graph/s]Processing graphs:  57%|█████▋    | 11707/20708 [02:30<02:05, 71.46graph/s]Processing graphs:  57%|█████▋    | 11715/20708 [02:30<02:10, 69.06graph/s]Processing graphs:  57%|█████▋    | 11723/20708 [02:30<02:05, 71.46graph/s]Processing graphs:  57%|█████▋    | 11731/20708 [02:30<02:05, 71.74graph/s]Processing graphs:  57%|█████▋    | 11739/20708 [02:30<02:06, 71.03graph/s]Processing graphs:  57%|█████▋    | 11747/20708 [02:31<02:05, 71.43graph/s]Processing graphs:  57%|█████▋    | 11755/20708 [02:31<02:03, 72.49graph/s]Processing graphs:  57%|█████▋    | 11763/20708 [02:31<02:03, 72.52graph/s]Processing graphs:  57%|█████▋    | 11771/20708 [02:31<02:05, 71.38graph/s]Processing graphs:  57%|█████▋    | 11779/20708 [02:31<02:01, 73.42graph/s]Processing graphs:  57%|█████▋    | 11788/20708 [02:31<01:56, 76.45graph/s]Processing graphs:  57%|█████▋    | 11796/20708 [02:31<01:59, 74.73graph/s]Processing graphs:  57%|█████▋    | 11804/20708 [02:31<01:59, 74.31graph/s]Processing graphs:  57%|█████▋    | 11812/20708 [02:31<02:01, 73.09graph/s]Processing graphs:  57%|█████▋    | 11820/20708 [02:32<02:00, 73.93graph/s]Processing graphs:  57%|█████▋    | 11828/20708 [02:32<01:59, 74.16graph/s]Processing graphs:  57%|█████▋    | 11836/20708 [02:32<02:02, 72.25graph/s]Processing graphs:  57%|█████▋    | 11844/20708 [02:32<02:02, 72.22graph/s]Processing graphs:  57%|█████▋    | 11852/20708 [02:32<02:00, 73.49graph/s]Processing graphs:  57%|█████▋    | 11860/20708 [02:32<02:03, 71.39graph/s]Processing graphs:  57%|█████▋    | 11868/20708 [02:32<02:02, 72.33graph/s]Processing graphs:  57%|█████▋    | 11876/20708 [02:32<02:03, 71.31graph/s]Processing graphs:  57%|█████▋    | 11884/20708 [02:32<02:00, 73.42graph/s]Processing graphs:  57%|█████▋    | 11892/20708 [02:33<02:04, 70.55graph/s]Processing graphs:  57%|█████▋    | 11900/20708 [02:33<02:05, 70.39graph/s]Processing graphs:  58%|█████▊    | 11908/20708 [02:33<02:07, 69.01graph/s]Processing graphs:  58%|█████▊    | 11916/20708 [02:33<02:04, 70.39graph/s]Processing graphs:  58%|█████▊    | 11924/20708 [02:33<02:06, 69.21graph/s]Processing graphs:  58%|█████▊    | 11932/20708 [02:33<02:03, 71.22graph/s]Processing graphs:  58%|█████▊    | 11940/20708 [02:33<02:02, 71.61graph/s]Processing graphs:  58%|█████▊    | 11948/20708 [02:33<02:00, 72.84graph/s]Processing graphs:  58%|█████▊    | 11956/20708 [02:33<01:58, 73.99graph/s]Processing graphs:  58%|█████▊    | 11964/20708 [02:34<01:59, 73.42graph/s]Processing graphs:  58%|█████▊    | 11972/20708 [02:34<02:04, 70.04graph/s]Processing graphs:  58%|█████▊    | 11980/20708 [02:34<02:02, 71.41graph/s]Processing graphs:  58%|█████▊    | 11989/20708 [02:34<01:57, 73.91graph/s]Processing graphs:  58%|█████▊    | 11997/20708 [02:34<02:01, 71.46graph/s]Processing graphs:  58%|█████▊    | 12005/20708 [02:34<01:58, 73.53graph/s]Processing graphs:  58%|█████▊    | 12015/20708 [02:34<01:51, 78.08graph/s]Processing graphs:  58%|█████▊    | 12024/20708 [02:34<01:47, 81.16graph/s]Processing graphs:  58%|█████▊    | 12033/20708 [02:34<01:45, 82.55graph/s]Processing graphs:  58%|█████▊    | 12044/20708 [02:35<01:39, 87.32graph/s]Processing graphs:  58%|█████▊    | 12053/20708 [02:35<01:43, 84.00graph/s]Processing graphs:  58%|█████▊    | 12063/20708 [02:35<01:40, 86.36graph/s]Processing graphs:  58%|█████▊    | 12073/20708 [02:35<01:37, 88.35graph/s]Processing graphs:  58%|█████▊    | 12082/20708 [02:35<01:40, 86.18graph/s]Processing graphs:  58%|█████▊    | 12091/20708 [02:35<01:39, 86.80graph/s]Processing graphs:  58%|█████▊    | 12101/20708 [02:35<01:37, 88.56graph/s]Processing graphs:  58%|█████▊    | 12111/20708 [02:35<01:35, 90.23graph/s]Processing graphs:  59%|█████▊    | 12121/20708 [02:35<01:40, 85.05graph/s]Processing graphs:  59%|█████▊    | 12131/20708 [02:36<01:37, 88.05graph/s]Processing graphs:  59%|█████▊    | 12142/20708 [02:36<01:32, 93.00graph/s]Processing graphs:  59%|█████▊    | 12152/20708 [02:36<01:33, 91.89graph/s]Processing graphs:  59%|█████▊    | 12162/20708 [02:36<01:33, 91.87graph/s]Processing graphs:  59%|█████▉    | 12173/20708 [02:36<01:31, 93.46graph/s]Processing graphs:  59%|█████▉    | 12183/20708 [02:36<01:33, 91.65graph/s]Processing graphs:  59%|█████▉    | 12193/20708 [02:36<01:33, 90.90graph/s]Processing graphs:  59%|█████▉    | 12203/20708 [02:36<01:33, 91.35graph/s]Processing graphs:  59%|█████▉    | 12213/20708 [02:36<01:33, 91.06graph/s]Processing graphs:  59%|█████▉    | 12224/20708 [02:37<01:29, 94.47graph/s]Processing graphs:  59%|█████▉    | 12234/20708 [02:37<01:30, 93.53graph/s]Processing graphs:  59%|█████▉    | 12244/20708 [02:37<01:33, 90.10graph/s]Processing graphs:  59%|█████▉    | 12254/20708 [02:37<01:35, 88.39graph/s]Processing graphs:  59%|█████▉    | 12264/20708 [02:37<01:34, 89.42graph/s]Processing graphs:  59%|█████▉    | 12274/20708 [02:37<01:33, 89.91graph/s]Processing graphs:  59%|█████▉    | 12284/20708 [02:37<01:34, 89.08graph/s]Processing graphs:  59%|█████▉    | 12293/20708 [02:37<01:37, 86.61graph/s]Processing graphs:  59%|█████▉    | 12303/20708 [02:37<01:36, 87.52graph/s]Processing graphs:  59%|█████▉    | 12313/20708 [02:38<01:32, 90.42graph/s]Processing graphs:  60%|█████▉    | 12323/20708 [02:38<01:33, 90.12graph/s]Processing graphs:  60%|█████▉    | 12333/20708 [02:38<01:33, 89.38graph/s]Processing graphs:  60%|█████▉    | 12343/20708 [02:38<01:32, 90.80graph/s]Processing graphs:  60%|█████▉    | 12353/20708 [02:38<01:31, 90.82graph/s]Processing graphs:  60%|█████▉    | 12363/20708 [02:38<01:34, 88.74graph/s]Processing graphs:  60%|█████▉    | 12373/20708 [02:38<01:31, 90.92graph/s]Processing graphs:  60%|█████▉    | 12383/20708 [02:38<01:32, 89.60graph/s]Processing graphs:  60%|█████▉    | 12393/20708 [02:38<01:30, 91.46graph/s]Processing graphs:  60%|█████▉    | 12403/20708 [02:39<01:34, 87.83graph/s]Processing graphs:  60%|█████▉    | 12412/20708 [02:39<01:34, 87.59graph/s]Processing graphs:  60%|█████▉    | 12421/20708 [02:39<01:35, 86.57graph/s]Processing graphs:  60%|██████    | 12431/20708 [02:39<01:33, 88.15graph/s]Processing graphs:  60%|██████    | 12441/20708 [02:39<01:31, 90.82graph/s]Processing graphs:  60%|██████    | 12452/20708 [02:39<01:28, 93.09graph/s]Processing graphs:  60%|██████    | 12463/20708 [02:39<01:26, 94.97graph/s]Processing graphs:  60%|██████    | 12473/20708 [02:39<01:27, 94.22graph/s]Processing graphs:  60%|██████    | 12483/20708 [02:39<01:26, 95.62graph/s]Processing graphs:  60%|██████    | 12493/20708 [02:40<01:27, 94.33graph/s]Processing graphs:  60%|██████    | 12503/20708 [02:40<01:31, 90.09graph/s]Processing graphs:  60%|██████    | 12513/20708 [02:40<01:32, 88.34graph/s]Processing graphs:  60%|██████    | 12522/20708 [02:40<01:37, 83.91graph/s]Processing graphs:  61%|██████    | 12532/20708 [02:40<01:34, 86.90graph/s]Processing graphs:  61%|██████    | 12541/20708 [02:40<01:33, 87.59graph/s]Processing graphs:  61%|██████    | 12552/20708 [02:40<01:28, 91.90graph/s]Processing graphs:  61%|██████    | 12562/20708 [02:40<01:31, 89.20graph/s]Processing graphs:  61%|██████    | 12571/20708 [02:40<01:32, 88.23graph/s]Processing graphs:  61%|██████    | 12581/20708 [02:41<01:30, 89.61graph/s]Processing graphs:  61%|██████    | 12591/20708 [02:41<01:27, 92.28graph/s]Processing graphs:  61%|██████    | 12601/20708 [02:41<01:28, 92.07graph/s]Processing graphs:  61%|██████    | 12611/20708 [02:41<01:31, 88.46graph/s]Processing graphs:  61%|██████    | 12622/20708 [02:41<01:29, 90.59graph/s]Processing graphs:  61%|██████    | 12632/20708 [02:41<01:27, 91.86graph/s]Processing graphs:  61%|██████    | 12642/20708 [02:41<01:28, 90.74graph/s]Processing graphs:  61%|██████    | 12652/20708 [02:41<01:29, 89.85graph/s]Processing graphs:  61%|██████    | 12662/20708 [02:41<01:29, 90.01graph/s]Processing graphs:  61%|██████    | 12672/20708 [02:42<01:27, 91.53graph/s]Processing graphs:  61%|██████    | 12682/20708 [02:42<01:28, 90.48graph/s]Processing graphs:  61%|██████▏   | 12692/20708 [02:42<01:29, 89.40graph/s]Processing graphs:  61%|██████▏   | 12701/20708 [02:42<01:30, 88.57graph/s]Processing graphs:  61%|██████▏   | 12711/20708 [02:42<01:29, 89.59graph/s]Processing graphs:  61%|██████▏   | 12720/20708 [02:42<01:29, 89.49graph/s]Processing graphs:  61%|██████▏   | 12730/20708 [02:42<01:27, 91.36graph/s]Processing graphs:  62%|██████▏   | 12740/20708 [02:42<01:29, 89.19graph/s]Processing graphs:  62%|██████▏   | 12750/20708 [02:42<01:26, 91.78graph/s]Processing graphs:  62%|██████▏   | 12761/20708 [02:43<01:23, 95.73graph/s]Processing graphs:  62%|██████▏   | 12771/20708 [02:43<01:25, 92.64graph/s]Processing graphs:  62%|██████▏   | 12781/20708 [02:43<01:27, 90.86graph/s]Processing graphs:  62%|██████▏   | 12791/20708 [02:43<01:24, 93.26graph/s]Processing graphs:  62%|██████▏   | 12801/20708 [02:43<01:26, 91.28graph/s]Processing graphs:  62%|██████▏   | 12811/20708 [02:43<01:28, 89.63graph/s]Processing graphs:  62%|██████▏   | 12820/20708 [02:43<01:28, 89.53graph/s]Processing graphs:  62%|██████▏   | 12830/20708 [02:43<01:25, 91.83graph/s]Processing graphs:  62%|██████▏   | 12840/20708 [02:43<01:27, 90.22graph/s]Processing graphs:  62%|██████▏   | 12850/20708 [02:44<01:28, 88.62graph/s]Processing graphs:  62%|██████▏   | 12860/20708 [02:44<01:27, 89.24graph/s]Processing graphs:  62%|██████▏   | 12869/20708 [02:44<01:28, 89.05graph/s]Processing graphs:  62%|██████▏   | 12879/20708 [02:44<01:25, 91.87graph/s]Processing graphs:  62%|██████▏   | 12889/20708 [02:44<01:25, 91.32graph/s]Processing graphs:  62%|██████▏   | 12899/20708 [02:44<01:28, 88.23graph/s]Processing graphs:  62%|██████▏   | 12910/20708 [02:44<01:25, 91.71graph/s]Processing graphs:  62%|██████▏   | 12920/20708 [02:44<01:24, 92.62graph/s]Processing graphs:  62%|██████▏   | 12930/20708 [02:44<01:25, 91.14graph/s]Processing graphs:  62%|██████▏   | 12940/20708 [02:45<01:25, 91.30graph/s]Processing graphs:  63%|██████▎   | 12950/20708 [02:45<01:25, 90.76graph/s]Processing graphs:  63%|██████▎   | 12960/20708 [02:45<01:24, 91.81graph/s]Processing graphs:  63%|██████▎   | 12970/20708 [02:45<01:23, 92.91graph/s]Processing graphs:  63%|██████▎   | 12980/20708 [02:45<01:25, 90.83graph/s]Processing graphs:  63%|██████▎   | 12990/20708 [02:45<01:24, 91.06graph/s]Processing graphs:  63%|██████▎   | 13000/20708 [02:45<01:24, 91.14graph/s]Processing graphs:  63%|██████▎   | 13010/20708 [02:45<01:25, 90.37graph/s]Processing graphs:  63%|██████▎   | 13020/20708 [02:45<01:29, 85.56graph/s]Processing graphs:  63%|██████▎   | 13029/20708 [02:46<01:33, 82.29graph/s]Processing graphs:  63%|██████▎   | 13038/20708 [02:46<01:38, 77.59graph/s]Processing graphs:  63%|██████▎   | 13046/20708 [02:46<01:40, 76.33graph/s]Processing graphs:  63%|██████▎   | 13054/20708 [02:46<01:40, 76.28graph/s]Processing graphs:  63%|██████▎   | 13062/20708 [02:46<01:41, 74.99graph/s]Processing graphs:  63%|██████▎   | 13071/20708 [02:46<01:38, 77.75graph/s]Processing graphs:  63%|██████▎   | 13079/20708 [02:46<01:38, 77.73graph/s]Processing graphs:  63%|██████▎   | 13087/20708 [02:46<01:39, 76.28graph/s]Processing graphs:  63%|██████▎   | 13095/20708 [02:46<01:42, 74.61graph/s]Processing graphs:  63%|██████▎   | 13103/20708 [02:47<01:41, 74.68graph/s]Processing graphs:  63%|██████▎   | 13111/20708 [02:47<01:40, 75.27graph/s]Processing graphs:  63%|██████▎   | 13120/20708 [02:47<01:37, 77.63graph/s]Processing graphs:  63%|██████▎   | 13129/20708 [02:47<01:35, 79.60graph/s]Processing graphs:  63%|██████▎   | 13137/20708 [02:47<01:37, 77.33graph/s]Processing graphs:  63%|██████▎   | 13146/20708 [02:47<01:36, 78.13graph/s]Processing graphs:  64%|██████▎   | 13154/20708 [02:47<01:36, 78.40graph/s]Processing graphs:  64%|██████▎   | 13162/20708 [02:47<01:37, 77.57graph/s]Processing graphs:  64%|██████▎   | 13171/20708 [02:47<01:35, 79.14graph/s]Processing graphs:  64%|██████▎   | 13179/20708 [02:48<01:38, 76.46graph/s]Processing graphs:  64%|██████▎   | 13188/20708 [02:48<01:37, 77.37graph/s]Processing graphs:  64%|██████▎   | 13196/20708 [02:48<01:36, 78.09graph/s]Processing graphs:  64%|██████▍   | 13204/20708 [02:48<01:38, 76.33graph/s]Processing graphs:  64%|██████▍   | 13212/20708 [02:48<01:38, 76.02graph/s]Processing graphs:  64%|██████▍   | 13220/20708 [02:48<01:39, 75.21graph/s]Processing graphs:  64%|██████▍   | 13228/20708 [02:48<01:38, 76.03graph/s]Processing graphs:  64%|██████▍   | 13236/20708 [02:48<01:39, 75.47graph/s]Processing graphs:  64%|██████▍   | 13244/20708 [02:48<01:38, 75.56graph/s]Processing graphs:  64%|██████▍   | 13252/20708 [02:48<01:40, 74.27graph/s]Processing graphs:  64%|██████▍   | 13260/20708 [02:49<01:38, 75.81graph/s]Processing graphs:  64%|██████▍   | 13268/20708 [02:49<01:37, 76.60graph/s]Processing graphs:  64%|██████▍   | 13276/20708 [02:49<01:36, 76.63graph/s]Processing graphs:  64%|██████▍   | 13284/20708 [02:49<01:36, 76.69graph/s]Processing graphs:  64%|██████▍   | 13293/20708 [02:49<01:34, 78.49graph/s]Processing graphs:  64%|██████▍   | 13302/20708 [02:49<01:30, 81.52graph/s]Processing graphs:  64%|██████▍   | 13311/20708 [02:49<01:33, 79.49graph/s]Processing graphs:  64%|██████▍   | 13320/20708 [02:49<01:32, 79.64graph/s]Processing graphs:  64%|██████▍   | 13328/20708 [02:49<01:36, 76.63graph/s]Processing graphs:  64%|██████▍   | 13336/20708 [02:50<01:37, 75.26graph/s]Processing graphs:  64%|██████▍   | 13345/20708 [02:50<01:32, 79.17graph/s]Processing graphs:  64%|██████▍   | 13354/20708 [02:50<01:30, 81.71graph/s]Processing graphs:  65%|██████▍   | 13363/20708 [02:50<01:35, 77.21graph/s]Processing graphs:  65%|██████▍   | 13371/20708 [02:50<01:35, 77.12graph/s]Processing graphs:  65%|██████▍   | 13380/20708 [02:50<01:32, 79.02graph/s]Processing graphs:  65%|██████▍   | 13388/20708 [02:50<01:33, 78.37graph/s]Processing graphs:  65%|██████▍   | 13396/20708 [02:50<01:33, 78.34graph/s]Processing graphs:  65%|██████▍   | 13404/20708 [02:50<01:33, 78.08graph/s]Processing graphs:  65%|██████▍   | 13412/20708 [02:51<01:36, 75.26graph/s]Processing graphs:  65%|██████▍   | 13420/20708 [02:51<01:39, 73.47graph/s]Processing graphs:  65%|██████▍   | 13428/20708 [02:51<01:38, 73.59graph/s]Processing graphs:  65%|██████▍   | 13437/20708 [02:51<01:36, 75.30graph/s]Processing graphs:  65%|██████▍   | 13446/20708 [02:51<01:32, 78.63graph/s]Processing graphs:  65%|██████▍   | 13454/20708 [02:51<01:34, 77.00graph/s]Processing graphs:  65%|██████▌   | 13462/20708 [02:51<01:34, 76.51graph/s]Processing graphs:  65%|██████▌   | 13470/20708 [02:51<01:34, 76.59graph/s]Processing graphs:  65%|██████▌   | 13478/20708 [02:51<01:34, 76.39graph/s]Processing graphs:  65%|██████▌   | 13486/20708 [02:51<01:36, 75.14graph/s]Processing graphs:  65%|██████▌   | 13494/20708 [02:52<01:39, 72.80graph/s]Processing graphs:  65%|██████▌   | 13502/20708 [02:52<01:39, 72.13graph/s]Processing graphs:  65%|██████▌   | 13510/20708 [02:52<01:37, 73.49graph/s]Processing graphs:  65%|██████▌   | 13518/20708 [02:52<01:39, 72.09graph/s]Processing graphs:  65%|██████▌   | 13526/20708 [02:52<01:37, 73.60graph/s]Processing graphs:  65%|██████▌   | 13534/20708 [02:52<01:36, 74.22graph/s]Processing graphs:  65%|██████▌   | 13542/20708 [02:52<01:36, 73.90graph/s]Processing graphs:  65%|██████▌   | 13550/20708 [02:52<01:36, 74.00graph/s]Processing graphs:  65%|██████▌   | 13559/20708 [02:52<01:32, 77.34graph/s]Processing graphs:  66%|██████▌   | 13567/20708 [02:53<01:34, 75.60graph/s]Processing graphs:  66%|██████▌   | 13575/20708 [02:53<01:33, 75.97graph/s]Processing graphs:  66%|██████▌   | 13583/20708 [02:53<01:32, 76.77graph/s]Processing graphs:  66%|██████▌   | 13591/20708 [02:53<01:34, 75.15graph/s]Processing graphs:  66%|██████▌   | 13599/20708 [02:53<01:33, 75.86graph/s]Processing graphs:  66%|██████▌   | 13607/20708 [02:53<01:33, 76.08graph/s]Processing graphs:  66%|██████▌   | 13615/20708 [02:53<01:34, 74.85graph/s]Processing graphs:  66%|██████▌   | 13624/20708 [02:53<01:31, 77.37graph/s]Processing graphs:  66%|██████▌   | 13633/20708 [02:53<01:29, 79.05graph/s]Processing graphs:  66%|██████▌   | 13641/20708 [02:54<01:29, 78.88graph/s]Processing graphs:  66%|██████▌   | 13649/20708 [02:54<01:32, 76.67graph/s]Processing graphs:  66%|██████▌   | 13657/20708 [02:54<01:32, 76.40graph/s]Processing graphs:  66%|██████▌   | 13666/20708 [02:54<01:29, 78.75graph/s]Processing graphs:  66%|██████▌   | 13674/20708 [02:54<01:30, 77.67graph/s]Processing graphs:  66%|██████▌   | 13682/20708 [02:54<01:31, 77.16graph/s]Processing graphs:  66%|██████▌   | 13691/20708 [02:54<01:29, 78.07graph/s]Processing graphs:  66%|██████▌   | 13699/20708 [02:54<01:32, 75.76graph/s]Processing graphs:  66%|██████▌   | 13708/20708 [02:54<01:30, 77.24graph/s]Processing graphs:  66%|██████▌   | 13716/20708 [02:55<01:30, 77.51graph/s]Processing graphs:  66%|██████▋   | 13724/20708 [02:55<01:30, 77.51graph/s]Processing graphs:  66%|██████▋   | 13732/20708 [02:55<01:31, 76.58graph/s]Processing graphs:  66%|██████▋   | 13741/20708 [02:55<01:29, 78.24graph/s]Processing graphs:  66%|██████▋   | 13749/20708 [02:55<01:30, 76.92graph/s]Processing graphs:  66%|██████▋   | 13757/20708 [02:55<01:29, 77.27graph/s]Processing graphs:  66%|██████▋   | 13765/20708 [02:55<01:32, 75.40graph/s]Processing graphs:  67%|██████▋   | 13774/20708 [02:55<01:29, 77.34graph/s]Processing graphs:  67%|██████▋   | 13782/20708 [02:55<01:30, 76.50graph/s]Processing graphs:  67%|██████▋   | 13791/20708 [02:55<01:28, 78.52graph/s]Processing graphs:  67%|██████▋   | 13800/20708 [02:56<01:26, 79.92graph/s]Processing graphs:  67%|██████▋   | 13808/20708 [02:56<01:28, 78.28graph/s]Processing graphs:  67%|██████▋   | 13817/20708 [02:56<01:27, 78.73graph/s]Processing graphs:  67%|██████▋   | 13825/20708 [02:56<01:28, 77.83graph/s]Processing graphs:  67%|██████▋   | 13834/20708 [02:56<01:27, 78.70graph/s]Processing graphs:  67%|██████▋   | 13843/20708 [02:56<01:24, 81.08graph/s]Processing graphs:  67%|██████▋   | 13852/20708 [02:56<01:26, 79.51graph/s]Processing graphs:  67%|██████▋   | 13860/20708 [02:56<01:28, 77.10graph/s]Processing graphs:  67%|██████▋   | 13868/20708 [02:56<01:30, 75.98graph/s]Processing graphs:  67%|██████▋   | 13877/20708 [02:57<01:27, 77.92graph/s]Processing graphs:  67%|██████▋   | 13885/20708 [02:57<01:28, 76.81graph/s]Processing graphs:  67%|██████▋   | 13893/20708 [02:57<01:28, 77.43graph/s]Processing graphs:  67%|██████▋   | 13902/20708 [02:57<01:26, 78.36graph/s]Processing graphs:  67%|██████▋   | 13910/20708 [02:57<01:27, 77.61graph/s]Processing graphs:  67%|██████▋   | 13918/20708 [02:57<01:28, 77.13graph/s]Processing graphs:  67%|██████▋   | 13927/20708 [02:57<01:24, 79.93graph/s]Processing graphs:  67%|██████▋   | 13935/20708 [02:57<01:25, 79.22graph/s]Processing graphs:  67%|██████▋   | 13943/20708 [02:57<01:26, 78.29graph/s]Processing graphs:  67%|██████▋   | 13951/20708 [02:58<01:27, 77.61graph/s]Processing graphs:  67%|██████▋   | 13959/20708 [02:58<01:26, 77.73graph/s]Processing graphs:  67%|██████▋   | 13967/20708 [02:58<01:28, 76.06graph/s]Processing graphs:  67%|██████▋   | 13975/20708 [02:58<01:28, 76.04graph/s]Processing graphs:  68%|██████▊   | 13983/20708 [02:58<01:31, 73.79graph/s]Processing graphs:  68%|██████▊   | 13992/20708 [02:58<01:28, 75.71graph/s]Processing graphs:  68%|██████▊   | 14000/20708 [02:58<01:27, 76.82graph/s]Processing graphs:  68%|██████▊   | 14009/20708 [02:58<01:23, 80.22graph/s]Processing graphs:  68%|██████▊   | 14019/20708 [02:58<01:18, 85.17graph/s]Processing graphs:  68%|██████▊   | 14029/20708 [02:58<01:16, 87.42graph/s]Processing graphs:  68%|██████▊   | 14038/20708 [02:59<01:18, 85.05graph/s]Processing graphs:  68%|██████▊   | 14047/20708 [02:59<01:17, 85.85graph/s]Processing graphs:  68%|██████▊   | 14056/20708 [02:59<01:18, 84.75graph/s]Processing graphs:  68%|██████▊   | 14065/20708 [02:59<01:19, 83.92graph/s]Processing graphs:  68%|██████▊   | 14075/20708 [02:59<01:16, 87.05graph/s]Processing graphs:  68%|██████▊   | 14084/20708 [02:59<01:17, 85.69graph/s]Processing graphs:  68%|██████▊   | 14094/20708 [02:59<01:15, 87.90graph/s]Processing graphs:  68%|██████▊   | 14103/20708 [02:59<01:15, 87.04graph/s]Processing graphs:  68%|██████▊   | 14113/20708 [02:59<01:13, 89.76graph/s]Processing graphs:  68%|██████▊   | 14122/20708 [03:00<01:14, 88.98graph/s]Processing graphs:  68%|██████▊   | 14131/20708 [03:00<01:14, 87.94graph/s]Processing graphs:  68%|██████▊   | 14141/20708 [03:00<01:13, 89.89graph/s]Processing graphs:  68%|██████▊   | 14150/20708 [03:00<01:14, 88.62graph/s]Processing graphs:  68%|██████▊   | 14160/20708 [03:00<01:12, 90.80graph/s]Processing graphs:  68%|██████▊   | 14170/20708 [03:00<01:14, 88.13graph/s]Processing graphs:  68%|██████▊   | 14179/20708 [03:00<01:15, 86.50graph/s]Processing graphs:  69%|██████▊   | 14188/20708 [03:00<01:16, 85.65graph/s]Processing graphs:  69%|██████▊   | 14197/20708 [03:00<01:16, 85.52graph/s]Processing graphs:  69%|██████▊   | 14206/20708 [03:01<01:16, 85.40graph/s]Processing graphs:  69%|██████▊   | 14215/20708 [03:01<01:16, 84.86graph/s]Processing graphs:  69%|██████▊   | 14224/20708 [03:01<01:16, 84.91graph/s]Processing graphs:  69%|██████▊   | 14233/20708 [03:01<01:15, 85.57graph/s]Processing graphs:  69%|██████▉   | 14242/20708 [03:01<01:16, 84.06graph/s]Processing graphs:  69%|██████▉   | 14251/20708 [03:01<01:16, 84.82graph/s]Processing graphs:  69%|██████▉   | 14261/20708 [03:01<01:13, 87.18graph/s]Processing graphs:  69%|██████▉   | 14270/20708 [03:01<01:14, 86.78graph/s]Processing graphs:  69%|██████▉   | 14279/20708 [03:01<01:17, 83.28graph/s]Processing graphs:  69%|██████▉   | 14288/20708 [03:02<01:17, 82.46graph/s]Processing graphs:  69%|██████▉   | 14297/20708 [03:02<01:17, 82.90graph/s]Processing graphs:  69%|██████▉   | 14307/20708 [03:02<01:13, 86.75graph/s]Processing graphs:  69%|██████▉   | 14317/20708 [03:02<01:11, 89.47graph/s]Processing graphs:  69%|██████▉   | 14327/20708 [03:02<01:11, 89.46graph/s]Processing graphs:  69%|██████▉   | 14337/20708 [03:02<01:11, 89.39graph/s]Processing graphs:  69%|██████▉   | 14346/20708 [03:02<01:11, 88.97graph/s]Processing graphs:  69%|██████▉   | 14355/20708 [03:02<01:12, 87.36graph/s]Processing graphs:  69%|██████▉   | 14365/20708 [03:02<01:11, 89.24graph/s]Processing graphs:  69%|██████▉   | 14374/20708 [03:02<01:12, 87.82graph/s]Processing graphs:  69%|██████▉   | 14384/20708 [03:03<01:11, 88.66graph/s]Processing graphs:  70%|██████▉   | 14393/20708 [03:03<01:11, 88.01graph/s]Processing graphs:  70%|██████▉   | 14403/20708 [03:03<01:09, 90.82graph/s]Processing graphs:  70%|██████▉   | 14413/20708 [03:03<01:09, 91.11graph/s]Processing graphs:  70%|██████▉   | 14423/20708 [03:03<01:07, 92.68graph/s]Processing graphs:  70%|██████▉   | 14433/20708 [03:03<01:11, 88.23graph/s]Processing graphs:  70%|██████▉   | 14442/20708 [03:03<01:10, 88.28graph/s]Processing graphs:  70%|██████▉   | 14451/20708 [03:03<01:13, 85.16graph/s]Processing graphs:  70%|██████▉   | 14461/20708 [03:03<01:10, 88.46graph/s]Processing graphs:  70%|██████▉   | 14470/20708 [03:04<01:11, 87.24graph/s]Processing graphs:  70%|██████▉   | 14479/20708 [03:04<01:14, 83.14graph/s]Processing graphs:  70%|██████▉   | 14489/20708 [03:04<01:11, 86.56graph/s]Processing graphs:  70%|███████   | 14499/20708 [03:04<01:09, 88.95graph/s]Processing graphs:  70%|███████   | 14508/20708 [03:04<01:11, 86.49graph/s]Processing graphs:  70%|███████   | 14518/20708 [03:04<01:09, 89.56graph/s]Processing graphs:  70%|███████   | 14528/20708 [03:04<01:07, 90.99graph/s]Processing graphs:  70%|███████   | 14538/20708 [03:04<01:12, 85.32graph/s]Processing graphs:  70%|███████   | 14547/20708 [03:04<01:11, 86.16graph/s]Processing graphs:  70%|███████   | 14558/20708 [03:05<01:08, 89.54graph/s]Processing graphs:  70%|███████   | 14568/20708 [03:05<01:07, 90.63graph/s]Processing graphs:  70%|███████   | 14578/20708 [03:05<01:10, 86.40graph/s]Processing graphs:  70%|███████   | 14588/20708 [03:05<01:10, 87.37graph/s]Processing graphs:  70%|███████   | 14597/20708 [03:05<01:09, 87.88graph/s]Processing graphs:  71%|███████   | 14606/20708 [03:05<01:10, 86.53graph/s]Processing graphs:  71%|███████   | 14615/20708 [03:05<01:11, 85.30graph/s]Processing graphs:  71%|███████   | 14624/20708 [03:05<01:14, 81.39graph/s]Processing graphs:  71%|███████   | 14634/20708 [03:05<01:13, 83.05graph/s]Processing graphs:  71%|███████   | 14644/20708 [03:06<01:09, 87.07graph/s]Processing graphs:  71%|███████   | 14653/20708 [03:06<01:11, 84.96graph/s]Processing graphs:  71%|███████   | 14662/20708 [03:06<01:12, 82.92graph/s]Processing graphs:  71%|███████   | 14671/20708 [03:06<01:12, 82.90graph/s]Processing graphs:  71%|███████   | 14682/20708 [03:06<01:07, 89.56graph/s]Processing graphs:  71%|███████   | 14692/20708 [03:06<01:08, 87.29graph/s]Processing graphs:  71%|███████   | 14701/20708 [03:06<01:09, 86.86graph/s]Processing graphs:  71%|███████   | 14710/20708 [03:06<01:08, 87.09graph/s]Processing graphs:  71%|███████   | 14720/20708 [03:06<01:07, 89.06graph/s]Processing graphs:  71%|███████   | 14730/20708 [03:07<01:06, 89.77graph/s]Processing graphs:  71%|███████   | 14739/20708 [03:07<01:07, 88.14graph/s]Processing graphs:  71%|███████   | 14748/20708 [03:07<01:07, 88.18graph/s]Processing graphs:  71%|███████▏  | 14757/20708 [03:07<01:07, 87.53graph/s]Processing graphs:  71%|███████▏  | 14767/20708 [03:07<01:07, 88.13graph/s]Processing graphs:  71%|███████▏  | 14777/20708 [03:07<01:05, 89.90graph/s]Processing graphs:  71%|███████▏  | 14788/20708 [03:07<01:03, 93.86graph/s]Processing graphs:  71%|███████▏  | 14798/20708 [03:07<01:02, 94.38graph/s]Processing graphs:  72%|███████▏  | 14808/20708 [03:07<01:04, 90.94graph/s]Processing graphs:  72%|███████▏  | 14818/20708 [03:08<01:09, 85.28graph/s]Processing graphs:  72%|███████▏  | 14827/20708 [03:08<01:08, 86.29graph/s]Processing graphs:  72%|███████▏  | 14836/20708 [03:08<01:08, 85.59graph/s]Processing graphs:  72%|███████▏  | 14845/20708 [03:08<01:10, 83.24graph/s]Processing graphs:  72%|███████▏  | 14854/20708 [03:08<01:10, 83.62graph/s]Processing graphs:  72%|███████▏  | 14864/20708 [03:08<01:07, 87.05graph/s]Processing graphs:  72%|███████▏  | 14873/20708 [03:08<01:06, 87.26graph/s]Processing graphs:  72%|███████▏  | 14882/20708 [03:08<01:07, 86.65graph/s]Processing graphs:  72%|███████▏  | 14891/20708 [03:08<01:06, 86.83graph/s]Processing graphs:  72%|███████▏  | 14900/20708 [03:09<01:11, 81.78graph/s]Processing graphs:  72%|███████▏  | 14909/20708 [03:09<01:09, 83.39graph/s]Processing graphs:  72%|███████▏  | 14918/20708 [03:09<01:08, 84.89graph/s]Processing graphs:  72%|███████▏  | 14927/20708 [03:09<01:09, 83.48graph/s]Processing graphs:  72%|███████▏  | 14937/20708 [03:09<01:07, 86.05graph/s]Processing graphs:  72%|███████▏  | 14948/20708 [03:09<01:04, 89.35graph/s]Processing graphs:  72%|███████▏  | 14958/20708 [03:09<01:03, 90.18graph/s]Processing graphs:  72%|███████▏  | 14968/20708 [03:09<01:09, 82.90graph/s]Processing graphs:  72%|███████▏  | 14977/20708 [03:09<01:08, 83.09graph/s]Processing graphs:  72%|███████▏  | 14986/20708 [03:10<01:07, 84.61graph/s]Processing graphs:  72%|███████▏  | 14995/20708 [03:10<01:06, 85.30graph/s]Processing graphs:  72%|███████▏  | 15004/20708 [03:10<01:06, 85.38graph/s]Processing graphs:  72%|███████▏  | 15013/20708 [03:10<01:08, 83.47graph/s]Processing graphs:  73%|███████▎  | 15022/20708 [03:10<01:10, 80.74graph/s]Processing graphs:  73%|███████▎  | 15031/20708 [03:10<01:12, 78.59graph/s]Processing graphs:  73%|███████▎  | 15039/20708 [03:10<01:14, 75.97graph/s]Processing graphs:  73%|███████▎  | 15047/20708 [03:10<01:13, 76.61graph/s]Processing graphs:  73%|███████▎  | 15055/20708 [03:10<01:15, 74.91graph/s]Processing graphs:  73%|███████▎  | 15063/20708 [03:11<01:16, 73.73graph/s]Processing graphs:  73%|███████▎  | 15071/20708 [03:11<01:17, 72.50graph/s]Processing graphs:  73%|███████▎  | 15079/20708 [03:11<01:19, 70.90graph/s]Processing graphs:  73%|███████▎  | 15087/20708 [03:11<01:16, 73.01graph/s]Processing graphs:  73%|███████▎  | 15095/20708 [03:11<01:16, 73.36graph/s]Processing graphs:  73%|███████▎  | 15103/20708 [03:11<01:18, 71.57graph/s]Processing graphs:  73%|███████▎  | 15111/20708 [03:11<01:16, 72.92graph/s]Processing graphs:  73%|███████▎  | 15119/20708 [03:11<01:15, 74.21graph/s]Processing graphs:  73%|███████▎  | 15127/20708 [03:11<01:14, 74.90graph/s]Processing graphs:  73%|███████▎  | 15135/20708 [03:12<01:15, 73.67graph/s]Processing graphs:  73%|███████▎  | 15144/20708 [03:12<01:13, 75.49graph/s]Processing graphs:  73%|███████▎  | 15153/20708 [03:12<01:11, 78.00graph/s]Processing graphs:  73%|███████▎  | 15161/20708 [03:12<01:10, 78.51graph/s]Processing graphs:  73%|███████▎  | 15169/20708 [03:12<01:11, 77.32graph/s]Processing graphs:  73%|███████▎  | 15178/20708 [03:12<01:08, 80.24graph/s]Processing graphs:  73%|███████▎  | 15187/20708 [03:12<01:10, 78.03graph/s]Processing graphs:  73%|███████▎  | 15196/20708 [03:12<01:09, 79.81graph/s]Processing graphs:  73%|███████▎  | 15205/20708 [03:12<01:12, 76.19graph/s]Processing graphs:  73%|███████▎  | 15213/20708 [03:13<01:14, 74.08graph/s]Processing graphs:  74%|███████▎  | 15221/20708 [03:13<01:12, 75.28graph/s]Processing graphs:  74%|███████▎  | 15229/20708 [03:13<01:13, 74.83graph/s]Processing graphs:  74%|███████▎  | 15238/20708 [03:13<01:11, 76.40graph/s]Processing graphs:  74%|███████▎  | 15246/20708 [03:13<01:14, 73.69graph/s]Processing graphs:  74%|███████▎  | 15255/20708 [03:13<01:11, 76.19graph/s]Processing graphs:  74%|███████▎  | 15263/20708 [03:13<01:12, 75.48graph/s]Processing graphs:  74%|███████▎  | 15271/20708 [03:13<01:13, 74.11graph/s]Processing graphs:  74%|███████▍  | 15279/20708 [03:13<01:14, 73.15graph/s]Processing graphs:  74%|███████▍  | 15287/20708 [03:14<01:15, 71.95graph/s]Processing graphs:  74%|███████▍  | 15295/20708 [03:14<01:14, 72.94graph/s]Processing graphs:  74%|███████▍  | 15303/20708 [03:14<01:14, 72.76graph/s]Processing graphs:  74%|███████▍  | 15311/20708 [03:14<01:13, 73.66graph/s]Processing graphs:  74%|███████▍  | 15320/20708 [03:14<01:11, 75.34graph/s]Processing graphs:  74%|███████▍  | 15328/20708 [03:14<01:11, 75.52graph/s]Processing graphs:  74%|███████▍  | 15337/20708 [03:14<01:10, 76.69graph/s]Processing graphs:  74%|███████▍  | 15345/20708 [03:14<01:12, 74.41graph/s]Processing graphs:  74%|███████▍  | 15353/20708 [03:14<01:12, 73.62graph/s]Processing graphs:  74%|███████▍  | 15361/20708 [03:15<01:12, 73.99graph/s]Processing graphs:  74%|███████▍  | 15370/20708 [03:15<01:09, 76.36graph/s]Processing graphs:  74%|███████▍  | 15378/20708 [03:15<01:09, 76.70graph/s]Processing graphs:  74%|███████▍  | 15386/20708 [03:15<01:08, 77.53graph/s]Processing graphs:  74%|███████▍  | 15394/20708 [03:15<01:09, 76.78graph/s]Processing graphs:  74%|███████▍  | 15402/20708 [03:15<01:11, 74.14graph/s]Processing graphs:  74%|███████▍  | 15410/20708 [03:15<01:12, 73.35graph/s]Processing graphs:  74%|███████▍  | 15418/20708 [03:15<01:11, 73.68graph/s]Processing graphs:  74%|███████▍  | 15426/20708 [03:15<01:11, 74.18graph/s]Processing graphs:  75%|███████▍  | 15434/20708 [03:15<01:11, 73.74graph/s]Processing graphs:  75%|███████▍  | 15442/20708 [03:16<01:11, 73.36graph/s]Processing graphs:  75%|███████▍  | 15450/20708 [03:16<01:12, 72.84graph/s]Processing graphs:  75%|███████▍  | 15458/20708 [03:16<01:10, 74.18graph/s]Processing graphs:  75%|███████▍  | 15467/20708 [03:16<01:09, 75.88graph/s]Processing graphs:  75%|███████▍  | 15475/20708 [03:16<01:09, 75.32graph/s]Processing graphs:  75%|███████▍  | 15483/20708 [03:16<01:10, 74.12graph/s]Processing graphs:  75%|███████▍  | 15491/20708 [03:16<01:09, 74.80graph/s]Processing graphs:  75%|███████▍  | 15499/20708 [03:16<01:10, 73.97graph/s]Processing graphs:  75%|███████▍  | 15508/20708 [03:16<01:07, 76.77graph/s]Processing graphs:  75%|███████▍  | 15516/20708 [03:17<01:08, 76.04graph/s]Processing graphs:  75%|███████▍  | 15524/20708 [03:17<01:07, 76.27graph/s]Processing graphs:  75%|███████▌  | 15532/20708 [03:17<01:08, 75.70graph/s]Processing graphs:  75%|███████▌  | 15540/20708 [03:17<01:07, 76.73graph/s]Processing graphs:  75%|███████▌  | 15549/20708 [03:17<01:06, 77.26graph/s]Processing graphs:  75%|███████▌  | 15558/20708 [03:17<01:05, 78.79graph/s]Processing graphs:  75%|███████▌  | 15566/20708 [03:17<01:06, 77.49graph/s]Processing graphs:  75%|███████▌  | 15575/20708 [03:17<01:04, 79.83graph/s]Processing graphs:  75%|███████▌  | 15584/20708 [03:17<01:03, 80.46graph/s]Processing graphs:  75%|███████▌  | 15593/20708 [03:18<01:03, 80.87graph/s]Processing graphs:  75%|███████▌  | 15602/20708 [03:18<01:01, 83.30graph/s]Processing graphs:  75%|███████▌  | 15611/20708 [03:18<01:03, 80.44graph/s]Processing graphs:  75%|███████▌  | 15620/20708 [03:18<01:01, 82.21graph/s]Processing graphs:  75%|███████▌  | 15629/20708 [03:18<01:00, 83.43graph/s]Processing graphs:  76%|███████▌  | 15638/20708 [03:18<01:02, 81.44graph/s]Processing graphs:  76%|███████▌  | 15647/20708 [03:18<01:03, 79.84graph/s]Processing graphs:  76%|███████▌  | 15656/20708 [03:18<01:05, 77.52graph/s]Processing graphs:  76%|███████▌  | 15664/20708 [03:18<01:04, 77.99graph/s]Processing graphs:  76%|███████▌  | 15672/20708 [03:19<01:04, 78.20graph/s]Processing graphs:  76%|███████▌  | 15680/20708 [03:19<01:04, 78.48graph/s]Processing graphs:  76%|███████▌  | 15688/20708 [03:19<01:05, 76.98graph/s]Processing graphs:  76%|███████▌  | 15697/20708 [03:19<01:04, 78.23graph/s]Processing graphs:  76%|███████▌  | 15705/20708 [03:19<01:04, 77.17graph/s]Processing graphs:  76%|███████▌  | 15713/20708 [03:19<01:04, 77.42graph/s]Processing graphs:  76%|███████▌  | 15723/20708 [03:19<01:00, 81.92graph/s]Processing graphs:  76%|███████▌  | 15732/20708 [03:19<01:01, 81.27graph/s]Processing graphs:  76%|███████▌  | 15741/20708 [03:19<00:59, 83.62graph/s]Processing graphs:  76%|███████▌  | 15750/20708 [03:19<01:00, 82.61graph/s]Processing graphs:  76%|███████▌  | 15759/20708 [03:20<00:59, 82.86graph/s]Processing graphs:  76%|███████▌  | 15770/20708 [03:20<00:55, 89.25graph/s]Processing graphs:  76%|███████▌  | 15779/20708 [03:20<00:57, 86.29graph/s]Processing graphs:  76%|███████▌  | 15788/20708 [03:20<00:58, 83.59graph/s]Processing graphs:  76%|███████▋  | 15797/20708 [03:20<01:00, 81.80graph/s]Processing graphs:  76%|███████▋  | 15806/20708 [03:20<00:59, 83.02graph/s]Processing graphs:  76%|███████▋  | 15815/20708 [03:20<00:58, 82.99graph/s]Processing graphs:  76%|███████▋  | 15824/20708 [03:20<01:00, 81.20graph/s]Processing graphs:  76%|███████▋  | 15834/20708 [03:20<00:58, 83.65graph/s]Processing graphs:  77%|███████▋  | 15844/20708 [03:21<00:57, 84.63graph/s]Processing graphs:  77%|███████▋  | 15853/20708 [03:21<00:58, 83.60graph/s]Processing graphs:  77%|███████▋  | 15862/20708 [03:21<00:57, 84.84graph/s]Processing graphs:  77%|███████▋  | 15871/20708 [03:21<00:56, 85.23graph/s]Processing graphs:  77%|███████▋  | 15880/20708 [03:21<00:55, 86.35graph/s]Processing graphs:  77%|███████▋  | 15889/20708 [03:21<00:57, 83.71graph/s]Processing graphs:  77%|███████▋  | 15898/20708 [03:21<00:58, 82.72graph/s]Processing graphs:  77%|███████▋  | 15907/20708 [03:21<00:56, 84.42graph/s]Processing graphs:  77%|███████▋  | 15916/20708 [03:21<00:57, 83.55graph/s]Processing graphs:  77%|███████▋  | 15925/20708 [03:22<00:59, 80.82graph/s]Processing graphs:  77%|███████▋  | 15934/20708 [03:22<00:58, 82.01graph/s]Processing graphs:  77%|███████▋  | 15943/20708 [03:22<00:58, 81.10graph/s]Processing graphs:  77%|███████▋  | 15953/20708 [03:22<00:56, 83.52graph/s]Processing graphs:  77%|███████▋  | 15962/20708 [03:22<00:57, 83.07graph/s]Processing graphs:  77%|███████▋  | 15971/20708 [03:22<00:59, 80.06graph/s]Processing graphs:  77%|███████▋  | 15980/20708 [03:22<01:00, 77.92graph/s]Processing graphs:  77%|███████▋  | 15988/20708 [03:22<01:00, 78.15graph/s]Processing graphs:  77%|███████▋  | 15997/20708 [03:22<00:57, 81.24graph/s]Processing graphs:  77%|███████▋  | 16006/20708 [03:23<00:58, 80.33graph/s]Processing graphs:  77%|███████▋  | 16015/20708 [03:23<00:57, 81.81graph/s]Processing graphs:  77%|███████▋  | 16025/20708 [03:23<00:55, 84.20graph/s]Processing graphs:  77%|███████▋  | 16035/20708 [03:23<00:53, 87.89graph/s]Processing graphs:  77%|███████▋  | 16045/20708 [03:23<00:52, 88.70graph/s]Processing graphs:  78%|███████▊  | 16054/20708 [03:23<00:53, 87.36graph/s]Processing graphs:  78%|███████▊  | 16063/20708 [03:23<00:54, 85.01graph/s]Processing graphs:  78%|███████▊  | 16072/20708 [03:23<00:55, 83.04graph/s]Processing graphs:  78%|███████▊  | 16081/20708 [03:23<00:55, 83.29graph/s]Processing graphs:  78%|███████▊  | 16090/20708 [03:24<00:55, 83.57graph/s]Processing graphs:  78%|███████▊  | 16099/20708 [03:24<00:54, 84.07graph/s]Processing graphs:  78%|███████▊  | 16108/20708 [03:24<00:56, 81.94graph/s]Processing graphs:  78%|███████▊  | 16118/20708 [03:24<00:55, 83.34graph/s]Processing graphs:  78%|███████▊  | 16127/20708 [03:24<00:55, 82.39graph/s]Processing graphs:  78%|███████▊  | 16136/20708 [03:24<00:56, 81.20graph/s]Processing graphs:  78%|███████▊  | 16145/20708 [03:24<00:55, 82.90graph/s]Processing graphs:  78%|███████▊  | 16154/20708 [03:24<00:55, 81.53graph/s]Processing graphs:  78%|███████▊  | 16163/20708 [03:24<00:58, 77.54graph/s]Processing graphs:  78%|███████▊  | 16172/20708 [03:25<00:58, 77.88graph/s]Processing graphs:  78%|███████▊  | 16181/20708 [03:25<00:56, 80.48graph/s]Processing graphs:  78%|███████▊  | 16191/20708 [03:25<00:54, 83.35graph/s]Processing graphs:  78%|███████▊  | 16201/20708 [03:25<00:51, 87.05graph/s]Processing graphs:  78%|███████▊  | 16211/20708 [03:25<00:50, 88.65graph/s]Processing graphs:  78%|███████▊  | 16221/20708 [03:25<00:50, 89.54graph/s]Processing graphs:  78%|███████▊  | 16231/20708 [03:25<00:49, 90.82graph/s]Processing graphs:  78%|███████▊  | 16241/20708 [03:25<00:48, 91.58graph/s]Processing graphs:  78%|███████▊  | 16251/20708 [03:25<00:48, 91.68graph/s]Processing graphs:  79%|███████▊  | 16261/20708 [03:26<00:50, 87.44graph/s]Processing graphs:  79%|███████▊  | 16270/20708 [03:26<00:53, 82.29graph/s]Processing graphs:  79%|███████▊  | 16279/20708 [03:26<00:53, 83.21graph/s]Processing graphs:  79%|███████▊  | 16288/20708 [03:26<00:53, 82.38graph/s]Processing graphs:  79%|███████▊  | 16297/20708 [03:26<00:53, 81.72graph/s]Processing graphs:  79%|███████▊  | 16306/20708 [03:26<00:53, 82.11graph/s]Processing graphs:  79%|███████▉  | 16316/20708 [03:26<00:51, 85.11graph/s]Processing graphs:  79%|███████▉  | 16325/20708 [03:26<00:54, 80.47graph/s]Processing graphs:  79%|███████▉  | 16334/20708 [03:26<00:54, 79.55graph/s]Processing graphs:  79%|███████▉  | 16342/20708 [03:27<00:56, 77.29graph/s]Processing graphs:  79%|███████▉  | 16352/20708 [03:27<00:53, 80.99graph/s]Processing graphs:  79%|███████▉  | 16361/20708 [03:27<00:53, 81.64graph/s]Processing graphs:  79%|███████▉  | 16371/20708 [03:27<00:50, 86.50graph/s]Processing graphs:  79%|███████▉  | 16380/20708 [03:27<00:49, 87.44graph/s]Processing graphs:  79%|███████▉  | 16389/20708 [03:27<00:49, 86.77graph/s]Processing graphs:  79%|███████▉  | 16398/20708 [03:27<00:52, 82.81graph/s]Processing graphs:  79%|███████▉  | 16407/20708 [03:27<00:52, 82.20graph/s]Processing graphs:  79%|███████▉  | 16417/20708 [03:27<00:51, 84.09graph/s]Processing graphs:  79%|███████▉  | 16426/20708 [03:28<00:52, 80.88graph/s]Processing graphs:  79%|███████▉  | 16436/20708 [03:28<00:50, 85.03graph/s]Processing graphs:  79%|███████▉  | 16445/20708 [03:28<00:50, 84.68graph/s]Processing graphs:  79%|███████▉  | 16454/20708 [03:28<00:53, 79.90graph/s]Processing graphs:  80%|███████▉  | 16463/20708 [03:28<00:52, 81.63graph/s]Processing graphs:  80%|███████▉  | 16472/20708 [03:28<00:52, 80.22graph/s]Processing graphs:  80%|███████▉  | 16481/20708 [03:28<00:53, 79.39graph/s]Processing graphs:  80%|███████▉  | 16490/20708 [03:28<00:51, 81.76graph/s]Processing graphs:  80%|███████▉  | 16499/20708 [03:28<00:50, 83.54graph/s]Processing graphs:  80%|███████▉  | 16508/20708 [03:29<00:50, 82.69graph/s]Processing graphs:  80%|███████▉  | 16518/20708 [03:29<00:49, 84.23graph/s]Processing graphs:  80%|███████▉  | 16527/20708 [03:29<00:49, 84.37graph/s]Processing graphs:  80%|███████▉  | 16536/20708 [03:29<00:51, 81.09graph/s]Processing graphs:  80%|███████▉  | 16545/20708 [03:29<00:51, 80.44graph/s]Processing graphs:  80%|███████▉  | 16554/20708 [03:29<00:52, 79.03graph/s]Processing graphs:  80%|███████▉  | 16563/20708 [03:29<00:50, 81.63graph/s]Processing graphs:  80%|████████  | 16572/20708 [03:29<00:51, 80.99graph/s]Processing graphs:  80%|████████  | 16581/20708 [03:29<00:49, 82.93graph/s]Processing graphs:  80%|████████  | 16591/20708 [03:30<00:47, 86.00graph/s]Processing graphs:  80%|████████  | 16600/20708 [03:30<00:47, 87.08graph/s]Processing graphs:  80%|████████  | 16609/20708 [03:30<00:47, 87.08graph/s]Processing graphs:  80%|████████  | 16618/20708 [03:30<00:46, 87.20graph/s]Processing graphs:  80%|████████  | 16627/20708 [03:30<00:47, 85.90graph/s]Processing graphs:  80%|████████  | 16636/20708 [03:30<00:47, 85.92graph/s]Processing graphs:  80%|████████  | 16646/20708 [03:30<00:46, 86.58graph/s]Processing graphs:  80%|████████  | 16655/20708 [03:30<00:47, 84.82graph/s]Processing graphs:  80%|████████  | 16665/20708 [03:30<00:46, 86.46graph/s]Processing graphs:  81%|████████  | 16675/20708 [03:31<00:46, 86.68graph/s]Processing graphs:  81%|████████  | 16685/20708 [03:31<00:45, 87.61graph/s]Processing graphs:  81%|████████  | 16694/20708 [03:31<00:47, 85.10graph/s]Processing graphs:  81%|████████  | 16703/20708 [03:31<00:49, 80.81graph/s]Processing graphs:  81%|████████  | 16712/20708 [03:31<00:51, 77.12graph/s]Processing graphs:  81%|████████  | 16721/20708 [03:31<00:50, 79.05graph/s]Processing graphs:  81%|████████  | 16729/20708 [03:31<00:52, 75.59graph/s]Processing graphs:  81%|████████  | 16737/20708 [03:31<00:52, 76.15graph/s]Processing graphs:  81%|████████  | 16745/20708 [03:31<00:52, 75.69graph/s]Processing graphs:  81%|████████  | 16754/20708 [03:32<00:50, 78.31graph/s]Processing graphs:  81%|████████  | 16763/20708 [03:32<00:49, 80.32graph/s]Processing graphs:  81%|████████  | 16772/20708 [03:32<00:49, 79.47graph/s]Processing graphs:  81%|████████  | 16781/20708 [03:32<00:49, 79.19graph/s]Processing graphs:  81%|████████  | 16789/20708 [03:32<00:51, 76.77graph/s]Processing graphs:  81%|████████  | 16797/20708 [03:32<00:52, 74.45graph/s]Processing graphs:  81%|████████  | 16806/20708 [03:32<00:51, 75.78graph/s]Processing graphs:  81%|████████  | 16814/20708 [03:32<00:50, 76.84graph/s]Processing graphs:  81%|████████  | 16822/20708 [03:32<00:51, 76.01graph/s]Processing graphs:  81%|████████▏ | 16831/20708 [03:33<00:49, 77.93graph/s]Processing graphs:  81%|████████▏ | 16839/20708 [03:33<00:50, 76.36graph/s]Processing graphs:  81%|████████▏ | 16847/20708 [03:33<00:50, 76.76graph/s]Processing graphs:  81%|████████▏ | 16855/20708 [03:33<00:51, 74.99graph/s]Processing graphs:  81%|████████▏ | 16864/20708 [03:33<00:49, 77.22graph/s]Processing graphs:  81%|████████▏ | 16873/20708 [03:33<00:49, 78.24graph/s]Processing graphs:  82%|████████▏ | 16881/20708 [03:33<00:49, 77.84graph/s]Processing graphs:  82%|████████▏ | 16889/20708 [03:33<00:49, 76.92graph/s]Processing graphs:  82%|████████▏ | 16898/20708 [03:33<00:48, 79.29graph/s]Processing graphs:  82%|████████▏ | 16907/20708 [03:34<00:47, 79.85graph/s]Processing graphs:  82%|████████▏ | 16915/20708 [03:34<00:49, 76.47graph/s]Processing graphs:  82%|████████▏ | 16923/20708 [03:34<00:49, 77.22graph/s]Processing graphs:  82%|████████▏ | 16931/20708 [03:34<00:50, 74.87graph/s]Processing graphs:  82%|████████▏ | 16939/20708 [03:34<00:50, 74.75graph/s]Processing graphs:  82%|████████▏ | 16947/20708 [03:34<00:49, 75.62graph/s]Processing graphs:  82%|████████▏ | 16955/20708 [03:34<00:50, 73.87graph/s]Processing graphs:  82%|████████▏ | 16963/20708 [03:34<00:49, 75.07graph/s]Processing graphs:  82%|████████▏ | 16971/20708 [03:34<00:51, 73.27graph/s]Processing graphs:  82%|████████▏ | 16980/20708 [03:35<00:48, 77.27graph/s]Processing graphs:  82%|████████▏ | 16988/20708 [03:35<00:48, 77.04graph/s]Processing graphs:  82%|████████▏ | 16997/20708 [03:35<00:47, 78.88graph/s]Processing graphs:  82%|████████▏ | 17006/20708 [03:35<00:45, 80.51graph/s]Processing graphs:  82%|████████▏ | 17015/20708 [03:35<00:46, 80.16graph/s]Processing graphs:  82%|████████▏ | 17024/20708 [03:35<00:45, 81.33graph/s]Processing graphs:  82%|████████▏ | 17033/20708 [03:35<00:47, 77.53graph/s]Processing graphs:  82%|████████▏ | 17041/20708 [03:35<00:48, 75.86graph/s]Processing graphs:  82%|████████▏ | 17050/20708 [03:35<00:45, 79.73graph/s]Processing graphs:  82%|████████▏ | 17059/20708 [03:36<00:47, 77.47graph/s]Processing graphs:  82%|████████▏ | 17068/20708 [03:36<00:46, 78.75graph/s]Processing graphs:  82%|████████▏ | 17076/20708 [03:36<00:46, 78.25graph/s]Processing graphs:  83%|████████▎ | 17085/20708 [03:36<00:44, 80.53graph/s]Processing graphs:  83%|████████▎ | 17094/20708 [03:36<00:47, 75.94graph/s]Processing graphs:  83%|████████▎ | 17102/20708 [03:36<00:48, 73.93graph/s]Processing graphs:  83%|████████▎ | 17110/20708 [03:36<00:49, 73.04graph/s]Processing graphs:  83%|████████▎ | 17118/20708 [03:36<00:48, 74.48graph/s]Processing graphs:  83%|████████▎ | 17126/20708 [03:36<00:48, 74.61graph/s]Processing graphs:  83%|████████▎ | 17135/20708 [03:37<00:45, 78.36graph/s]Processing graphs:  83%|████████▎ | 17143/20708 [03:37<00:45, 78.65graph/s]Processing graphs:  83%|████████▎ | 17151/20708 [03:37<00:46, 76.82graph/s]Processing graphs:  83%|████████▎ | 17159/20708 [03:37<00:46, 76.55graph/s]Processing graphs:  83%|████████▎ | 17167/20708 [03:37<00:46, 75.74graph/s]Processing graphs:  83%|████████▎ | 17175/20708 [03:37<00:46, 76.59graph/s]Processing graphs:  83%|████████▎ | 17184/20708 [03:37<00:45, 78.02graph/s]Processing graphs:  83%|████████▎ | 17192/20708 [03:37<00:45, 76.88graph/s]Processing graphs:  83%|████████▎ | 17202/20708 [03:37<00:43, 81.16graph/s]Processing graphs:  83%|████████▎ | 17211/20708 [03:37<00:43, 80.16graph/s]Processing graphs:  83%|████████▎ | 17220/20708 [03:38<00:43, 79.96graph/s]Processing graphs:  83%|████████▎ | 17228/20708 [03:38<00:45, 76.86graph/s]Processing graphs:  83%|████████▎ | 17236/20708 [03:38<00:45, 76.29graph/s]Processing graphs:  83%|████████▎ | 17244/20708 [03:38<00:45, 76.94graph/s]Processing graphs:  83%|████████▎ | 17253/20708 [03:38<00:43, 79.84graph/s]Processing graphs:  83%|████████▎ | 17261/20708 [03:38<00:44, 77.67graph/s]Processing graphs:  83%|████████▎ | 17270/20708 [03:38<00:42, 80.09graph/s]Processing graphs:  83%|████████▎ | 17279/20708 [03:38<00:42, 80.49graph/s]Processing graphs:  83%|████████▎ | 17288/20708 [03:38<00:42, 80.52graph/s]Processing graphs:  84%|████████▎ | 17297/20708 [03:39<00:41, 81.79graph/s]Processing graphs:  84%|████████▎ | 17306/20708 [03:39<00:41, 81.42graph/s]Processing graphs:  84%|████████▎ | 17315/20708 [03:39<00:43, 78.83graph/s]Processing graphs:  84%|████████▎ | 17323/20708 [03:39<00:43, 78.56graph/s]Processing graphs:  84%|████████▎ | 17331/20708 [03:39<00:44, 76.69graph/s]Processing graphs:  84%|████████▎ | 17339/20708 [03:39<00:43, 77.20graph/s]Processing graphs:  84%|████████▍ | 17348/20708 [03:39<00:42, 79.01graph/s]Processing graphs:  84%|████████▍ | 17356/20708 [03:39<00:42, 78.18graph/s]Processing graphs:  84%|████████▍ | 17365/20708 [03:39<00:42, 78.72graph/s]Processing graphs:  84%|████████▍ | 17374/20708 [03:40<00:41, 79.99graph/s]Processing graphs:  84%|████████▍ | 17383/20708 [03:40<00:40, 81.24graph/s]Processing graphs:  84%|████████▍ | 17392/20708 [03:40<00:40, 81.14graph/s]Processing graphs:  84%|████████▍ | 17401/20708 [03:40<00:42, 78.37graph/s]Processing graphs:  84%|████████▍ | 17410/20708 [03:40<00:41, 80.14graph/s]Processing graphs:  84%|████████▍ | 17419/20708 [03:40<00:41, 80.09graph/s]Processing graphs:  84%|████████▍ | 17429/20708 [03:40<00:39, 83.80graph/s]Processing graphs:  84%|████████▍ | 17438/20708 [03:40<00:39, 83.80graph/s]Processing graphs:  84%|████████▍ | 17447/20708 [03:40<00:40, 80.88graph/s]Processing graphs:  84%|████████▍ | 17456/20708 [03:41<00:40, 80.35graph/s]Processing graphs:  84%|████████▍ | 17465/20708 [03:41<00:40, 79.67graph/s]Processing graphs:  84%|████████▍ | 17473/20708 [03:41<00:41, 78.36graph/s]Processing graphs:  84%|████████▍ | 17481/20708 [03:41<00:42, 75.46graph/s]Processing graphs:  84%|████████▍ | 17490/20708 [03:41<00:41, 76.87graph/s]Processing graphs:  84%|████████▍ | 17498/20708 [03:41<00:41, 76.62graph/s]Processing graphs:  85%|████████▍ | 17506/20708 [03:41<00:42, 75.23graph/s]Processing graphs:  85%|████████▍ | 17514/20708 [03:41<00:42, 75.01graph/s]Processing graphs:  85%|████████▍ | 17522/20708 [03:41<00:42, 74.68graph/s]Processing graphs:  85%|████████▍ | 17530/20708 [03:42<00:42, 75.00graph/s]Processing graphs:  85%|████████▍ | 17539/20708 [03:42<00:40, 77.80graph/s]Processing graphs:  85%|████████▍ | 17547/20708 [03:42<00:41, 75.30graph/s]Processing graphs:  85%|████████▍ | 17556/20708 [03:42<00:40, 78.04graph/s]Processing graphs:  85%|████████▍ | 17564/20708 [03:42<00:41, 76.67graph/s]Processing graphs:  85%|████████▍ | 17573/20708 [03:42<00:39, 78.41graph/s]Processing graphs:  85%|████████▍ | 17581/20708 [03:42<00:41, 75.98graph/s]Processing graphs:  85%|████████▍ | 17590/20708 [03:42<00:39, 78.68graph/s]Processing graphs:  85%|████████▍ | 17598/20708 [03:42<00:40, 77.19graph/s]Processing graphs:  85%|████████▌ | 17607/20708 [03:43<00:39, 79.20graph/s]Processing graphs:  85%|████████▌ | 17615/20708 [03:43<00:38, 79.34graph/s]Processing graphs:  85%|████████▌ | 17624/20708 [03:43<00:39, 79.07graph/s]Processing graphs:  85%|████████▌ | 17633/20708 [03:43<00:38, 79.22graph/s]Processing graphs:  85%|████████▌ | 17642/20708 [03:43<00:37, 81.37graph/s]Processing graphs:  85%|████████▌ | 17652/20708 [03:43<00:36, 83.13graph/s]Processing graphs:  85%|████████▌ | 17661/20708 [03:43<00:38, 79.08graph/s]Processing graphs:  85%|████████▌ | 17669/20708 [03:43<00:38, 79.12graph/s]Processing graphs:  85%|████████▌ | 17677/20708 [03:43<00:38, 77.73graph/s]Processing graphs:  85%|████████▌ | 17686/20708 [03:44<00:38, 79.38graph/s]Processing graphs:  85%|████████▌ | 17694/20708 [03:44<00:38, 79.07graph/s]Processing graphs:  85%|████████▌ | 17702/20708 [03:44<00:39, 75.74graph/s]Processing graphs:  86%|████████▌ | 17711/20708 [03:44<00:38, 78.16graph/s]Processing graphs:  86%|████████▌ | 17719/20708 [03:44<00:39, 75.90graph/s]Processing graphs:  86%|████████▌ | 17727/20708 [03:44<00:39, 75.51graph/s]Processing graphs:  86%|████████▌ | 17735/20708 [03:44<00:39, 74.75graph/s]Processing graphs:  86%|████████▌ | 17743/20708 [03:44<00:38, 76.11graph/s]Processing graphs:  86%|████████▌ | 17751/20708 [03:44<00:39, 75.73graph/s]Processing graphs:  86%|████████▌ | 17759/20708 [03:44<00:38, 75.81graph/s]Processing graphs:  86%|████████▌ | 17767/20708 [03:45<00:39, 75.30graph/s]Processing graphs:  86%|████████▌ | 17775/20708 [03:45<00:39, 74.28graph/s]Processing graphs:  86%|████████▌ | 17783/20708 [03:45<00:39, 73.93graph/s]Processing graphs:  86%|████████▌ | 17791/20708 [03:45<00:38, 75.16graph/s]Processing graphs:  86%|████████▌ | 17799/20708 [03:45<00:38, 75.14graph/s]Processing graphs:  86%|████████▌ | 17807/20708 [03:45<00:38, 74.63graph/s]Processing graphs:  86%|████████▌ | 17815/20708 [03:45<00:38, 74.75graph/s]Processing graphs:  86%|████████▌ | 17823/20708 [03:45<00:38, 75.20graph/s]Processing graphs:  86%|████████▌ | 17831/20708 [03:45<00:37, 75.90graph/s]Processing graphs:  86%|████████▌ | 17839/20708 [03:46<00:37, 75.62graph/s]Processing graphs:  86%|████████▌ | 17847/20708 [03:46<00:37, 76.21graph/s]Processing graphs:  86%|████████▌ | 17855/20708 [03:46<00:37, 75.92graph/s]Processing graphs:  86%|████████▋ | 17863/20708 [03:46<00:37, 76.41graph/s]Processing graphs:  86%|████████▋ | 17871/20708 [03:46<00:37, 75.21graph/s]Processing graphs:  86%|████████▋ | 17879/20708 [03:46<00:38, 74.42graph/s]Processing graphs:  86%|████████▋ | 17887/20708 [03:46<00:38, 73.80graph/s]Processing graphs:  86%|████████▋ | 17895/20708 [03:46<00:38, 73.27graph/s]Processing graphs:  86%|████████▋ | 17903/20708 [03:46<00:38, 73.02graph/s]Processing graphs:  86%|████████▋ | 17911/20708 [03:47<00:38, 73.29graph/s]Processing graphs:  87%|████████▋ | 17919/20708 [03:47<00:38, 73.04graph/s]Processing graphs:  87%|████████▋ | 17927/20708 [03:47<00:38, 72.94graph/s]Processing graphs:  87%|████████▋ | 17935/20708 [03:47<00:37, 73.31graph/s]Processing graphs:  87%|████████▋ | 17943/20708 [03:47<00:38, 72.72graph/s]Processing graphs:  87%|████████▋ | 17951/20708 [03:47<00:37, 73.87graph/s]Processing graphs:  87%|████████▋ | 17959/20708 [03:47<00:37, 73.91graph/s]Processing graphs:  87%|████████▋ | 17967/20708 [03:47<00:37, 73.81graph/s]Processing graphs:  87%|████████▋ | 17975/20708 [03:47<00:36, 74.19graph/s]Processing graphs:  87%|████████▋ | 17983/20708 [03:48<00:37, 72.46graph/s]Processing graphs:  87%|████████▋ | 17991/20708 [03:48<00:36, 74.33graph/s]Processing graphs:  87%|████████▋ | 17999/20708 [03:48<00:37, 72.70graph/s]Processing graphs:  87%|████████▋ | 18007/20708 [03:48<00:37, 72.59graph/s]Processing graphs:  87%|████████▋ | 18015/20708 [03:48<00:36, 73.40graph/s]Processing graphs:  87%|████████▋ | 18023/20708 [03:48<00:37, 72.13graph/s]Processing graphs:  87%|████████▋ | 18031/20708 [03:48<00:36, 73.19graph/s]Processing graphs:  87%|████████▋ | 18039/20708 [03:48<00:36, 73.32graph/s]Processing graphs:  87%|████████▋ | 18047/20708 [03:48<00:36, 72.95graph/s]Processing graphs:  87%|████████▋ | 18055/20708 [03:49<00:36, 72.06graph/s]Processing graphs:  87%|████████▋ | 18063/20708 [03:49<00:36, 71.89graph/s]Processing graphs:  87%|████████▋ | 18071/20708 [03:49<00:36, 73.08graph/s]Processing graphs:  87%|████████▋ | 18079/20708 [03:49<00:35, 73.68graph/s]Processing graphs:  87%|████████▋ | 18087/20708 [03:49<00:35, 74.00graph/s]Processing graphs:  87%|████████▋ | 18095/20708 [03:49<00:35, 74.62graph/s]Processing graphs:  87%|████████▋ | 18103/20708 [03:49<00:35, 73.63graph/s]Processing graphs:  87%|████████▋ | 18111/20708 [03:49<00:35, 73.59graph/s]Processing graphs:  87%|████████▋ | 18119/20708 [03:49<00:35, 72.34graph/s]Processing graphs:  88%|████████▊ | 18127/20708 [03:49<00:34, 74.12graph/s]Processing graphs:  88%|████████▊ | 18135/20708 [03:50<00:34, 73.62graph/s]Processing graphs:  88%|████████▊ | 18143/20708 [03:50<00:34, 73.53graph/s]Processing graphs:  88%|████████▊ | 18151/20708 [03:50<00:34, 73.33graph/s]Processing graphs:  88%|████████▊ | 18159/20708 [03:50<00:34, 74.63graph/s]Processing graphs:  88%|████████▊ | 18167/20708 [03:50<00:33, 75.60graph/s]Processing graphs:  88%|████████▊ | 18175/20708 [03:50<00:34, 74.45graph/s]Processing graphs:  88%|████████▊ | 18183/20708 [03:50<00:33, 74.68graph/s]Processing graphs:  88%|████████▊ | 18191/20708 [03:50<00:33, 74.08graph/s]Processing graphs:  88%|████████▊ | 18199/20708 [03:50<00:33, 74.23graph/s]Processing graphs:  88%|████████▊ | 18207/20708 [03:51<00:33, 74.04graph/s]Processing graphs:  88%|████████▊ | 18215/20708 [03:51<00:33, 74.38graph/s]Processing graphs:  88%|████████▊ | 18223/20708 [03:51<00:32, 75.92graph/s]Processing graphs:  88%|████████▊ | 18231/20708 [03:51<00:32, 75.84graph/s]Processing graphs:  88%|████████▊ | 18239/20708 [03:51<00:33, 74.81graph/s]Processing graphs:  88%|████████▊ | 18247/20708 [03:51<00:33, 73.22graph/s]Processing graphs:  88%|████████▊ | 18255/20708 [03:51<00:33, 73.79graph/s]Processing graphs:  88%|████████▊ | 18263/20708 [03:51<00:33, 73.15graph/s]Processing graphs:  88%|████████▊ | 18271/20708 [03:51<00:33, 73.16graph/s]Processing graphs:  88%|████████▊ | 18279/20708 [03:52<00:32, 74.29graph/s]Processing graphs:  88%|████████▊ | 18288/20708 [03:52<00:31, 76.14graph/s]Processing graphs:  88%|████████▊ | 18296/20708 [03:52<00:32, 75.05graph/s]Processing graphs:  88%|████████▊ | 18304/20708 [03:52<00:31, 75.59graph/s]Processing graphs:  88%|████████▊ | 18312/20708 [03:52<00:32, 74.59graph/s]Processing graphs:  88%|████████▊ | 18320/20708 [03:52<00:32, 74.09graph/s]Processing graphs:  89%|████████▊ | 18328/20708 [03:52<00:31, 74.39graph/s]Processing graphs:  89%|████████▊ | 18336/20708 [03:52<00:31, 74.47graph/s]Processing graphs:  89%|████████▊ | 18344/20708 [03:52<00:31, 75.87graph/s]Processing graphs:  89%|████████▊ | 18352/20708 [03:52<00:30, 76.31graph/s]Processing graphs:  89%|████████▊ | 18360/20708 [03:53<00:31, 75.06graph/s]Processing graphs:  89%|████████▊ | 18368/20708 [03:53<00:31, 75.00graph/s]Processing graphs:  89%|████████▊ | 18376/20708 [03:53<00:31, 73.57graph/s]Processing graphs:  89%|████████▉ | 18384/20708 [03:53<00:31, 72.92graph/s]Processing graphs:  89%|████████▉ | 18392/20708 [03:53<00:31, 72.69graph/s]Processing graphs:  89%|████████▉ | 18400/20708 [03:53<00:31, 72.57graph/s]Processing graphs:  89%|████████▉ | 18408/20708 [03:53<00:31, 72.21graph/s]Processing graphs:  89%|████████▉ | 18416/20708 [03:53<00:31, 71.82graph/s]Processing graphs:  89%|████████▉ | 18425/20708 [03:53<00:30, 74.31graph/s]Processing graphs:  89%|████████▉ | 18434/20708 [03:54<00:29, 76.34graph/s]Processing graphs:  89%|████████▉ | 18442/20708 [03:54<00:29, 76.09graph/s]Processing graphs:  89%|████████▉ | 18450/20708 [03:54<00:30, 74.14graph/s]Processing graphs:  89%|████████▉ | 18458/20708 [03:54<00:30, 74.33graph/s]Processing graphs:  89%|████████▉ | 18466/20708 [03:54<00:30, 73.62graph/s]Processing graphs:  89%|████████▉ | 18474/20708 [03:54<00:29, 75.28graph/s]Processing graphs:  89%|████████▉ | 18482/20708 [03:54<00:29, 75.26graph/s]Processing graphs:  89%|████████▉ | 18490/20708 [03:54<00:29, 75.01graph/s]Processing graphs:  89%|████████▉ | 18498/20708 [03:54<00:29, 74.15graph/s]Processing graphs:  89%|████████▉ | 18506/20708 [03:55<00:29, 74.57graph/s]Processing graphs:  89%|████████▉ | 18514/20708 [03:55<00:29, 73.50graph/s]Processing graphs:  89%|████████▉ | 18522/20708 [03:55<00:29, 74.70graph/s]Processing graphs:  89%|████████▉ | 18530/20708 [03:55<00:29, 74.78graph/s]Processing graphs:  90%|████████▉ | 18538/20708 [03:55<00:28, 74.89graph/s]Processing graphs:  90%|████████▉ | 18546/20708 [03:55<00:29, 73.45graph/s]Processing graphs:  90%|████████▉ | 18554/20708 [03:55<00:29, 73.64graph/s]Processing graphs:  90%|████████▉ | 18562/20708 [03:55<00:29, 73.20graph/s]Processing graphs:  90%|████████▉ | 18570/20708 [03:55<00:28, 74.40graph/s]Processing graphs:  90%|████████▉ | 18578/20708 [03:56<00:28, 75.55graph/s]Processing graphs:  90%|████████▉ | 18586/20708 [03:56<00:28, 73.84graph/s]Processing graphs:  90%|████████▉ | 18594/20708 [03:56<00:28, 73.76graph/s]Processing graphs:  90%|████████▉ | 18602/20708 [03:56<00:28, 74.48graph/s]Processing graphs:  90%|████████▉ | 18610/20708 [03:56<00:28, 74.77graph/s]Processing graphs:  90%|████████▉ | 18618/20708 [03:56<00:27, 75.02graph/s]Processing graphs:  90%|████████▉ | 18626/20708 [03:56<00:28, 74.29graph/s]Processing graphs:  90%|████████▉ | 18634/20708 [03:56<00:27, 75.30graph/s]Processing graphs:  90%|█████████ | 18642/20708 [03:56<00:28, 72.84graph/s]Processing graphs:  90%|█████████ | 18650/20708 [03:57<00:27, 74.30graph/s]Processing graphs:  90%|█████████ | 18658/20708 [03:57<00:27, 74.28graph/s]Processing graphs:  90%|█████████ | 18666/20708 [03:57<00:27, 74.32graph/s]Processing graphs:  90%|█████████ | 18674/20708 [03:57<00:27, 74.45graph/s]Processing graphs:  90%|█████████ | 18682/20708 [03:57<00:26, 75.08graph/s]Processing graphs:  90%|█████████ | 18690/20708 [03:57<00:27, 74.71graph/s]Processing graphs:  90%|█████████ | 18698/20708 [03:57<00:26, 74.45graph/s]Processing graphs:  90%|█████████ | 18706/20708 [03:57<00:26, 75.27graph/s]Processing graphs:  90%|█████████ | 18714/20708 [03:57<00:26, 75.30graph/s]Processing graphs:  90%|█████████ | 18722/20708 [03:57<00:26, 75.29graph/s]Processing graphs:  90%|█████████ | 18731/20708 [03:58<00:25, 77.32graph/s]Processing graphs:  90%|█████████ | 18739/20708 [03:58<00:25, 77.10graph/s]Processing graphs:  91%|█████████ | 18748/20708 [03:58<00:25, 78.12graph/s]Processing graphs:  91%|█████████ | 18756/20708 [03:58<00:25, 76.71graph/s]Processing graphs:  91%|█████████ | 18765/20708 [03:58<00:25, 77.39graph/s]Processing graphs:  91%|█████████ | 18773/20708 [03:58<00:26, 73.59graph/s]Processing graphs:  91%|█████████ | 18781/20708 [03:58<00:26, 72.95graph/s]Processing graphs:  91%|█████████ | 18790/20708 [03:58<00:25, 76.23graph/s]Processing graphs:  91%|█████████ | 18798/20708 [03:58<00:24, 76.98graph/s]Processing graphs:  91%|█████████ | 18807/20708 [03:59<00:23, 79.53graph/s]Processing graphs:  91%|█████████ | 18815/20708 [03:59<00:24, 77.94graph/s]Processing graphs:  91%|█████████ | 18823/20708 [03:59<00:24, 77.90graph/s]Processing graphs:  91%|█████████ | 18831/20708 [03:59<00:24, 76.72graph/s]Processing graphs:  91%|█████████ | 18840/20708 [03:59<00:23, 78.50graph/s]Processing graphs:  91%|█████████ | 18849/20708 [03:59<00:23, 79.94graph/s]Processing graphs:  91%|█████████ | 18858/20708 [03:59<00:22, 81.87graph/s]Processing graphs:  91%|█████████ | 18867/20708 [03:59<00:22, 81.47graph/s]Processing graphs:  91%|█████████ | 18876/20708 [03:59<00:22, 80.61graph/s]Processing graphs:  91%|█████████ | 18885/20708 [04:00<00:22, 79.80graph/s]Processing graphs:  91%|█████████ | 18893/20708 [04:00<00:22, 79.63graph/s]Processing graphs:  91%|█████████▏| 18901/20708 [04:00<00:22, 78.68graph/s]Processing graphs:  91%|█████████▏| 18909/20708 [04:00<00:23, 78.00graph/s]Processing graphs:  91%|█████████▏| 18917/20708 [04:00<00:23, 77.13graph/s]Processing graphs:  91%|█████████▏| 18925/20708 [04:00<00:23, 76.96graph/s]Processing graphs:  91%|█████████▏| 18934/20708 [04:00<00:22, 77.81graph/s]Processing graphs:  91%|█████████▏| 18944/20708 [04:00<00:21, 81.06graph/s]Processing graphs:  92%|█████████▏| 18953/20708 [04:00<00:21, 81.10graph/s]Processing graphs:  92%|█████████▏| 18962/20708 [04:01<00:22, 79.02graph/s]Processing graphs:  92%|█████████▏| 18970/20708 [04:01<00:22, 76.92graph/s]Processing graphs:  92%|█████████▏| 18978/20708 [04:01<00:22, 76.72graph/s]Processing graphs:  92%|█████████▏| 18987/20708 [04:01<00:21, 79.52graph/s]Processing graphs:  92%|█████████▏| 18995/20708 [04:01<00:21, 78.20graph/s]Processing graphs:  92%|█████████▏| 19004/20708 [04:01<00:21, 81.13graph/s]Processing graphs:  92%|█████████▏| 19013/20708 [04:01<00:20, 82.12graph/s]Processing graphs:  92%|█████████▏| 19022/20708 [04:01<00:21, 79.67graph/s]Processing graphs:  92%|█████████▏| 19031/20708 [04:01<00:20, 81.46graph/s]Processing graphs:  92%|█████████▏| 19040/20708 [04:01<00:19, 83.46graph/s]Processing graphs:  92%|█████████▏| 19049/20708 [04:02<00:19, 83.73graph/s]Processing graphs:  92%|█████████▏| 19058/20708 [04:02<00:19, 83.09graph/s]Processing graphs:  92%|█████████▏| 19067/20708 [04:02<00:19, 82.07graph/s]Processing graphs:  92%|█████████▏| 19076/20708 [04:02<00:20, 79.55graph/s]Processing graphs:  92%|█████████▏| 19084/20708 [04:02<00:20, 79.30graph/s]Processing graphs:  92%|█████████▏| 19092/20708 [04:02<00:20, 79.00graph/s]Processing graphs:  92%|█████████▏| 19101/20708 [04:02<00:19, 80.96graph/s]Processing graphs:  92%|█████████▏| 19110/20708 [04:02<00:20, 79.47graph/s]Processing graphs:  92%|█████████▏| 19119/20708 [04:02<00:19, 81.27graph/s]Processing graphs:  92%|█████████▏| 19128/20708 [04:03<00:19, 81.37graph/s]Processing graphs:  92%|█████████▏| 19137/20708 [04:03<00:19, 79.44graph/s]Processing graphs:  92%|█████████▏| 19145/20708 [04:03<00:20, 77.66graph/s]Processing graphs:  92%|█████████▏| 19153/20708 [04:03<00:20, 76.19graph/s]Processing graphs:  93%|█████████▎| 19162/20708 [04:03<00:19, 78.20graph/s]Processing graphs:  93%|█████████▎| 19172/20708 [04:03<00:18, 82.83graph/s]Processing graphs:  93%|█████████▎| 19181/20708 [04:03<00:18, 82.94graph/s]Processing graphs:  93%|█████████▎| 19190/20708 [04:03<00:18, 82.75graph/s]Processing graphs:  93%|█████████▎| 19199/20708 [04:03<00:18, 80.54graph/s]Processing graphs:  93%|█████████▎| 19209/20708 [04:04<00:17, 84.30graph/s]Processing graphs:  93%|█████████▎| 19218/20708 [04:04<00:18, 81.30graph/s]Processing graphs:  93%|█████████▎| 19227/20708 [04:04<00:17, 82.34graph/s]Processing graphs:  93%|█████████▎| 19236/20708 [04:04<00:17, 83.06graph/s]Processing graphs:  93%|█████████▎| 19245/20708 [04:04<00:17, 84.93graph/s]Processing graphs:  93%|█████████▎| 19254/20708 [04:04<00:17, 82.35graph/s]Processing graphs:  93%|█████████▎| 19264/20708 [04:04<00:16, 85.23graph/s]Processing graphs:  93%|█████████▎| 19273/20708 [04:04<00:17, 79.76graph/s]Processing graphs:  93%|█████████▎| 19282/20708 [04:04<00:18, 77.58graph/s]Processing graphs:  93%|█████████▎| 19290/20708 [04:05<00:18, 76.51graph/s]Processing graphs:  93%|█████████▎| 19298/20708 [04:05<00:18, 76.70graph/s]Processing graphs:  93%|█████████▎| 19306/20708 [04:05<00:18, 77.58graph/s]Processing graphs:  93%|█████████▎| 19315/20708 [04:05<00:17, 78.90graph/s]Processing graphs:  93%|█████████▎| 19323/20708 [04:05<00:17, 78.38graph/s]Processing graphs:  93%|█████████▎| 19331/20708 [04:05<00:17, 78.34graph/s]Processing graphs:  93%|█████████▎| 19339/20708 [04:05<00:17, 77.81graph/s]Processing graphs:  93%|█████████▎| 19347/20708 [04:05<00:17, 77.45graph/s]Processing graphs:  93%|█████████▎| 19356/20708 [04:05<00:17, 78.68graph/s]Processing graphs:  94%|█████████▎| 19365/20708 [04:06<00:16, 79.56graph/s]Processing graphs:  94%|█████████▎| 19373/20708 [04:06<00:16, 79.19graph/s]Processing graphs:  94%|█████████▎| 19382/20708 [04:06<00:16, 79.73graph/s]Processing graphs:  94%|█████████▎| 19391/20708 [04:06<00:16, 81.82graph/s]Processing graphs:  94%|█████████▎| 19400/20708 [04:06<00:16, 81.15graph/s]Processing graphs:  94%|█████████▎| 19409/20708 [04:06<00:15, 81.93graph/s]Processing graphs:  94%|█████████▍| 19418/20708 [04:06<00:15, 83.33graph/s]Processing graphs:  94%|█████████▍| 19427/20708 [04:06<00:15, 84.10graph/s]Processing graphs:  94%|█████████▍| 19436/20708 [04:06<00:15, 82.83graph/s]Processing graphs:  94%|█████████▍| 19445/20708 [04:07<00:15, 80.84graph/s]Processing graphs:  94%|█████████▍| 19454/20708 [04:07<00:15, 78.74graph/s]Processing graphs:  94%|█████████▍| 19463/20708 [04:07<00:15, 79.76graph/s]Processing graphs:  94%|█████████▍| 19472/20708 [04:07<00:15, 80.32graph/s]Processing graphs:  94%|█████████▍| 19482/20708 [04:07<00:14, 84.51graph/s]Processing graphs:  94%|█████████▍| 19491/20708 [04:07<00:14, 82.32graph/s]Processing graphs:  94%|█████████▍| 19500/20708 [04:07<00:15, 80.16graph/s]Processing graphs:  94%|█████████▍| 19509/20708 [04:07<00:15, 79.51graph/s]Processing graphs:  94%|█████████▍| 19517/20708 [04:07<00:15, 76.79graph/s]Processing graphs:  94%|█████████▍| 19526/20708 [04:08<00:15, 77.83graph/s]Processing graphs:  94%|█████████▍| 19534/20708 [04:08<00:15, 78.11graph/s]Processing graphs:  94%|█████████▍| 19542/20708 [04:08<00:15, 76.86graph/s]Processing graphs:  94%|█████████▍| 19550/20708 [04:08<00:15, 75.46graph/s]Processing graphs:  94%|█████████▍| 19559/20708 [04:08<00:14, 76.94graph/s]Processing graphs:  94%|█████████▍| 19567/20708 [04:08<00:14, 76.37graph/s]Processing graphs:  95%|█████████▍| 19576/20708 [04:08<00:14, 79.49graph/s]Processing graphs:  95%|█████████▍| 19584/20708 [04:08<00:14, 77.41graph/s]Processing graphs:  95%|█████████▍| 19592/20708 [04:08<00:14, 77.04graph/s]Processing graphs:  95%|█████████▍| 19600/20708 [04:09<00:14, 74.89graph/s]Processing graphs:  95%|█████████▍| 19609/20708 [04:09<00:14, 76.30graph/s]Processing graphs:  95%|█████████▍| 19617/20708 [04:09<00:14, 76.80graph/s]Processing graphs:  95%|█████████▍| 19625/20708 [04:09<00:14, 74.37graph/s]Processing graphs:  95%|█████████▍| 19633/20708 [04:09<00:14, 72.68graph/s]Processing graphs:  95%|█████████▍| 19642/20708 [04:09<00:14, 75.28graph/s]Processing graphs:  95%|█████████▍| 19650/20708 [04:09<00:14, 73.69graph/s]Processing graphs:  95%|█████████▍| 19659/20708 [04:09<00:13, 77.76graph/s]Processing graphs:  95%|█████████▍| 19667/20708 [04:09<00:13, 75.80graph/s]Processing graphs:  95%|█████████▌| 19675/20708 [04:10<00:13, 75.29graph/s]Processing graphs:  95%|█████████▌| 19683/20708 [04:10<00:13, 75.44graph/s]Processing graphs:  95%|█████████▌| 19691/20708 [04:10<00:13, 76.02graph/s]Processing graphs:  95%|█████████▌| 19699/20708 [04:10<00:13, 76.94graph/s]Processing graphs:  95%|█████████▌| 19708/20708 [04:10<00:12, 78.50graph/s]Processing graphs:  95%|█████████▌| 19716/20708 [04:10<00:12, 78.11graph/s]Processing graphs:  95%|█████████▌| 19724/20708 [04:10<00:13, 75.65graph/s]Processing graphs:  95%|█████████▌| 19732/20708 [04:10<00:13, 74.35graph/s]Processing graphs:  95%|█████████▌| 19740/20708 [04:10<00:13, 73.00graph/s]Processing graphs:  95%|█████████▌| 19748/20708 [04:10<00:13, 72.09graph/s]Processing graphs:  95%|█████████▌| 19756/20708 [04:11<00:13, 72.25graph/s]Processing graphs:  95%|█████████▌| 19764/20708 [04:11<00:13, 72.38graph/s]Processing graphs:  95%|█████████▌| 19772/20708 [04:11<00:13, 70.66graph/s]Processing graphs:  96%|█████████▌| 19780/20708 [04:11<00:13, 71.17graph/s]Processing graphs:  96%|█████████▌| 19788/20708 [04:11<00:13, 69.46graph/s]Processing graphs:  96%|█████████▌| 19796/20708 [04:11<00:12, 71.91graph/s]Processing graphs:  96%|█████████▌| 19804/20708 [04:11<00:12, 71.44graph/s]Processing graphs:  96%|█████████▌| 19812/20708 [04:11<00:12, 70.87graph/s]Processing graphs:  96%|█████████▌| 19820/20708 [04:11<00:12, 73.33graph/s]Processing graphs:  96%|█████████▌| 19828/20708 [04:12<00:11, 73.93graph/s]Processing graphs:  96%|█████████▌| 19836/20708 [04:12<00:11, 74.26graph/s]Processing graphs:  96%|█████████▌| 19844/20708 [04:12<00:11, 74.43graph/s]Processing graphs:  96%|█████████▌| 19852/20708 [04:12<00:11, 74.62graph/s]Processing graphs:  96%|█████████▌| 19860/20708 [04:12<00:11, 73.66graph/s]Processing graphs:  96%|█████████▌| 19869/20708 [04:12<00:10, 76.36graph/s]Processing graphs:  96%|█████████▌| 19877/20708 [04:12<00:11, 73.43graph/s]Processing graphs:  96%|█████████▌| 19885/20708 [04:12<00:10, 75.08graph/s]Processing graphs:  96%|█████████▌| 19893/20708 [04:12<00:11, 73.09graph/s]Processing graphs:  96%|█████████▌| 19901/20708 [04:13<00:11, 71.48graph/s]Processing graphs:  96%|█████████▌| 19909/20708 [04:13<00:10, 72.66graph/s]Processing graphs:  96%|█████████▌| 19917/20708 [04:13<00:10, 72.46graph/s]Processing graphs:  96%|█████████▌| 19925/20708 [04:13<00:10, 71.85graph/s]Processing graphs:  96%|█████████▋| 19933/20708 [04:13<00:10, 70.68graph/s]Processing graphs:  96%|█████████▋| 19941/20708 [04:13<00:10, 71.30graph/s]Processing graphs:  96%|█████████▋| 19949/20708 [04:13<00:10, 71.60graph/s]Processing graphs:  96%|█████████▋| 19957/20708 [04:13<00:10, 72.44graph/s]Processing graphs:  96%|█████████▋| 19965/20708 [04:13<00:10, 71.39graph/s]Processing graphs:  96%|█████████▋| 19973/20708 [04:14<00:10, 72.88graph/s]Processing graphs:  96%|█████████▋| 19981/20708 [04:14<00:09, 74.13graph/s]Processing graphs:  97%|█████████▋| 19989/20708 [04:14<00:09, 72.49graph/s]Processing graphs:  97%|█████████▋| 19997/20708 [04:14<00:09, 71.71graph/s]Processing graphs:  97%|█████████▋| 20005/20708 [04:14<00:09, 70.84graph/s]Processing graphs:  97%|█████████▋| 20013/20708 [04:14<00:09, 71.15graph/s]Processing graphs:  97%|█████████▋| 20021/20708 [04:14<00:09, 69.39graph/s]Processing graphs:  97%|█████████▋| 20028/20708 [04:14<00:09, 68.86graph/s]Processing graphs:  97%|█████████▋| 20037/20708 [04:14<00:09, 72.26graph/s]Processing graphs:  97%|█████████▋| 20045/20708 [04:15<00:09, 71.72graph/s]Processing graphs:  97%|█████████▋| 20053/20708 [04:15<00:09, 71.51graph/s]Processing graphs:  97%|█████████▋| 20061/20708 [04:15<00:09, 71.30graph/s]Processing graphs:  97%|█████████▋| 20069/20708 [04:15<00:08, 71.57graph/s]Processing graphs:  97%|█████████▋| 20077/20708 [04:15<00:08, 70.25graph/s]Processing graphs:  97%|█████████▋| 20085/20708 [04:15<00:08, 70.63graph/s]Processing graphs:  97%|█████████▋| 20093/20708 [04:15<00:08, 69.49graph/s]Processing graphs:  97%|█████████▋| 20100/20708 [04:15<00:08, 69.44graph/s]Processing graphs:  97%|█████████▋| 20108/20708 [04:16<00:08, 71.02graph/s]Processing graphs:  97%|█████████▋| 20116/20708 [04:16<00:08, 72.76graph/s]Processing graphs:  97%|█████████▋| 20124/20708 [04:16<00:08, 71.31graph/s]Processing graphs:  97%|█████████▋| 20132/20708 [04:16<00:07, 72.01graph/s]Processing graphs:  97%|█████████▋| 20140/20708 [04:16<00:07, 72.24graph/s]Processing graphs:  97%|█████████▋| 20148/20708 [04:16<00:07, 72.95graph/s]Processing graphs:  97%|█████████▋| 20156/20708 [04:16<00:07, 72.74graph/s]Processing graphs:  97%|█████████▋| 20164/20708 [04:16<00:07, 71.99graph/s]Processing graphs:  97%|█████████▋| 20172/20708 [04:16<00:07, 70.83graph/s]Processing graphs:  97%|█████████▋| 20180/20708 [04:16<00:07, 71.83graph/s]Processing graphs:  97%|█████████▋| 20188/20708 [04:17<00:07, 72.08graph/s]Processing graphs:  98%|█████████▊| 20196/20708 [04:17<00:07, 72.97graph/s]Processing graphs:  98%|█████████▊| 20204/20708 [04:17<00:07, 70.67graph/s]Processing graphs:  98%|█████████▊| 20212/20708 [04:17<00:07, 70.28graph/s]Processing graphs:  98%|█████████▊| 20220/20708 [04:17<00:06, 72.59graph/s]Processing graphs:  98%|█████████▊| 20229/20708 [04:17<00:06, 75.15graph/s]Processing graphs:  98%|█████████▊| 20237/20708 [04:17<00:06, 74.25graph/s]Processing graphs:  98%|█████████▊| 20245/20708 [04:17<00:06, 74.13graph/s]Processing graphs:  98%|█████████▊| 20253/20708 [04:17<00:06, 75.09graph/s]Processing graphs:  98%|█████████▊| 20261/20708 [04:18<00:06, 74.30graph/s]Processing graphs:  98%|█████████▊| 20269/20708 [04:18<00:05, 73.94graph/s]Processing graphs:  98%|█████████▊| 20277/20708 [04:18<00:05, 73.60graph/s]Processing graphs:  98%|█████████▊| 20285/20708 [04:18<00:05, 70.72graph/s]Processing graphs:  98%|█████████▊| 20293/20708 [04:18<00:05, 70.44graph/s]Processing graphs:  98%|█████████▊| 20301/20708 [04:18<00:05, 72.30graph/s]Processing graphs:  98%|█████████▊| 20310/20708 [04:18<00:05, 75.98graph/s]Processing graphs:  98%|█████████▊| 20318/20708 [04:18<00:05, 75.09graph/s]Processing graphs:  98%|█████████▊| 20326/20708 [04:18<00:05, 74.36graph/s]Processing graphs:  98%|█████████▊| 20334/20708 [04:19<00:05, 73.76graph/s]Processing graphs:  98%|█████████▊| 20342/20708 [04:19<00:04, 73.52graph/s]Processing graphs:  98%|█████████▊| 20350/20708 [04:19<00:04, 72.77graph/s]Processing graphs:  98%|█████████▊| 20358/20708 [04:19<00:04, 72.78graph/s]Processing graphs:  98%|█████████▊| 20366/20708 [04:19<00:04, 70.56graph/s]Processing graphs:  98%|█████████▊| 20374/20708 [04:19<00:04, 70.76graph/s]Processing graphs:  98%|█████████▊| 20382/20708 [04:19<00:04, 72.07graph/s]Processing graphs:  98%|█████████▊| 20390/20708 [04:19<00:04, 71.77graph/s]Processing graphs:  99%|█████████▊| 20398/20708 [04:19<00:04, 71.85graph/s]Processing graphs:  99%|█████████▊| 20406/20708 [04:20<00:04, 71.20graph/s]Processing graphs:  99%|█████████▊| 20415/20708 [04:20<00:03, 74.00graph/s]Processing graphs:  99%|█████████▊| 20423/20708 [04:20<00:03, 72.37graph/s]Processing graphs:  99%|█████████▊| 20431/20708 [04:20<00:03, 73.87graph/s]Processing graphs:  99%|█████████▊| 20439/20708 [04:20<00:03, 75.56graph/s]Processing graphs:  99%|█████████▊| 20448/20708 [04:20<00:03, 76.90graph/s]Processing graphs:  99%|█████████▉| 20456/20708 [04:20<00:03, 76.37graph/s]Processing graphs:  99%|█████████▉| 20464/20708 [04:20<00:03, 75.64graph/s]Processing graphs:  99%|█████████▉| 20472/20708 [04:20<00:03, 75.55graph/s]Processing graphs:  99%|█████████▉| 20480/20708 [04:21<00:03, 73.41graph/s]Processing graphs:  99%|█████████▉| 20488/20708 [04:21<00:02, 74.48graph/s]Processing graphs:  99%|█████████▉| 20496/20708 [04:21<00:02, 74.29graph/s]Processing graphs:  99%|█████████▉| 20504/20708 [04:21<00:02, 71.24graph/s]Processing graphs:  99%|█████████▉| 20512/20708 [04:21<00:02, 70.97graph/s]Processing graphs:  99%|█████████▉| 20522/20708 [04:21<00:02, 76.63graph/s]Processing graphs:  99%|█████████▉| 20530/20708 [04:21<00:02, 77.05graph/s]Processing graphs:  99%|█████████▉| 20538/20708 [04:21<00:02, 73.52graph/s]Processing graphs:  99%|█████████▉| 20546/20708 [04:21<00:02, 74.81graph/s]Processing graphs:  99%|█████████▉| 20554/20708 [04:22<00:02, 74.35graph/s]Processing graphs:  99%|█████████▉| 20562/20708 [04:22<00:02, 72.75graph/s]Processing graphs:  99%|█████████▉| 20570/20708 [04:22<00:01, 70.52graph/s]Processing graphs:  99%|█████████▉| 20578/20708 [04:22<00:01, 71.82graph/s]Processing graphs:  99%|█████████▉| 20586/20708 [04:22<00:01, 73.19graph/s]Processing graphs:  99%|█████████▉| 20594/20708 [04:22<00:01, 74.14graph/s]Processing graphs:  99%|█████████▉| 20603/20708 [04:22<00:01, 76.76graph/s]Processing graphs: 100%|█████████▉| 20611/20708 [04:22<00:01, 76.38graph/s]Processing graphs: 100%|█████████▉| 20620/20708 [04:22<00:01, 77.03graph/s]Processing graphs: 100%|█████████▉| 20628/20708 [04:23<00:01, 74.63graph/s]Processing graphs: 100%|█████████▉| 20637/20708 [04:23<00:00, 77.35graph/s]Processing graphs: 100%|█████████▉| 20645/20708 [04:23<00:00, 75.21graph/s]Processing graphs: 100%|█████████▉| 20653/20708 [04:23<00:00, 75.66graph/s]Processing graphs: 100%|█████████▉| 20661/20708 [04:23<00:00, 75.35graph/s]Processing graphs: 100%|█████████▉| 20669/20708 [04:23<00:00, 73.00graph/s]Processing graphs: 100%|█████████▉| 20677/20708 [04:23<00:00, 72.27graph/s]Processing graphs: 100%|█████████▉| 20685/20708 [04:23<00:00, 70.16graph/s]Processing graphs: 100%|█████████▉| 20693/20708 [04:23<00:00, 71.09graph/s]Processing graphs: 100%|█████████▉| 20701/20708 [04:24<00:00, 69.64graph/s]Processing graphs: 100%|██████████| 20708/20708 [04:24<00:00, 78.38graph/s]processing graph data completed
training and testing

FOLD 0
TRAIN: 18637/20708
TEST: 2071/20708

Initialize model
GNN(
  (convs): ModuleList(
    (0): GraphSN(
      (mlp): Sequential(
        (0): Linear(in_features=1461, out_features=64, bias=True)
        (1): Dropout(p=0.25, inplace=False)
        (2): ReLU()
        (3): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Dropout(p=0.25, inplace=False)
        (6): ReLU()
        (7): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (linear): Linear(in_features=64, out_features=64, bias=True)
    )
    (1): GraphSN(
      (mlp): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Dropout(p=0.25, inplace=False)
        (2): ReLU()
        (3): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Dropout(p=0.25, inplace=False)
        (6): ReLU()
        (7): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (linear): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (instance_projector): Sequential(
    (0): Linear(in_features=1589, out_features=1589, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1589, out_features=1461, bias=True)
  )
  (cluster_projector): Sequential(
    (0): Linear(in_features=1589, out_features=1589, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1589, out_features=21, bias=True)
    (3): Softmax(dim=1)
  )
  (out_proj): Linear(in_features=1589, out_features=21, bias=True)
)
N trainable parameters: 7557400
Sim min: 0.8966659307479858 Sim max: 1.0
ClusterLoss debug: loss_val=3.689171, ne_loss=0.263366, total=3.952537
Train Epoch: 0 [128/18637 (0%)]	Instance Loss: 4.551993	Cluster Loss: 3.952537 	sec/iter: 2.6173
Sim min: 3.8717911365893087e-07 Sim max: 1.0
ClusterLoss debug: loss_val=3.608542, ne_loss=1.431213, total=5.039755
Sim min: 7.095579124261064e-10 Sim max: 1.0
ClusterLoss debug: loss_val=3.614222, ne_loss=1.143273, total=4.757496
Sim min: 5.048609777848559e-14 Sim max: 1.0
ClusterLoss debug: loss_val=3.527326, ne_loss=1.620273, total=5.147599
Sim min: 1.015867077958088e-11 Sim max: 1.0
ClusterLoss debug: loss_val=3.479190, ne_loss=2.826396, total=6.305586
Sim min: 5.144555571476306e-13 Sim max: 1.0
ClusterLoss debug: loss_val=3.632020, ne_loss=4.485739, total=8.117759
Sim min: 1.015194150966181e-08 Sim max: 1.0
ClusterLoss debug: loss_val=3.655280, ne_loss=5.435907, total=9.091187
Sim min: 0.004267025738954544 Sim max: 1.0
ClusterLoss debug: loss_val=3.817304, ne_loss=5.082076, total=8.899381
Sim min: 0.07727925479412079 Sim max: 1.0
ClusterLoss debug: loss_val=3.541515, ne_loss=3.033324, total=6.574839
Sim min: 0.0915944054722786 Sim max: 1.0
ClusterLoss debug: loss_val=3.543067, ne_loss=2.801484, total=6.344551
Sim min: 0.10381055623292923 Sim max: 1.0
ClusterLoss debug: loss_val=3.551453, ne_loss=2.227738, total=5.779191
Train Epoch: 0 [1408/18637 (4%)]	Instance Loss: 4.514848	Cluster Loss: 5.779191 	sec/iter: 1.9397
Sim min: 0.10061497986316681 Sim max: 1.0
ClusterLoss debug: loss_val=3.527570, ne_loss=0.919898, total=4.447469
Sim min: 0.09278172254562378 Sim max: 1.0
ClusterLoss debug: loss_val=3.542663, ne_loss=0.515737, total=4.058400
Sim min: 0.06284737586975098 Sim max: 1.0
ClusterLoss debug: loss_val=3.565098, ne_loss=0.509829, total=4.074927
Sim min: 0.02782556787133217 Sim max: 1.0
ClusterLoss debug: loss_val=3.607133, ne_loss=0.721514, total=4.328647
Sim min: 0.022207284346222878 Sim max: 1.0
ClusterLoss debug: loss_val=3.550425, ne_loss=0.357784, total=3.908209
Sim min: 0.09064321219921112 Sim max: 1.0
ClusterLoss debug: loss_val=3.552689, ne_loss=0.685489, total=4.238178
Sim min: 0.04839266464114189 Sim max: 1.0
ClusterLoss debug: loss_val=3.573557, ne_loss=0.840280, total=4.413837
Sim min: 0.025941086933016777 Sim max: 1.0
ClusterLoss debug: loss_val=3.536662, ne_loss=0.802801, total=4.339463
Sim min: 0.01931115612387657 Sim max: 1.0
ClusterLoss debug: loss_val=3.511074, ne_loss=0.459137, total=3.970211
Sim min: 0.06210780516266823 Sim max: 1.0
ClusterLoss debug: loss_val=3.460953, ne_loss=0.727746, total=4.188699
Train Epoch: 0 [2688/18637 (7%)]	Instance Loss: 4.193149	Cluster Loss: 4.188699 	sec/iter: 1.9192
Sim min: 0.0642637088894844 Sim max: 1.0
ClusterLoss debug: loss_val=3.464483, ne_loss=0.565416, total=4.029899
Sim min: 0.03895306587219238 Sim max: 1.0
ClusterLoss debug: loss_val=3.464805, ne_loss=0.941602, total=4.406406
Sim min: 0.09047146141529083 Sim max: 1.0
ClusterLoss debug: loss_val=3.430895, ne_loss=0.700297, total=4.131192
Sim min: 0.02416929230093956 Sim max: 1.0
ClusterLoss debug: loss_val=3.432048, ne_loss=0.756876, total=4.188925
Sim min: 0.02717924863100052 Sim max: 1.0
ClusterLoss debug: loss_val=3.409447, ne_loss=0.622974, total=4.032421
Sim min: 0.0178788173943758 Sim max: 1.0
ClusterLoss debug: loss_val=3.364518, ne_loss=0.652019, total=4.016537
Sim min: 0.048437390476465225 Sim max: 1.0
ClusterLoss debug: loss_val=3.324307, ne_loss=0.947766, total=4.272073
Sim min: 0.020817531272768974 Sim max: 1.0
ClusterLoss debug: loss_val=3.308695, ne_loss=0.479989, total=3.788684
Sim min: 0.01533504668623209 Sim max: 1.0
ClusterLoss debug: loss_val=3.301200, ne_loss=0.322217, total=3.623416
Sim min: 0.024137303233146667 Sim max: 1.0
ClusterLoss debug: loss_val=3.318527, ne_loss=0.262033, total=3.580560
Train Epoch: 0 [3968/18637 (11%)]	Instance Loss: 4.189857	Cluster Loss: 3.580560 	sec/iter: 1.9172
Sim min: 0.02478182129561901 Sim max: 1.0
ClusterLoss debug: loss_val=3.310733, ne_loss=0.161715, total=3.472448
Sim min: 0.027724426239728928 Sim max: 1.0
ClusterLoss debug: loss_val=3.246872, ne_loss=0.167497, total=3.414369
Sim min: 0.024147627875208855 Sim max: 1.0
ClusterLoss debug: loss_val=3.270256, ne_loss=0.084318, total=3.354574
Sim min: 0.028095485642552376 Sim max: 1.0
ClusterLoss debug: loss_val=3.223978, ne_loss=0.054093, total=3.278070
Sim min: 0.01672775112092495 Sim max: 1.0
ClusterLoss debug: loss_val=3.169656, ne_loss=0.055362, total=3.225018
Sim min: 0.02279282920062542 Sim max: 1.0
ClusterLoss debug: loss_val=3.198816, ne_loss=0.064453, total=3.263268
Sim min: 0.026117246598005295 Sim max: 1.0
ClusterLoss debug: loss_val=3.241487, ne_loss=0.095290, total=3.336777
Sim min: 0.021879572421312332 Sim max: 1.0
ClusterLoss debug: loss_val=3.243530, ne_loss=0.111413, total=3.354944
Sim min: 0.023377174511551857 Sim max: 1.0
ClusterLoss debug: loss_val=3.233143, ne_loss=0.046261, total=3.279404
Sim min: 0.014612005092203617 Sim max: 1.0
ClusterLoss debug: loss_val=3.206634, ne_loss=0.044024, total=3.250658
Train Epoch: 0 [5248/18637 (14%)]	Instance Loss: 4.010012	Cluster Loss: 3.250658 	sec/iter: 1.9381
Sim min: 0.019621239975094795 Sim max: 1.0
ClusterLoss debug: loss_val=3.193668, ne_loss=0.015668, total=3.209336
Sim min: 0.025992099195718765 Sim max: 1.0
ClusterLoss debug: loss_val=3.183933, ne_loss=0.014714, total=3.198647
Sim min: 0.01774097979068756 Sim max: 1.0
ClusterLoss debug: loss_val=3.265674, ne_loss=0.044726, total=3.310400
Sim min: 0.014493992552161217 Sim max: 1.0
ClusterLoss debug: loss_val=3.199370, ne_loss=0.038236, total=3.237606
Sim min: 0.015453021973371506 Sim max: 1.0
ClusterLoss debug: loss_val=3.189632, ne_loss=0.012561, total=3.202193
Sim min: 0.016478655859827995 Sim max: 1.0
ClusterLoss debug: loss_val=3.185320, ne_loss=0.014573, total=3.199893
Sim min: 0.012331216596066952 Sim max: 1.0
ClusterLoss debug: loss_val=3.242891, ne_loss=0.034848, total=3.277739
Sim min: 0.021044105291366577 Sim max: 1.0
ClusterLoss debug: loss_val=3.253123, ne_loss=0.063181, total=3.316304
Sim min: 0.012642370536923409 Sim max: 1.0
ClusterLoss debug: loss_val=3.160578, ne_loss=0.020138, total=3.180716
Sim min: 0.014915249310433865 Sim max: 1.0
ClusterLoss debug: loss_val=3.174608, ne_loss=0.011671, total=3.186279
Train Epoch: 0 [6528/18637 (17%)]	Instance Loss: 4.091464	Cluster Loss: 3.186279 	sec/iter: 1.9432
Sim min: 0.012688258662819862 Sim max: 1.0
ClusterLoss debug: loss_val=3.137393, ne_loss=0.007681, total=3.145075
Sim min: 0.025081155821681023 Sim max: 1.0
ClusterLoss debug: loss_val=3.162762, ne_loss=0.045274, total=3.208036
Sim min: 0.011222182773053646 Sim max: 1.0
ClusterLoss debug: loss_val=3.142663, ne_loss=0.070101, total=3.212765
Sim min: 0.012884389609098434 Sim max: 1.0
ClusterLoss debug: loss_val=3.117257, ne_loss=0.029540, total=3.146797
Sim min: 0.009439508430659771 Sim max: 1.0
ClusterLoss debug: loss_val=3.123020, ne_loss=0.028714, total=3.151735
Sim min: 0.010329320095479488 Sim max: 1.0
ClusterLoss debug: loss_val=3.133320, ne_loss=0.058110, total=3.191429
Sim min: 0.013480775989592075 Sim max: 1.0
ClusterLoss debug: loss_val=3.140608, ne_loss=0.036053, total=3.176661
Sim min: 0.010085906833410263 Sim max: 1.0
ClusterLoss debug: loss_val=3.123332, ne_loss=0.051502, total=3.174834
Sim min: 0.022492054849863052 Sim max: 1.0
ClusterLoss debug: loss_val=3.186249, ne_loss=0.085793, total=3.272041
Sim min: 0.009148058481514454 Sim max: 1.0
ClusterLoss debug: loss_val=3.132926, ne_loss=0.040755, total=3.173681
Train Epoch: 0 [7808/18637 (21%)]	Instance Loss: 4.322175	Cluster Loss: 3.173681 	sec/iter: 1.9447
Sim min: 0.005295710638165474 Sim max: 1.0
ClusterLoss debug: loss_val=3.187007, ne_loss=0.040893, total=3.227901
Sim min: 0.018535912036895752 Sim max: 1.0
ClusterLoss debug: loss_val=3.190166, ne_loss=0.050344, total=3.240511
Sim min: 0.012083042412996292 Sim max: 1.0
ClusterLoss debug: loss_val=3.164023, ne_loss=0.020755, total=3.184778
Sim min: 0.007418760564178228 Sim max: 1.0
ClusterLoss debug: loss_val=3.129937, ne_loss=0.027003, total=3.156940
Sim min: 0.007837883196771145 Sim max: 1.0
ClusterLoss debug: loss_val=3.118581, ne_loss=0.024698, total=3.143279
Sim min: 0.010158476419746876 Sim max: 1.0
ClusterLoss debug: loss_val=3.124171, ne_loss=0.059545, total=3.183715
Sim min: 0.014184797182679176 Sim max: 1.0
ClusterLoss debug: loss_val=3.108878, ne_loss=0.021215, total=3.130093
Sim min: 0.006666027475148439 Sim max: 1.0
ClusterLoss debug: loss_val=3.103071, ne_loss=0.049602, total=3.152674
Sim min: 0.006540015805512667 Sim max: 1.0
ClusterLoss debug: loss_val=3.093305, ne_loss=0.050069, total=3.143374
Sim min: 0.012788710184395313 Sim max: 1.0
ClusterLoss debug: loss_val=3.114457, ne_loss=0.061217, total=3.175674
Train Epoch: 0 [9088/18637 (24%)]	Instance Loss: 3.780768	Cluster Loss: 3.175674 	sec/iter: 1.9444
Sim min: 0.006373782176524401 Sim max: 1.0
ClusterLoss debug: loss_val=3.114583, ne_loss=0.065681, total=3.180264
Sim min: 0.005292528308928013 Sim max: 1.0
ClusterLoss debug: loss_val=3.059030, ne_loss=0.050133, total=3.109163
Sim min: 0.023504676297307014 Sim max: 1.0
ClusterLoss debug: loss_val=3.135615, ne_loss=0.026487, total=3.162102
Sim min: 0.008585739880800247 Sim max: 1.0
ClusterLoss debug: loss_val=3.121265, ne_loss=0.039294, total=3.160559
Sim min: 0.003504241118207574 Sim max: 1.0
ClusterLoss debug: loss_val=3.051435, ne_loss=0.056956, total=3.108391
Sim min: 0.009481916204094887 Sim max: 1.0
ClusterLoss debug: loss_val=3.070112, ne_loss=0.056155, total=3.126267
Sim min: 0.02024044655263424 Sim max: 1.0
ClusterLoss debug: loss_val=3.078296, ne_loss=0.056714, total=3.135010
Sim min: 0.006517915055155754 Sim max: 1.0
ClusterLoss debug: loss_val=3.060900, ne_loss=0.076138, total=3.137038
Sim min: 0.02918296679854393 Sim max: 1.0
ClusterLoss debug: loss_val=3.116886, ne_loss=0.042305, total=3.159190
Sim min: 0.013477661646902561 Sim max: 1.0
ClusterLoss debug: loss_val=3.102713, ne_loss=0.051937, total=3.154649
Train Epoch: 0 [10368/18637 (28%)]	Instance Loss: 3.647818	Cluster Loss: 3.154649 	sec/iter: 1.9481
Sim min: 0.006658302620053291 Sim max: 1.0
ClusterLoss debug: loss_val=3.151783, ne_loss=0.063242, total=3.215025
Sim min: 0.007740354165434837 Sim max: 1.0
ClusterLoss debug: loss_val=3.059312, ne_loss=0.067585, total=3.126897
Sim min: 0.010734187439084053 Sim max: 1.0
ClusterLoss debug: loss_val=3.083465, ne_loss=0.083447, total=3.166912
Sim min: 0.006500515155494213 Sim max: 1.0
ClusterLoss debug: loss_val=3.066553, ne_loss=0.005845, total=3.072398
Sim min: 0.004310906399041414 Sim max: 1.0
ClusterLoss debug: loss_val=3.041870, ne_loss=0.013175, total=3.055045
Sim min: 0.0051711066626012325 Sim max: 1.0
ClusterLoss debug: loss_val=3.090182, ne_loss=0.042339, total=3.132521
Sim min: 0.00828988291323185 Sim max: 1.0
ClusterLoss debug: loss_val=3.052121, ne_loss=0.016205, total=3.068326
Sim min: 0.004670483060181141 Sim max: 1.0
ClusterLoss debug: loss_val=3.049504, ne_loss=0.031795, total=3.081298
Sim min: 0.005728000309318304 Sim max: 1.0
ClusterLoss debug: loss_val=3.043485, ne_loss=0.051850, total=3.095335
Sim min: 0.010656609199941158 Sim max: 1.0
ClusterLoss debug: loss_val=3.091544, ne_loss=0.067844, total=3.159388
Train Epoch: 0 [11648/18637 (31%)]	Instance Loss: 3.835625	Cluster Loss: 3.159388 	sec/iter: 1.9672
Sim min: 0.012091283686459064 Sim max: 1.0
ClusterLoss debug: loss_val=3.066041, ne_loss=0.044974, total=3.111015
Sim min: 0.008098363876342773 Sim max: 1.0
ClusterLoss debug: loss_val=3.038225, ne_loss=0.038984, total=3.077209
Sim min: 0.0068498351611196995 Sim max: 1.0
ClusterLoss debug: loss_val=3.091655, ne_loss=0.078871, total=3.170527
Sim min: 0.005142677575349808 Sim max: 1.0
ClusterLoss debug: loss_val=3.061282, ne_loss=0.033404, total=3.094686
Sim min: 0.0053880284540355206 Sim max: 1.0
ClusterLoss debug: loss_val=3.068109, ne_loss=0.050361, total=3.118470
Sim min: 0.0041917492635548115 Sim max: 1.0
ClusterLoss debug: loss_val=3.054320, ne_loss=0.026909, total=3.081229
Sim min: 0.004653633572161198 Sim max: 1.0
ClusterLoss debug: loss_val=3.055042, ne_loss=0.009065, total=3.064107
Sim min: 0.009582940489053726 Sim max: 1.0
ClusterLoss debug: loss_val=3.060267, ne_loss=0.047762, total=3.108029
Sim min: 0.006473804824054241 Sim max: 1.0
ClusterLoss debug: loss_val=3.045202, ne_loss=0.082290, total=3.127493
Sim min: 0.004962854087352753 Sim max: 1.0
ClusterLoss debug: loss_val=3.026501, ne_loss=0.064782, total=3.091283
Train Epoch: 0 [12928/18637 (35%)]	Instance Loss: 3.430049	Cluster Loss: 3.091283 	sec/iter: 1.9738
Sim min: 0.004125825595110655 Sim max: 1.0
ClusterLoss debug: loss_val=3.053056, ne_loss=0.233871, total=3.286927
Sim min: 0.002148674102500081 Sim max: 1.0
ClusterLoss debug: loss_val=2.991787, ne_loss=0.096679, total=3.088466
Sim min: 0.008106675930321217 Sim max: 1.0
ClusterLoss debug: loss_val=3.039333, ne_loss=0.083626, total=3.122959
Sim min: 0.0031015186104923487 Sim max: 1.0
ClusterLoss debug: loss_val=3.031999, ne_loss=0.065044, total=3.097043
Sim min: 0.009141150861978531 Sim max: 1.0
ClusterLoss debug: loss_val=3.051215, ne_loss=0.064488, total=3.115703
Sim min: 0.0062757073901593685 Sim max: 1.0
ClusterLoss debug: loss_val=3.006991, ne_loss=0.051572, total=3.058563
Sim min: 0.005674089305102825 Sim max: 1.0
ClusterLoss debug: loss_val=3.028039, ne_loss=0.075467, total=3.103506
Sim min: 0.0051417043432593346 Sim max: 1.0
ClusterLoss debug: loss_val=2.998395, ne_loss=0.050131, total=3.048526
Sim min: 0.00875888206064701 Sim max: 1.0
ClusterLoss debug: loss_val=3.014461, ne_loss=0.035516, total=3.049977
Sim min: 0.0102893877774477 Sim max: 1.0
ClusterLoss debug: loss_val=3.030394, ne_loss=0.090527, total=3.120921
Train Epoch: 0 [14208/18637 (38%)]	Instance Loss: 3.877253	Cluster Loss: 3.120921 	sec/iter: 1.9721
Sim min: 0.004080028273165226 Sim max: 1.0
ClusterLoss debug: loss_val=3.011698, ne_loss=0.050585, total=3.062283
Sim min: 0.011004979722201824 Sim max: 1.0
ClusterLoss debug: loss_val=3.032006, ne_loss=0.030139, total=3.062145
Sim min: 0.005486449226737022 Sim max: 1.0
ClusterLoss debug: loss_val=3.032701, ne_loss=0.047395, total=3.080096
Sim min: 0.0088006891310215 Sim max: 1.0
ClusterLoss debug: loss_val=3.008714, ne_loss=0.037970, total=3.046685
Sim min: 0.005035559646785259 Sim max: 1.0
ClusterLoss debug: loss_val=3.031430, ne_loss=0.064926, total=3.096356
Sim min: 0.004273510072380304 Sim max: 1.0
ClusterLoss debug: loss_val=2.991947, ne_loss=0.061707, total=3.053653
Sim min: 0.00559901213273406 Sim max: 1.0
ClusterLoss debug: loss_val=3.037946, ne_loss=0.123338, total=3.161284
Sim min: 0.006782028824090958 Sim max: 1.0
ClusterLoss debug: loss_val=3.028150, ne_loss=0.089555, total=3.117704
Sim min: 0.006072998978197575 Sim max: 1.0
ClusterLoss debug: loss_val=3.024970, ne_loss=0.024823, total=3.049793
Sim min: 0.005152926314622164 Sim max: 1.0
ClusterLoss debug: loss_val=3.015127, ne_loss=0.049588, total=3.064715
Train Epoch: 0 [15488/18637 (41%)]	Instance Loss: 3.185597	Cluster Loss: 3.064715 	sec/iter: 1.9701
Sim min: 0.005685190670192242 Sim max: 1.0
ClusterLoss debug: loss_val=3.007185, ne_loss=0.020874, total=3.028060
Sim min: 0.006028435193002224 Sim max: 1.0
ClusterLoss debug: loss_val=2.989530, ne_loss=0.047316, total=3.036846
Sim min: 0.011789016425609589 Sim max: 1.0
ClusterLoss debug: loss_val=3.013769, ne_loss=0.031205, total=3.044974
Sim min: 0.00911807268857956 Sim max: 1.0
ClusterLoss debug: loss_val=3.034595, ne_loss=0.091015, total=3.125610
Sim min: 0.005469216499477625 Sim max: 1.0
ClusterLoss debug: loss_val=3.000202, ne_loss=0.034120, total=3.034322
Sim min: 0.011453894898295403 Sim max: 1.0
ClusterLoss debug: loss_val=3.032717, ne_loss=0.055271, total=3.087988
Sim min: 0.011250746436417103 Sim max: 1.0
ClusterLoss debug: loss_val=3.019757, ne_loss=0.042834, total=3.062590
Sim min: 0.004895299207419157 Sim max: 1.0
ClusterLoss debug: loss_val=3.014624, ne_loss=0.069209, total=3.083833
Sim min: 0.006435937713831663 Sim max: 1.0
ClusterLoss debug: loss_val=2.988924, ne_loss=0.073673, total=3.062597
Sim min: 0.006942202802747488 Sim max: 1.0
ClusterLoss debug: loss_val=3.004653, ne_loss=0.042704, total=3.047357
Train Epoch: 0 [16768/18637 (45%)]	Instance Loss: 3.570361	Cluster Loss: 3.047357 	sec/iter: 1.9814
Sim min: 0.006675416138023138 Sim max: 1.0
ClusterLoss debug: loss_val=2.983436, ne_loss=0.045191, total=3.028628
Sim min: 0.0062003047205507755 Sim max: 1.0
ClusterLoss debug: loss_val=2.990195, ne_loss=0.038466, total=3.028661
Sim min: 0.005172055680304766 Sim max: 1.0
ClusterLoss debug: loss_val=3.024827, ne_loss=0.068065, total=3.092891
Sim min: 0.009440786205232143 Sim max: 1.0
ClusterLoss debug: loss_val=3.030359, ne_loss=0.069294, total=3.099652
Sim min: 0.006244446150958538 Sim max: 1.0
ClusterLoss debug: loss_val=2.989066, ne_loss=0.024931, total=3.013997
Sim min: 0.010057217441499233 Sim max: 1.0
ClusterLoss debug: loss_val=3.017726, ne_loss=0.099731, total=3.117457
Sim min: 0.005497235804796219 Sim max: 1.0
ClusterLoss debug: loss_val=2.987559, ne_loss=0.055650, total=3.043209
Sim min: 0.0042026350274682045 Sim max: 1.0
ClusterLoss debug: loss_val=2.957304, ne_loss=0.108498, total=3.065802
Sim min: 0.005591924302279949 Sim max: 1.0
ClusterLoss debug: loss_val=2.964344, ne_loss=0.111704, total=3.076047
Sim min: 0.006200388073921204 Sim max: 1.0
ClusterLoss debug: loss_val=2.992897, ne_loss=0.048041, total=3.040938
Train Epoch: 0 [18048/18637 (48%)]	Instance Loss: 3.313421	Cluster Loss: 3.040938 	sec/iter: 1.9914
Sim min: 0.006673735100775957 Sim max: 1.0
ClusterLoss debug: loss_val=2.979618, ne_loss=0.033024, total=3.012642
Sim min: 0.0031316070817410946 Sim max: 1.0
ClusterLoss debug: loss_val=2.965252, ne_loss=0.113123, total=3.078375
Sim min: 0.006511352490633726 Sim max: 1.0
ClusterLoss debug: loss_val=3.013932, ne_loss=0.082567, total=3.096499
Sim min: 0.004410996567457914 Sim max: 1.0
ClusterLoss debug: loss_val=2.995759, ne_loss=0.065531, total=3.061290
Sim min: 0.0035155685618519783 Sim max: 1.0
ClusterLoss debug: loss_val=2.975252, ne_loss=0.082222, total=3.057474
Sim min: 0.005629600491374731 Sim max: 1.0
ClusterLoss debug: loss_val=2.996746, ne_loss=0.106409, total=3.103155
Sim min: 0.004888979718089104 Sim max: 1.0
ClusterLoss debug: loss_val=2.993993, ne_loss=0.157388, total=3.151381
Sim min: 0.004360975697636604 Sim max: 1.0
ClusterLoss debug: loss_val=2.967587, ne_loss=0.092591, total=3.060178
Sim min: 0.0036882199347019196 Sim max: 1.0
ClusterLoss debug: loss_val=2.964134, ne_loss=0.091758, total=3.055892
Sim min: 0.006223673466593027 Sim max: 1.0
ClusterLoss debug: loss_val=2.960227, ne_loss=0.049148, total=3.009375
Train Epoch: 0 [19328/18637 (52%)]	Instance Loss: 2.963625	Cluster Loss: 3.009375 	sec/iter: 1.9963
Sim min: 0.005083191674202681 Sim max: 1.0
ClusterLoss debug: loss_val=2.963367, ne_loss=0.059226, total=3.022593
Sim min: 0.004801610950380564 Sim max: 1.0
ClusterLoss debug: loss_val=2.971579, ne_loss=0.082160, total=3.053738
Sim min: 0.005364080425351858 Sim max: 1.0
ClusterLoss debug: loss_val=2.986892, ne_loss=0.057763, total=3.044655
Sim min: 0.00949562806636095 Sim max: 1.0
ClusterLoss debug: loss_val=2.957350, ne_loss=0.093034, total=3.050383
Sim min: 0.002561677945777774 Sim max: 1.0
ClusterLoss debug: loss_val=2.960412, ne_loss=0.040923, total=3.001335
Sim min: 0.0029859060887247324 Sim max: 1.0
ClusterLoss debug: loss_val=2.943494, ne_loss=0.054380, total=2.997874
Sim min: 0.006567583419382572 Sim max: 1.0
ClusterLoss debug: loss_val=2.976619, ne_loss=0.069323, total=3.045942
Sim min: 0.006071509327739477 Sim max: 1.0
ClusterLoss debug: loss_val=2.962665, ne_loss=0.121077, total=3.083742
Sim min: 0.008154074661433697 Sim max: 1.0
ClusterLoss debug: loss_val=2.970874, ne_loss=0.115416, total=3.086290
Sim min: 0.004726491868495941 Sim max: 1.0
ClusterLoss debug: loss_val=2.952298, ne_loss=0.060220, total=3.012517
Train Epoch: 0 [20608/18637 (55%)]	Instance Loss: 3.181883	Cluster Loss: 3.012517 	sec/iter: 1.9934
Sim min: 0.0036494554951786995 Sim max: 1.0
ClusterLoss debug: loss_val=2.940216, ne_loss=0.064360, total=3.004576
Sim min: 0.011700037866830826 Sim max: 1.0
ClusterLoss debug: loss_val=2.988058, ne_loss=0.107503, total=3.095561
Sim min: 0.00653500109910965 Sim max: 1.0
ClusterLoss debug: loss_val=2.967182, ne_loss=0.076557, total=3.043738
Sim min: 0.004343106411397457 Sim max: 1.0
ClusterLoss debug: loss_val=2.955490, ne_loss=0.233936, total=3.189426
Sim min: 0.004278757609426975 Sim max: 1.0
ClusterLoss debug: loss_val=2.940326, ne_loss=0.053610, total=2.993935
Sim min: 0.005316706374287605 Sim max: 1.0
ClusterLoss debug: loss_val=2.980794, ne_loss=0.063607, total=3.044401
Sim min: 0.0035355128347873688 Sim max: 1.0
ClusterLoss debug: loss_val=2.950930, ne_loss=0.056566, total=3.007496
Sim min: 0.00591491162776947 Sim max: 1.0
ClusterLoss debug: loss_val=2.965779, ne_loss=0.089425, total=3.055204
Sim min: 0.003255921881645918 Sim max: 1.0
ClusterLoss debug: loss_val=2.943291, ne_loss=0.047277, total=2.990568
Sim min: 0.001978031825274229 Sim max: 1.0
ClusterLoss debug: loss_val=2.961396, ne_loss=0.109377, total=3.070773
Train Epoch: 0 [21888/18637 (59%)]	Instance Loss: 3.212761	Cluster Loss: 3.070773 	sec/iter: 1.9870
Sim min: 0.0030606924556195736 Sim max: 1.0
ClusterLoss debug: loss_val=2.975207, ne_loss=0.171530, total=3.146738
Sim min: 0.004014059901237488 Sim max: 1.0
ClusterLoss debug: loss_val=2.949642, ne_loss=0.057715, total=3.007357
Sim min: 0.0030193794518709183 Sim max: 1.0
ClusterLoss debug: loss_val=2.930861, ne_loss=0.056937, total=2.987797
Sim min: 0.003945412114262581 Sim max: 1.0
ClusterLoss debug: loss_val=2.947639, ne_loss=0.099906, total=3.047546
Sim min: 0.00584568502381444 Sim max: 1.0
ClusterLoss debug: loss_val=2.946217, ne_loss=0.071564, total=3.017781
Sim min: 0.004255998879671097 Sim max: 1.0
ClusterLoss debug: loss_val=2.967050, ne_loss=0.130175, total=3.097225
Sim min: 0.005212437827140093 Sim max: 1.0
ClusterLoss debug: loss_val=2.936224, ne_loss=0.050026, total=2.986250
Sim min: 0.004019216168671846 Sim max: 1.0
ClusterLoss debug: loss_val=2.967292, ne_loss=0.052705, total=3.019997
Sim min: 0.004748236853629351 Sim max: 1.0
ClusterLoss debug: loss_val=2.972121, ne_loss=0.080013, total=3.052134
Sim min: 0.005884083919227123 Sim max: 1.0
ClusterLoss debug: loss_val=2.928979, ne_loss=0.055581, total=2.984560
Train Epoch: 0 [23168/18637 (62%)]	Instance Loss: 2.916399	Cluster Loss: 2.984560 	sec/iter: 1.9918
Sim min: 0.004318715538829565 Sim max: 1.0
ClusterLoss debug: loss_val=2.929502, ne_loss=0.079424, total=3.008925
Sim min: 0.0029016146436333656 Sim max: 1.0
ClusterLoss debug: loss_val=2.945179, ne_loss=0.104353, total=3.049532
Sim min: 0.006210157182067633 Sim max: 1.0
ClusterLoss debug: loss_val=2.948391, ne_loss=0.125082, total=3.073474
Sim min: 0.0027564901392906904 Sim max: 1.0
ClusterLoss debug: loss_val=2.930635, ne_loss=0.072466, total=3.003102
Sim min: 0.0033345920965075493 Sim max: 1.0
ClusterLoss debug: loss_val=2.924006, ne_loss=0.080831, total=3.004837
Sim min: 0.005073255859315395 Sim max: 1.0
ClusterLoss debug: loss_val=2.930074, ne_loss=0.079543, total=3.009617
Sim min: 0.0027104655746370554 Sim max: 1.0
ClusterLoss debug: loss_val=2.934239, ne_loss=0.053556, total=2.987795
Sim min: 0.00295047159306705 Sim max: 1.0
ClusterLoss debug: loss_val=2.930678, ne_loss=0.105730, total=3.036408
Sim min: 0.004333179909735918 Sim max: 1.0
ClusterLoss debug: loss_val=2.936938, ne_loss=0.092167, total=3.029105
Sim min: 0.005675022955983877 Sim max: 1.0
ClusterLoss debug: loss_val=2.922732, ne_loss=0.048302, total=2.971034
Train Epoch: 0 [24448/18637 (65%)]	Instance Loss: 2.874418	Cluster Loss: 2.971034 	sec/iter: 1.9967
Sim min: 0.007567889988422394 Sim max: 1.0
ClusterLoss debug: loss_val=2.957522, ne_loss=0.035112, total=2.992635
Sim min: 0.003959684632718563 Sim max: 1.0
ClusterLoss debug: loss_val=2.931273, ne_loss=0.068117, total=2.999390
Sim min: 0.003985183779150248 Sim max: 1.0
ClusterLoss debug: loss_val=2.946695, ne_loss=0.067223, total=3.013917
Sim min: 0.004653336014598608 Sim max: 1.0
ClusterLoss debug: loss_val=2.942580, ne_loss=0.129034, total=3.071614
Sim min: 0.003525426611304283 Sim max: 1.0
ClusterLoss debug: loss_val=2.938694, ne_loss=0.074795, total=3.013488
Sim min: 0.007801161613315344 Sim max: 1.0
ClusterLoss debug: loss_val=2.974777, ne_loss=0.098162, total=3.072939
Sim min: 0.00978658627718687 Sim max: 1.0
ClusterLoss debug: loss_val=2.944941, ne_loss=0.069349, total=3.014290
Sim min: 0.005849665962159634 Sim max: 1.0
ClusterLoss debug: loss_val=2.953587, ne_loss=0.060587, total=3.014174
Sim min: 0.004827161319553852 Sim max: 1.0
ClusterLoss debug: loss_val=2.924305, ne_loss=0.051165, total=2.975470
Sim min: 0.0022102477960288525 Sim max: 1.0
ClusterLoss debug: loss_val=2.938478, ne_loss=0.086386, total=3.024864
Train Epoch: 0 [25728/18637 (69%)]	Instance Loss: 3.277390	Cluster Loss: 3.024864 	sec/iter: 1.9971
Sim min: 0.005291146691888571 Sim max: 1.0
ClusterLoss debug: loss_val=2.956356, ne_loss=0.117271, total=3.073627
Sim min: 0.00662439689040184 Sim max: 1.0
ClusterLoss debug: loss_val=2.953769, ne_loss=0.128644, total=3.082413
Sim min: 0.00410649087280035 Sim max: 1.0
ClusterLoss debug: loss_val=2.930792, ne_loss=0.146039, total=3.076831
Sim min: 0.007964279502630234 Sim max: 1.0
ClusterLoss debug: loss_val=2.945360, ne_loss=0.050767, total=2.996128
Sim min: 0.004969813395291567 Sim max: 1.0
ClusterLoss debug: loss_val=2.920109, ne_loss=0.049216, total=2.969324
Sim min: 0.0055012390948832035 Sim max: 1.0
ClusterLoss debug: loss_val=2.902297, ne_loss=0.110676, total=3.012973
Sim min: 0.009761516004800797 Sim max: 1.0
ClusterLoss debug: loss_val=2.945812, ne_loss=0.101807, total=3.047619
Sim min: 0.006423762999475002 Sim max: 1.0
ClusterLoss debug: loss_val=2.961608, ne_loss=0.109256, total=3.070864
Sim min: 0.006843341514468193 Sim max: 1.0
ClusterLoss debug: loss_val=2.949170, ne_loss=0.088911, total=3.038080
Sim min: 0.004165453836321831 Sim max: 1.0
ClusterLoss debug: loss_val=2.935638, ne_loss=0.133946, total=3.069584
Train Epoch: 0 [27008/18637 (72%)]	Instance Loss: 3.128345	Cluster Loss: 3.069584 	sec/iter: 1.9914
Sim min: 0.008104217238724232 Sim max: 1.0
ClusterLoss debug: loss_val=2.934848, ne_loss=0.013309, total=2.948157
Sim min: 0.007633621338754892 Sim max: 1.0
ClusterLoss debug: loss_val=2.929344, ne_loss=0.083772, total=3.013116
Sim min: 0.008112550713121891 Sim max: 1.0
ClusterLoss debug: loss_val=2.943469, ne_loss=0.030394, total=2.973863
Sim min: 0.003354862332344055 Sim max: 1.0
ClusterLoss debug: loss_val=2.937296, ne_loss=0.117146, total=3.054443
Sim min: 0.010080298408865929 Sim max: 1.0
ClusterLoss debug: loss_val=2.950494, ne_loss=0.097145, total=3.047639
Sim min: 0.004413581918925047 Sim max: 1.0
ClusterLoss debug: loss_val=2.940454, ne_loss=0.102294, total=3.042747
Sim min: 0.005695371422916651 Sim max: 1.0
ClusterLoss debug: loss_val=2.912898, ne_loss=0.074743, total=2.987641
Sim min: 0.005236963741481304 Sim max: 1.0
ClusterLoss debug: loss_val=2.964700, ne_loss=0.117846, total=3.082546
Sim min: 0.007317193318158388 Sim max: 1.0
ClusterLoss debug: loss_val=2.922053, ne_loss=0.066924, total=2.988977
Sim min: 0.004740320146083832 Sim max: 1.0
ClusterLoss debug: loss_val=2.921450, ne_loss=0.057655, total=2.979105
Train Epoch: 0 [28288/18637 (76%)]	Instance Loss: 3.337030	Cluster Loss: 2.979105 	sec/iter: 1.9955
Sim min: 0.004089908208698034 Sim max: 1.0
ClusterLoss debug: loss_val=2.945225, ne_loss=0.084961, total=3.030185
Sim min: 0.006801551207900047 Sim max: 1.0
ClusterLoss debug: loss_val=2.935739, ne_loss=0.078796, total=3.014534
Sim min: 0.0037847107741981745 Sim max: 1.0
ClusterLoss debug: loss_val=2.941218, ne_loss=0.089005, total=3.030223
Sim min: 0.0019854444544762373 Sim max: 1.0
ClusterLoss debug: loss_val=2.920038, ne_loss=0.158908, total=3.078947
Sim min: 0.003913962282240391 Sim max: 1.0
ClusterLoss debug: loss_val=2.940576, ne_loss=0.039777, total=2.980352
Sim min: 0.0024979047011584044 Sim max: 1.0
ClusterLoss debug: loss_val=2.963834, ne_loss=0.093782, total=3.057616
Sim min: 0.004097873345017433 Sim max: 1.0
ClusterLoss debug: loss_val=2.928958, ne_loss=0.077803, total=3.006761
Sim min: 0.002093243645504117 Sim max: 1.0
ClusterLoss debug: loss_val=2.925135, ne_loss=0.063443, total=2.988578
Sim min: 0.007210845127701759 Sim max: 1.0
ClusterLoss debug: loss_val=2.955680, ne_loss=0.088832, total=3.044512
Sim min: 0.0047954111360013485 Sim max: 1.0
ClusterLoss debug: loss_val=2.938128, ne_loss=0.072707, total=3.010835
Train Epoch: 0 [29568/18637 (79%)]	Instance Loss: 2.780194	Cluster Loss: 3.010835 	sec/iter: 2.0000
Sim min: 0.005060204304754734 Sim max: 1.0
ClusterLoss debug: loss_val=2.921145, ne_loss=0.108199, total=3.029344
Sim min: 0.0037701260298490524 Sim max: 1.0
ClusterLoss debug: loss_val=2.911438, ne_loss=0.124077, total=3.035515
Sim min: 0.004917141981422901 Sim max: 1.0
ClusterLoss debug: loss_val=2.924967, ne_loss=0.125529, total=3.050496
Sim min: 0.00466600526124239 Sim max: 1.0
ClusterLoss debug: loss_val=2.931665, ne_loss=0.126267, total=3.057933
Sim min: 0.0066969506442546844 Sim max: 1.0
ClusterLoss debug: loss_val=2.953233, ne_loss=0.126958, total=3.080191
Sim min: 0.006938244681805372 Sim max: 1.0
ClusterLoss debug: loss_val=2.925033, ne_loss=0.107927, total=3.032961
Sim min: 0.006158347707241774 Sim max: 1.0
ClusterLoss debug: loss_val=2.940616, ne_loss=0.126181, total=3.066796
Sim min: 0.008510246872901917 Sim max: 1.0
ClusterLoss debug: loss_val=2.959457, ne_loss=0.129288, total=3.088745
Sim min: 0.004734607879072428 Sim max: 1.0
ClusterLoss debug: loss_val=2.920005, ne_loss=0.115700, total=3.035704
Sim min: 0.006458660122007132 Sim max: 1.0
ClusterLoss debug: loss_val=2.917797, ne_loss=0.049296, total=2.967093
Train Epoch: 0 [30848/18637 (83%)]	Instance Loss: 2.870207	Cluster Loss: 2.967093 	sec/iter: 1.9979
Sim min: 0.0047364612109959126 Sim max: 1.0
ClusterLoss debug: loss_val=2.922816, ne_loss=0.122478, total=3.045295
Sim min: 0.004129507578909397 Sim max: 1.0
ClusterLoss debug: loss_val=2.921827, ne_loss=0.091092, total=3.012918
Sim min: 0.003957595210522413 Sim max: 1.0
ClusterLoss debug: loss_val=2.926625, ne_loss=0.108548, total=3.035173
Sim min: 0.009028803557157516 Sim max: 1.0
ClusterLoss debug: loss_val=2.957623, ne_loss=0.102182, total=3.059806
Sim min: 0.008021481335163116 Sim max: 1.0
ClusterLoss debug: loss_val=2.920562, ne_loss=0.088552, total=3.009114
Sim min: 0.007086071651428938 Sim max: 1.0
ClusterLoss debug: loss_val=2.921255, ne_loss=0.092864, total=3.014118
Sim min: 0.006377080921083689 Sim max: 1.0
ClusterLoss debug: loss_val=2.938415, ne_loss=0.160883, total=3.099299
Sim min: 0.004070810973644257 Sim max: 1.0
ClusterLoss debug: loss_val=2.953914, ne_loss=0.058136, total=3.012051
Sim min: 0.0014025010168552399 Sim max: 1.0

ClusterLoss debug: loss_val=2.907487, ne_loss=0.096954, total=3.004441
Sim min: 0.013391673564910889 Sim max: 1.0
ClusterLoss debug: loss_val=2.931006, ne_loss=0.113925, total=3.044931
Train Epoch: 0 [32128/18637 (86%)]	Instance Loss: 3.142844	Cluster Loss: 3.044931 	sec/iter: 1.9964
Sim min: 0.0033440066035836935 Sim max: 1.0
ClusterLoss debug: loss_val=2.949021, ne_loss=0.163487, total=3.112508
Sim min: 0.003788005793467164 Sim max: 1.0
ClusterLoss debug: loss_val=2.939466, ne_loss=0.088891, total=3.028357
Sim min: 0.005862426944077015 Sim max: 1.0
ClusterLoss debug: loss_val=2.939407, ne_loss=0.072688, total=3.012095
Sim min: 0.003941973205655813 Sim max: 1.0
ClusterLoss debug: loss_val=2.955978, ne_loss=0.061188, total=3.017166
Sim min: 0.004860553424805403 Sim max: 1.0
ClusterLoss debug: loss_val=2.922378, ne_loss=0.165100, total=3.087479
Sim min: 0.00363165489397943 Sim max: 1.0
ClusterLoss debug: loss_val=2.927729, ne_loss=0.093522, total=3.021251
Sim min: 0.006730640772730112 Sim max: 1.0
ClusterLoss debug: loss_val=2.947138, ne_loss=0.069666, total=3.016804
Sim min: 0.003402595641091466 Sim max: 1.0
ClusterLoss debug: loss_val=2.918960, ne_loss=0.049659, total=2.968618
Sim min: 0.007216918747872114 Sim max: 1.0
ClusterLoss debug: loss_val=2.925541, ne_loss=0.090515, total=3.016056
Sim min: 0.005942768417298794 Sim max: 1.0
ClusterLoss debug: loss_val=2.939875, ne_loss=0.099771, total=3.039646
Train Epoch: 0 [33408/18637 (89%)]	Instance Loss: 3.370946	Cluster Loss: 3.039646 	sec/iter: 1.9983
Sim min: 0.008416438475251198 Sim max: 1.0
ClusterLoss debug: loss_val=2.926193, ne_loss=0.133181, total=3.059374
Sim min: 0.00584650132805109 Sim max: 1.0
ClusterLoss debug: loss_val=2.937218, ne_loss=0.088939, total=3.026158
Sim min: 0.004602179396897554 Sim max: 1.0
ClusterLoss debug: loss_val=2.929347, ne_loss=0.122545, total=3.051892
Sim min: 0.00797494500875473 Sim max: 1.0
ClusterLoss debug: loss_val=2.911041, ne_loss=0.116473, total=3.027514
Sim min: 0.007245154585689306 Sim max: 1.0
ClusterLoss debug: loss_val=2.962525, ne_loss=0.058834, total=3.021359
Sim min: 0.003374622669070959 Sim max: 1.0
ClusterLoss debug: loss_val=2.916313, ne_loss=0.124260, total=3.040573
Sim min: 0.0018360280664637685 Sim max: 1.0
ClusterLoss debug: loss_val=2.896913, ne_loss=0.129401, total=3.026314
Sim min: 0.0012927241623401642 Sim max: 1.0
ClusterLoss debug: loss_val=2.908389, ne_loss=0.098238, total=3.006627
Sim min: 0.0033202441409230232 Sim max: 1.0
ClusterLoss debug: loss_val=2.904268, ne_loss=0.051965, total=2.956233
Sim min: 0.0019518537446856499 Sim max: 1.0
ClusterLoss debug: loss_val=2.917342, ne_loss=0.114767, total=3.032109
Train Epoch: 0 [34688/18637 (93%)]	Instance Loss: 3.219155	Cluster Loss: 3.032109 	sec/iter: 2.0030
Sim min: 0.0031561318319290876 Sim max: 1.0
ClusterLoss debug: loss_val=2.900366, ne_loss=0.108716, total=3.009082
Sim min: 0.0035257621202617884 Sim max: 1.0
ClusterLoss debug: loss_val=2.907890, ne_loss=0.146474, total=3.054364
Sim min: 0.002181172138080001 Sim max: 1.0
ClusterLoss debug: loss_val=2.888811, ne_loss=0.130391, total=3.019202
Sim min: 0.003909081686288118 Sim max: 1.0
ClusterLoss debug: loss_val=2.894628, ne_loss=0.056170, total=2.950797
Sim min: 0.005471463780850172 Sim max: 1.0
ClusterLoss debug: loss_val=2.932206, ne_loss=0.066992, total=2.999198
Sim min: 0.004363676067441702 Sim max: 1.0
ClusterLoss debug: loss_val=2.920183, ne_loss=0.079877, total=3.000061
Sim min: 0.003955112770199776 Sim max: 1.0
ClusterLoss debug: loss_val=2.911565, ne_loss=0.114121, total=3.025686
Sim min: 0.006214205641299486 Sim max: 1.0
ClusterLoss debug: loss_val=2.914119, ne_loss=0.057342, total=2.971461
Sim min: 0.004681830294430256 Sim max: 1.0
ClusterLoss debug: loss_val=2.917588, ne_loss=0.094965, total=3.012553
Sim min: 0.0032250103540718555 Sim max: 1.0
ClusterLoss debug: loss_val=2.911554, ne_loss=0.057021, total=2.968575
Train Epoch: 0 [35968/18637 (96%)]	Instance Loss: 2.866342	Cluster Loss: 2.968575 	sec/iter: 2.0067
Sim min: 0.004014699254184961 Sim max: 1.0
ClusterLoss debug: loss_val=2.921300, ne_loss=0.162242, total=3.083542
Sim min: 0.0033087467309087515 Sim max: 1.0
ClusterLoss debug: loss_val=2.899076, ne_loss=0.092677, total=2.991753
Sim min: 0.0031347235199064016 Sim max: 1.0
ClusterLoss debug: loss_val=2.907136, ne_loss=0.072747, total=2.979883
Sim min: 0.0026197233237326145 Sim max: 1.0
ClusterLoss debug: loss_val=2.902611, ne_loss=0.069676, total=2.972287
Sim min: 0.005863288417458534 Sim max: 1.0
ClusterLoss debug: loss_val=2.938167, ne_loss=0.127001, total=3.065167
Sim min: 0.002201777184382081 Sim max: 1.0
ClusterLoss debug: loss_val=2.929758, ne_loss=0.103688, total=3.033446
Sim min: 0.0034141624346375465 Sim max: 1.0
ClusterLoss debug: loss_val=2.908185, ne_loss=0.055954, total=2.964140
Sim min: 0.0032866287510842085 Sim max: 1.0
ClusterLoss debug: loss_val=2.937581, ne_loss=0.088947, total=3.026528
Sim min: 0.0025876362342387438 Sim max: 1.0
ClusterLoss debug: loss_val=2.919181, ne_loss=0.091107, total=3.010289
Sim min: 0.0030632098205387592 Sim max: 1.0
ClusterLoss debug: loss_val=2.943695, ne_loss=0.132965, total=3.076660
Train Epoch: 0 [37248/18637 (100%)]	Instance Loss: 2.723172	Cluster Loss: 3.076660 	sec/iter: 2.0074
Sim min: 0.006311435718089342 Sim max: 1.0
ClusterLoss debug: loss_val=3.049982, ne_loss=0.370251, total=3.420233
Train Epoch: 0 [37274/18637 (100%)]	Instance Loss: 0.895957	Cluster Loss: 3.420233 	sec/iter: 2.0021
Model saved to model_epoch_0.pth
请输入数字1以继续运行: 
===== 分类评估 =====
Epoch 0 - Loss: 3.1034, Acc: 4.49%
Recall:     0.0800 0.0000 0.0000 0.0500 0.0000 0.0100 0.0000 0.0000 0.1400 0.2400 0.0000 0.0000 0.0000 0.0000 0.0000 0.0141 0.3900 0.0100 0.0000 0.0000 0.0000
Precision:  0.0370 0.0000 0.0000 0.0150 0.0000 0.0526 0.0000 0.0000 0.0593 0.2105 0.0000 0.0000 0.0000 0.0000 0.0000 0.0065 0.1105 0.0078 0.0000 0.0000 0.0000
F1-score:   0.0506 0.0000 0.0000 0.0230 0.0000 0.0168 0.0000 0.0000 0.0833 0.2243 0.0000 0.0000 0.0000 0.0000 0.0000 0.0089 0.1722 0.0087 0.0000 0.0000 0.0000

===== 聚类评估 =====
混淆矩阵:
 [[ 2  0  0  1  0  0  0  0  2  0  0  2  0  0  0  0 90  0  0  3  0]
 [ 9  0  0  3  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0 71]
 [ 2  0  0  2  0  0  0  0  1  0  0  0  0  0  4  0 91  0  0  0  0]
 [17  0  0  5  0  0  0  0  2  0  0  0  0  0 72  2  2  0  0  0  0]
 [ 4  0  0  5 43  0  0  0 32  0  0  0  1  0  0  0 13  0  0  2  0]
 [83  0  0  4  0  0  0  0  1  0  0  1  0  0  4  0  4  0  0  2  1]
 [ 0 20 58  3  0  0  0  0  0 17  0  0  0  0  1  0  0  0  1  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0 90  0  0  0  0  0  0  0  0]
 [ 3  0 13 71  0  0  0  0  0  0  2  0  0  1  2  0  7  0  0  1  0]
 [ 0  0  2 33  0  0  0  0  0  0  0  0  0  0  0  0 21 43  0  1  0]
 [ 1  0  1  0  0  0  1  0  0  0  0  1  0  0  0  0 95  0  0  1  0]
 [ 2  0 50 10  0 27  0  0  0  0  0  0  0  0 10  0  1  0  0  0  0]
 [14  0  0  1  0  0  0  0  0  0  0  0  0 85  0  0  0  0  0  0  0]
 [34  0  2  2  0  0 47  0  0  0  0  5  0  0  3  0  3  0  0  4  0]
 [11  0  0  0  0  0  2  0  0  0  0 84  0  0  0  0  3  0  0  0  0]
 [ 1  0 12 36  0  0  0  0  2  0  0  0  0  0  3  0 17  0  0  0  0]
 [ 2  0  0  1  0  0  0  0 92  0  0  1  0  0  1  0  3  0  0  0  0]
 [ 2  0  0  1  0  0  0  0  1  0  1  0  0  0  0  0  3  0  0 92  0]
 [ 0  0  0  1 58  0  0  0 36  0  0  0  0  0  1  0  3  0  0  1  0]
 [ 5  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0 91  0]
 [16  0  0  1  0  0  0  0  1  0 80  0  0  0  1  0  0  0  0  1  0]]
代价矩阵:
 [[206. 199. 206. 191. 204. 125. 208. 208. 205. 208. 207. 206. 194. 174.
  197. 207. 206. 206. 208. 203. 192.]
 [ 20.  20.  20.  20.  20.  20.   0.  20.  20.  20.  20.  20.  20.  20.
   20.  20.  20.  20.  20.  20.  20.]
 [138. 138. 138. 138. 138. 138.  80. 138. 125. 136. 137.  88. 138. 136.
  138. 126. 138. 138. 138. 138. 138.]
 [180. 178. 179. 176. 176. 177. 178. 181. 110. 148. 181. 171. 180. 179.
  181. 145. 180. 180. 180. 180. 180.]
 [101. 101. 101. 101.  58. 101. 101. 101. 101. 101. 101. 101. 101. 101.
  101. 101. 101. 101.  43. 101. 101.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 50.  50.  50.  50.  50.  50.  50.  50.  50.  50.  49.  50.  50.   3.
   48.  50.  50.  50.  50.  50.  50.]
 [ 10.  10.  10.  10.  10.  10.  10.   0.  10.  10.  10.  10.  10.  10.
   10.  10.  10.  10.  10.  10.  10.]
 [168. 170. 169. 168. 138. 169. 170. 170. 170. 170. 170. 170. 170. 170.
  170. 168.  78. 169. 134. 170. 169.]
 [ 17.  17.  17.  17.  17.  17.   0.  17.  17.  17.  17.  17.  17.  17.
   17.  17.  17.  17.  17.  17.  17.]
 [100.  83. 100. 100. 100. 100. 100. 100.  98. 100. 100. 100. 100. 100.
  100. 100. 100.  99. 100. 100.  20.]
 [ 92.  94.  94.  94.  94.  93.  94.  94.  94.  94.  93.  94.  94.  89.
   10.  94.  93.  94.  94.  94.  94.]
 [ 91.  91.  91.  91.  90.  91.  91.   1.  91.  91.  91.  91.  91.  91.
   91.  91.  91.  91.  91.  91.  91.]
 [ 86.  86.  86.  86.  86.  86.  86.  86.  85.  86.  86.  86.   1.  86.
   86.  86.  86.  86.  86.  86.  86.]
 [102. 102.  98.  30. 102.  98. 101. 102. 100. 102. 102.  92. 102.  99.
  102.  99. 101. 102. 101. 102. 101.]
 [  2.   2.   2.   0.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [269. 359. 268. 357. 346. 355. 359. 359. 352. 338. 264. 358. 359. 356.
  356. 342. 356. 356. 356. 356. 359.]
 [ 43.  43.  43.  43.  43.  43.  43.  43.  43.   0.  43.  43.  43.  43.
   43.  43.  43.  43.  43.  43.  43.]
 [  1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.]
 [196. 199. 199. 199. 197. 197. 199. 199. 198. 198. 198. 199. 199. 195.
  199. 199. 199. 107. 198. 108. 198.]
 [ 72.   1.  72.  72.  72.  71.  72.  72.  72.  72.  72.  72.  72.  72.
   72.  72.  72.  72.  72.  72.  72.]]
最佳映射关系: [ 5. 19.  6.  8. 18. 11. 13.  2. 16.  0. 20. 14.  7. 12.  3.  4. 10.  9.
 15. 17.  1.]
聚类分配的最大索引: 20

=== 实例级嵌入 ===
NMI: 0.6624  ARI: 0.4117
F-score: 0.4650  Silhouette: 0.3283
Adjusted Acc: 0.5543
混淆矩阵:
 [[ 0  6 31  0  0  0  5  3  0 10  0  0  0  0  0  1 23  0  0 19  2]
 [ 0  3  2  1  0  0  2  0 38  0 27  0  1  0 15  0  0  4  0  7  0]
 [ 3  1 31  6  0  1  0  0  0  7  0  0  3  0  2  0  1 15  0 16 14]
 [ 0  1  3 46  0 15  0  0  1  0  0  2 27  0  0  0  0  2  0  3  0]
 [34  3 13  1  0  0 21  0  0  0  0  0  2  0  0  0 11  3  0  4  8]
 [ 0 12  9  4  3  1  8 11  0  6  7  0  1  0  2  4 13  4  0  8  7]
 [ 0  0  0 39  0 23  0  0  0  0  0 38  0  0  0  0  0  0  0  0  0]
 [ 0  0  0 41  0  1  0  0  0  0  0 11 47  0  0  0  0  0  0  0  0]
 [ 0 11 21  1 12 18 11  0  2  3  0  0  1  0  0  8  4  5  0  2  1]
 [ 0  0 23  1 31  0  0  0  0  0  0  0  9  0  0 35  0  1  0  0  0]
 [ 0  2  9  1  9  1  0  1  0  0  1  0  0  0  0 66  1  1  0  8  0]
 [ 0  1  0 14  0 26  0  0  0  0  0  0 31 27  0  0  0  0  0  1  0]
 [ 0 17  0 56  0  0  7  6  4  0  0  0  1  0  9  0  0  0  0  0  0]
 [ 0  9  4  1  0  3  4 51  0  0  0  0  3  0  0  0  3  0 15  4  3]
 [ 0 14  3  5  0  0  1 49  0  7  0  0  3  0  4  0  6  2  0  3  3]
 [ 0  3 18  3  0 12  0  0  0  0  0  0 21  0  0  0  0  3  0  0 11]
 [ 1  7 14  2  0  0  2  1  0  0  0  0  0  0  1  0  0 62  0 10  0]
 [ 0 20 10  1  5  0  7  1  0 19  0  0  0  0  6 19  3  3  0  4  2]
 [57  1  3  1  0  0  0  0  0 32  0  0  0  0  0  0  0  0  0  2  4]
 [ 0 12  7  0  0  3 30  0  0  2  0  0  3  0  0  1  7  6  0  2 27]
 [ 0  2  0  1  0  0  0 26 31  0  0  0  1  0  7  0  0 24  8  0  0]]
代价矩阵:
 [[ 95.  95.  92.  95.  61.  95.  95.  95.  95.  95.  95.  95.  95.  95.
   95.  95.  94.  95.  38.  95.  95.]
 [119. 122. 124. 124. 122. 113. 125. 125. 114. 125. 123. 124. 108. 116.
  111. 122. 118. 105. 124. 113. 123.]
 [170. 199. 170. 198. 188. 192. 201. 201. 180. 178. 192. 201. 201. 197.
  198. 183. 187. 191. 198. 194. 201.]
 [225. 224. 219. 179. 224. 221. 186. 184. 224. 224. 224. 211. 169. 224.
  220. 222. 223. 224. 224. 225. 224.]
 [ 60.  60.  60.  60.  60.  57.  60.  60.  48.  29.  51.  60.  60.  60.
   60.  60.  60.  55.  60.  60.  60.]
 [104. 104. 103.  89. 104. 103.  81. 103.  86. 104. 103.  78. 104. 101.
  104.  92. 104. 104. 104. 101. 104.]
 [ 93.  96.  98.  98.  77.  90.  98.  98.  87.  98.  98.  98.  91.  94.
   97.  98.  96.  91.  98.  68.  98.]
 [146. 149. 149. 149. 149. 138. 149. 149. 149. 149. 148. 149. 143.  98.
  100. 149. 148. 148. 149. 149. 123.]
 [ 76.  38.  76.  75.  76.  76.  76.  76.  74.  76.  76.  76.  72.  76.
   76.  76.  76.  76.  76.  76.  45.]
 [ 76.  86.  79.  86.  86.  80.  86.  86.  83.  86.  86.  86.  86.  86.
   79.  86.  86.  67.  54.  84.  86.]
 [ 35.   8.  35.  35.  35.  28.  35.  35.  35.  35.  34.  35.  35.  35.
   35.  35.  35.  35.  35.  35.  35.]
 [ 51.  51.  51.  49.  51.  51.  13.  40.  51.  51.  51.  51.  51.  51.
   51.  51.  51.  51.  51.  51.  51.]
 [154. 153. 151. 127. 152. 153. 154. 107. 153. 145. 154. 123. 153. 151.
  151. 133. 154. 154. 154. 151. 153.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 46.  31.  44.  46.  46.  44.  46.  46.  46.  46.  46.  46.  37.  46.
   42.  46.  45.  40.  46.  46.  39.]
 [133. 134. 134. 134. 134. 130. 134. 134. 126.  99.  68. 134. 134. 134.
  134. 134. 134. 115. 134. 133. 134.]
 [ 49.  72.  71.  72.  61.  59.  72.  72.  68.  72.  71.  72.  72.  69.
   66.  72.  72.  69.  72.  65.  72.]
 [135. 131. 120. 133. 132. 131. 135. 135. 130. 134. 134. 135. 135. 135.
  133. 132.  73. 132. 135. 129. 111.]
 [ 23.  23.  23.  23.  23.  23.  23.  23.  23.  23.  23.  23.  23.   8.
   23.  23.  23.  23.  23.  23.  15.]
 [ 74.  86.  77.  90.  89.  85.  93.  93.  91.  93.  85.  92.  93.  89.
   90.  93.  83.  89.  91.  91.  93.]
 [ 80.  82.  68.  82.  74.  75.  82.  82.  81.  82.  82.  82.  82.  79.
   79.  71.  82.  80.  78.  55.  82.]]
最佳映射关系: [18.  8.  2. 12.  9.  3.  4. 14. 20. 17.  1.  6.  7. 11. 15. 10.  5. 16.
 13.  0. 19.]
聚类分配的最大索引: 20

=== 簇级嵌入 ===
NMI: 0.4244  ARI: 0.1871
F-score: 0.2322  Silhouette: 0.5284
Adjusted Acc: 0.3197

总耗时: 32.07s
分类准确率: 4.49%
簇级嵌入NMI: 0.4244
请输入数字2以继续运行: Sim min: 0.0039040197152644396 Sim max: 1.0
ClusterLoss debug: loss_val=2.949911, ne_loss=0.069185, total=3.019095
Train Epoch: 1 [128/18637 (0%)]	Instance Loss: 2.856122	Cluster Loss: 3.019095 	sec/iter: 0.5865
Sim min: 0.004402406979352236 Sim max: 1.0
ClusterLoss debug: loss_val=2.910524, ne_loss=0.142866, total=3.053390
Sim min: 0.006529644131660461 Sim max: 1.0
ClusterLoss debug: loss_val=2.923470, ne_loss=0.118990, total=3.042460
Sim min: 0.005051987711340189 Sim max: 1.0
ClusterLoss debug: loss_val=2.933450, ne_loss=0.085483, total=3.018933
Sim min: 0.005569331347942352 Sim max: 1.0
ClusterLoss debug: loss_val=2.932733, ne_loss=0.126689, total=3.059422
Sim min: 0.007692839950323105 Sim max: 1.0
ClusterLoss debug: loss_val=2.914099, ne_loss=0.105677, total=3.019776
Sim min: 0.006156993564218283 Sim max: 1.0
ClusterLoss debug: loss_val=2.923840, ne_loss=0.086346, total=3.010186
Sim min: 0.006473025307059288 Sim max: 1.0
ClusterLoss debug: loss_val=2.930842, ne_loss=0.048292, total=2.979134
Sim min: 0.006647355854511261 Sim max: 1.0
ClusterLoss debug: loss_val=2.919411, ne_loss=0.068969, total=2.988380
Sim min: 0.005772098433226347 Sim max: 1.0
ClusterLoss debug: loss_val=2.922175, ne_loss=0.178092, total=3.100267
Sim min: 0.006635598838329315 Sim max: 1.0
ClusterLoss debug: loss_val=2.923892, ne_loss=0.108616, total=3.032508
Train Epoch: 1 [1408/18637 (4%)]	Instance Loss: 2.316234	Cluster Loss: 3.032508 	sec/iter: 0.6205
Sim min: 0.0038664855528622866 Sim max: 1.0
ClusterLoss debug: loss_val=2.933307, ne_loss=0.056217, total=2.989524
Sim min: 0.005025792401283979 Sim max: 1.0
ClusterLoss debug: loss_val=2.905742, ne_loss=0.094990, total=3.000732
Sim min: 0.005872044712305069 Sim max: 1.0
ClusterLoss debug: loss_val=2.915226, ne_loss=0.085891, total=3.001117
Sim min: 0.005407091695815325 Sim max: 1.0
ClusterLoss debug: loss_val=2.900806, ne_loss=0.108328, total=3.009134
Sim min: 0.004758752882480621 Sim max: 1.0
ClusterLoss debug: loss_val=2.899063, ne_loss=0.114986, total=3.014050
Sim min: 0.007958212867379189 Sim max: 1.0
ClusterLoss debug: loss_val=2.907761, ne_loss=0.071770, total=2.979531
Sim min: 0.003902272554114461 Sim max: 1.0
ClusterLoss debug: loss_val=2.907479, ne_loss=0.138047, total=3.045526
Sim min: 0.0009746282012201846 Sim max: 1.0
ClusterLoss debug: loss_val=2.905322, ne_loss=0.132613, total=3.037935
Sim min: 0.002125362865626812 Sim max: 1.0
ClusterLoss debug: loss_val=2.928481, ne_loss=0.104459, total=3.032940
Sim min: 0.004325902089476585 Sim max: 1.0
ClusterLoss debug: loss_val=2.937198, ne_loss=0.094664, total=3.031863
Train Epoch: 1 [2688/18637 (7%)]	Instance Loss: 2.798067	Cluster Loss: 3.031863 	sec/iter: 0.6115
Sim min: 0.007031686138361692 Sim max: 1.0
ClusterLoss debug: loss_val=2.937697, ne_loss=0.201189, total=3.138885
Sim min: 0.0050216862000525 Sim max: 1.0
ClusterLoss debug: loss_val=2.937869, ne_loss=0.074870, total=3.012738
Sim min: 0.0022675665095448494 Sim max: 1.0
ClusterLoss debug: loss_val=2.918161, ne_loss=0.083551, total=3.001712
Sim min: 0.003020158503204584 Sim max: 1.0
ClusterLoss debug: loss_val=2.896385, ne_loss=0.046030, total=2.942414
Sim min: 0.0034843157045543194 Sim max: 1.0
ClusterLoss debug: loss_val=2.897307, ne_loss=0.127034, total=3.024342
Sim min: 0.004891580902040005 Sim max: 1.0
ClusterLoss debug: loss_val=2.925012, ne_loss=0.048858, total=2.973870
Sim min: 0.0028872215189039707 Sim max: 1.0
ClusterLoss debug: loss_val=2.900168, ne_loss=0.106671, total=3.006839
Sim min: 0.0038868330884724855 Sim max: 1.0
ClusterLoss debug: loss_val=2.897941, ne_loss=0.040699, total=2.938640
Sim min: 0.007519769947975874 Sim max: 1.0
ClusterLoss debug: loss_val=2.932583, ne_loss=0.145300, total=3.077883
Sim min: 0.00432769488543272 Sim max: 1.0
ClusterLoss debug: loss_val=2.905218, ne_loss=0.111293, total=3.016511
Train Epoch: 1 [3968/18637 (11%)]	Instance Loss: 2.549704	Cluster Loss: 3.016511 	sec/iter: 0.6221
Sim min: 0.004280831664800644 Sim max: 1.0
ClusterLoss debug: loss_val=2.911655, ne_loss=0.109952, total=3.021607
Sim min: 0.004713432397693396 Sim max: 1.0
ClusterLoss debug: loss_val=2.904971, ne_loss=0.048630, total=2.953601
Sim min: 0.0022219433449208736 Sim max: 1.0
ClusterLoss debug: loss_val=2.924152, ne_loss=0.081811, total=3.005963
Sim min: 0.0035338099114596844 Sim max: 1.0
ClusterLoss debug: loss_val=2.894590, ne_loss=0.074968, total=2.969558
Sim min: 0.004734150134027004 Sim max: 1.0
ClusterLoss debug: loss_val=2.891418, ne_loss=0.108811, total=3.000228
Sim min: 0.0030455291271209717 Sim max: 1.0
ClusterLoss debug: loss_val=2.917463, ne_loss=0.076872, total=2.994335
Sim min: 0.0021762559190392494 Sim max: 1.0
ClusterLoss debug: loss_val=2.925600, ne_loss=0.082402, total=3.008001
Sim min: 0.0015260936925187707 Sim max: 1.0
ClusterLoss debug: loss_val=2.921624, ne_loss=0.210652, total=3.132276
Sim min: 0.0037041273899376392 Sim max: 1.0
ClusterLoss debug: loss_val=2.899495, ne_loss=0.093149, total=2.992644
Sim min: 0.001718424609862268 Sim max: 1.0
ClusterLoss debug: loss_val=2.902742, ne_loss=0.093283, total=2.996025
Train Epoch: 1 [5248/18637 (14%)]	Instance Loss: 2.593701	Cluster Loss: 2.996025 	sec/iter: 0.6622
Sim min: 0.0036225111689418554 Sim max: 1.0
ClusterLoss debug: loss_val=2.918693, ne_loss=0.092647, total=3.011340
Sim min: 0.0028919174801558256 Sim max: 1.0
ClusterLoss debug: loss_val=2.904732, ne_loss=0.088461, total=2.993193
Sim min: 0.002051382325589657 Sim max: 1.0
ClusterLoss debug: loss_val=2.900364, ne_loss=0.058218, total=2.958582
Sim min: 0.0038025323301553726 Sim max: 1.0
ClusterLoss debug: loss_val=2.912915, ne_loss=0.112934, total=3.025849
Sim min: 0.0036488757468760014 Sim max: 1.0
ClusterLoss debug: loss_val=2.933517, ne_loss=0.085336, total=3.018853
Sim min: 0.007788811810314655 Sim max: 1.0
ClusterLoss debug: loss_val=2.945610, ne_loss=0.215981, total=3.161591
Sim min: 0.004676924552768469 Sim max: 1.0
ClusterLoss debug: loss_val=2.918469, ne_loss=0.064153, total=2.982622
Sim min: 0.004102589096873999 Sim max: 1.0
ClusterLoss debug: loss_val=2.949469, ne_loss=0.152204, total=3.101673
Sim min: 0.005307616200298071 Sim max: 1.0
ClusterLoss debug: loss_val=2.907212, ne_loss=0.099133, total=3.006344
Sim min: 0.0027309854049235582 Sim max: 1.0
ClusterLoss debug: loss_val=2.919788, ne_loss=0.156234, total=3.076022
Train Epoch: 1 [6528/18637 (17%)]	Instance Loss: 2.784730	Cluster Loss: 3.076022 	sec/iter: 0.6834
Sim min: 0.008023852482438087 Sim max: 1.0
ClusterLoss debug: loss_val=2.916410, ne_loss=0.062016, total=2.978427
Sim min: 0.004818097688257694 Sim max: 1.0
ClusterLoss debug: loss_val=2.925228, ne_loss=0.073882, total=2.999110
Sim min: 0.003774086246266961 Sim max: 1.0
ClusterLoss debug: loss_val=2.912133, ne_loss=0.058029, total=2.970161
Sim min: 0.002652436261996627 Sim max: 1.0
ClusterLoss debug: loss_val=2.915514, ne_loss=0.129201, total=3.044715
Sim min: 0.00480511412024498 Sim max: 1.0
ClusterLoss debug: loss_val=2.914828, ne_loss=0.079633, total=2.994461
Sim min: 0.002421838231384754 Sim max: 1.0
ClusterLoss debug: loss_val=2.916983, ne_loss=0.120631, total=3.037613
Sim min: 0.0032088751904666424 Sim max: 1.0
ClusterLoss debug: loss_val=2.907620, ne_loss=0.123547, total=3.031167
Sim min: 0.004515507258474827 Sim max: 1.0
ClusterLoss debug: loss_val=2.917969, ne_loss=0.073820, total=2.991788
Sim min: 0.0032652877271175385 Sim max: 1.0
ClusterLoss debug: loss_val=2.902991, ne_loss=0.049151, total=2.952142
Sim min: 0.004621082916855812 Sim max: 1.0
ClusterLoss debug: loss_val=2.910849, ne_loss=0.100215, total=3.011065
Train Epoch: 1 [7808/18637 (21%)]	Instance Loss: 3.057849	Cluster Loss: 3.011065 	sec/iter: 0.7022
Sim min: 0.006509806960821152 Sim max: 1.0
ClusterLoss debug: loss_val=2.905611, ne_loss=0.075127, total=2.980738
Sim min: 0.004359010141342878 Sim max: 1.0
ClusterLoss debug: loss_val=2.908174, ne_loss=0.150961, total=3.059134
Sim min: 0.003036726266145706 Sim max: 1.0
ClusterLoss debug: loss_val=2.913601, ne_loss=0.083232, total=2.996833
Sim min: 0.0014991818461567163 Sim max: 1.0
ClusterLoss debug: loss_val=2.906869, ne_loss=0.127756, total=3.034625
Sim min: 0.0023727677762508392 Sim max: 1.0
ClusterLoss debug: loss_val=2.900303, ne_loss=0.078103, total=2.978406
Sim min: 0.002106014871969819 Sim max: 1.0
ClusterLoss debug: loss_val=2.892370, ne_loss=0.076014, total=2.968384
Sim min: 0.002785457530990243 Sim max: 1.0
ClusterLoss debug: loss_val=2.905207, ne_loss=0.094205, total=2.999412
Sim min: 0.00566116813570261 Sim max: 1.0
ClusterLoss debug: loss_val=2.895599, ne_loss=0.116207, total=3.011805
Sim min: 0.002916728612035513 Sim max: 1.0
ClusterLoss debug: loss_val=2.927180, ne_loss=0.123558, total=3.050737
Sim min: 0.002675565192475915 Sim max: 1.0
ClusterLoss debug: loss_val=2.912338, ne_loss=0.119099, total=3.031437
Train Epoch: 1 [9088/18637 (24%)]	Instance Loss: 3.088042	Cluster Loss: 3.031437 	sec/iter: 0.7118
Sim min: 0.005769262555986643 Sim max: 1.0
ClusterLoss debug: loss_val=2.901796, ne_loss=0.121543, total=3.023339
Sim min: 0.004563224036246538 Sim max: 1.0
ClusterLoss debug: loss_val=2.914071, ne_loss=0.090612, total=3.004683
Sim min: 0.0027295108884572983 Sim max: 1.0
ClusterLoss debug: loss_val=2.904100, ne_loss=0.037736, total=2.941835
Sim min: 0.0038999237585812807 Sim max: 1.0
ClusterLoss debug: loss_val=2.902250, ne_loss=0.079671, total=2.981921
Sim min: 0.001779048005118966 Sim max: 1.0
ClusterLoss debug: loss_val=2.906269, ne_loss=0.060842, total=2.967111
Sim min: 0.0016776135889813304 Sim max: 1.0
ClusterLoss debug: loss_val=2.897984, ne_loss=0.166296, total=3.064279
Sim min: 0.004960878752171993 Sim max: 1.0
ClusterLoss debug: loss_val=2.909914, ne_loss=0.093264, total=3.003178
Sim min: 0.002817369531840086 Sim max: 1.0
ClusterLoss debug: loss_val=2.897031, ne_loss=0.061032, total=2.958063
Sim min: 0.0017143212025985122 Sim max: 1.0
ClusterLoss debug: loss_val=2.904619, ne_loss=0.053527, total=2.958146
Sim min: 0.004550793208181858 Sim max: 1.0
ClusterLoss debug: loss_val=2.910358, ne_loss=0.097157, total=3.007515
Train Epoch: 1 [10368/18637 (28%)]	Instance Loss: 2.247256	Cluster Loss: 3.007515 	sec/iter: 0.7232
Sim min: 0.0043853032402694225 Sim max: 1.0
ClusterLoss debug: loss_val=2.910734, ne_loss=0.128902, total=3.039636
Sim min: 0.005116033833473921 Sim max: 1.0
ClusterLoss debug: loss_val=2.935712, ne_loss=0.070699, total=3.006411
Sim min: 0.0017168807098641992 Sim max: 1.0
ClusterLoss debug: loss_val=2.918951, ne_loss=0.190804, total=3.109755
Sim min: 0.0022977786138653755 Sim max: 1.0
ClusterLoss debug: loss_val=2.902347, ne_loss=0.097690, total=3.000037
Sim min: 0.0037879368755966425 Sim max: 1.0
ClusterLoss debug: loss_val=2.903869, ne_loss=0.063632, total=2.967502
Sim min: 0.0027721545193344355 Sim max: 1.0
ClusterLoss debug: loss_val=2.918739, ne_loss=0.124213, total=3.042951
Sim min: 0.004439044278115034 Sim max: 1.0
ClusterLoss debug: loss_val=2.921642, ne_loss=0.049877, total=2.971519
Sim min: 0.005162305198609829 Sim max: 1.0
ClusterLoss debug: loss_val=2.930616, ne_loss=0.087925, total=3.018541
Sim min: 0.004629418253898621 Sim max: 1.0
ClusterLoss debug: loss_val=2.886984, ne_loss=0.096750, total=2.983734
Sim min: 0.0014458505902439356 Sim max: 1.0
ClusterLoss debug: loss_val=2.879256, ne_loss=0.121653, total=3.000909
Train Epoch: 1 [11648/18637 (31%)]	Instance Loss: 2.605104	Cluster Loss: 3.000909 	sec/iter: 0.7278
Sim min: 0.0044405958615243435 Sim max: 1.0
ClusterLoss debug: loss_val=2.916513, ne_loss=0.111395, total=3.027908
Sim min: 0.007294825278222561 Sim max: 1.0
ClusterLoss debug: loss_val=2.918466, ne_loss=0.067690, total=2.986156
Sim min: 0.005074817221611738 Sim max: 1.0
ClusterLoss debug: loss_val=2.922995, ne_loss=0.086025, total=3.009020
Sim min: 0.006413176655769348 Sim max: 1.0
ClusterLoss debug: loss_val=2.929272, ne_loss=0.114030, total=3.043302
Sim min: 0.005825279746204615 Sim max: 1.0
ClusterLoss debug: loss_val=2.929904, ne_loss=0.113864, total=3.043769
Sim min: 0.00273960642516613 Sim max: 1.0
ClusterLoss debug: loss_val=2.914716, ne_loss=0.130339, total=3.045055
Sim min: 0.0046234107576310635 Sim max: 1.0
ClusterLoss debug: loss_val=2.914626, ne_loss=0.141625, total=3.056251
Sim min: 0.004263239912688732 Sim max: 1.0
ClusterLoss debug: loss_val=2.906076, ne_loss=0.092457, total=2.998533
Sim min: 0.003301723161712289 Sim max: 1.0
ClusterLoss debug: loss_val=2.903479, ne_loss=0.079705, total=2.983185
Sim min: 0.007236765697598457 Sim max: 1.0
ClusterLoss debug: loss_val=2.915412, ne_loss=0.094851, total=3.010264
Train Epoch: 1 [12928/18637 (35%)]	Instance Loss: 2.879336	Cluster Loss: 3.010264 	sec/iter: 0.7460
Sim min: 0.005553940776735544 Sim max: 1.0
ClusterLoss debug: loss_val=2.908534, ne_loss=0.102470, total=3.011004
Sim min: 0.003492036834359169 Sim max: 1.0
ClusterLoss debug: loss_val=2.905179, ne_loss=0.084673, total=2.989851
Sim min: 0.0018077262211591005 Sim max: 1.0
ClusterLoss debug: loss_val=2.891076, ne_loss=0.094047, total=2.985123
Sim min: 0.007595068775117397 Sim max: 1.0
ClusterLoss debug: loss_val=2.904900, ne_loss=0.140392, total=3.045291
Sim min: 0.0060392990708351135 Sim max: 1.0
ClusterLoss debug: loss_val=2.904953, ne_loss=0.080284, total=2.985236
Sim min: 0.002793310210108757 Sim max: 1.0
ClusterLoss debug: loss_val=2.926791, ne_loss=0.142013, total=3.068804
Sim min: 0.004323422443121672 Sim max: 1.0
ClusterLoss debug: loss_val=2.891082, ne_loss=0.056206, total=2.947287
Sim min: 0.0037715418729931116 Sim max: 1.0
ClusterLoss debug: loss_val=2.904980, ne_loss=0.055566, total=2.960547
Sim min: 0.0018799531972035766 Sim max: 1.0
ClusterLoss debug: loss_val=2.898438, ne_loss=0.091595, total=2.990033
Sim min: 0.00384624139405787 Sim max: 1.0
ClusterLoss debug: loss_val=2.893921, ne_loss=0.081645, total=2.975566
Train Epoch: 1 [14208/18637 (38%)]	Instance Loss: 2.628553	Cluster Loss: 2.975566 	sec/iter: 0.7614
Sim min: 0.00575460959225893 Sim max: 1.0
ClusterLoss debug: loss_val=2.916003, ne_loss=0.089678, total=3.005681
Sim min: 0.0061654881574213505 Sim max: 1.0
ClusterLoss debug: loss_val=2.914828, ne_loss=0.054792, total=2.969619
Sim min: 0.005199190229177475 Sim max: 1.0
ClusterLoss debug: loss_val=2.925870, ne_loss=0.108177, total=3.034047
Sim min: 0.002883324632421136 Sim max: 1.0
ClusterLoss debug: loss_val=2.944134, ne_loss=0.074171, total=3.018305
Sim min: 0.002608566777780652 Sim max: 1.0
ClusterLoss debug: loss_val=2.897850, ne_loss=0.074417, total=2.972267
Sim min: 0.004352929070591927 Sim max: 1.0
ClusterLoss debug: loss_val=2.908993, ne_loss=0.049428, total=2.958421
Sim min: 0.001665032235905528 Sim max: 1.0
ClusterLoss debug: loss_val=2.905136, ne_loss=0.109541, total=3.014677
Sim min: 0.0036022935528308153 Sim max: 1.0
ClusterLoss debug: loss_val=2.913107, ne_loss=0.068938, total=2.982044
Sim min: 0.0017177286790683866 Sim max: 1.0
ClusterLoss debug: loss_val=2.905054, ne_loss=0.055773, total=2.960827
Sim min: 0.0026892335154116154 Sim max: 1.0
ClusterLoss debug: loss_val=2.915282, ne_loss=0.117138, total=3.032420
Train Epoch: 1 [15488/18637 (41%)]	Instance Loss: 2.377319	Cluster Loss: 3.032420 	sec/iter: 0.7560
Sim min: 0.002315062563866377 Sim max: 1.0
ClusterLoss debug: loss_val=2.909236, ne_loss=0.131929, total=3.041165
Sim min: 0.005054751876741648 Sim max: 1.0
ClusterLoss debug: loss_val=2.919221, ne_loss=0.118474, total=3.037695
Sim min: 0.004446564707905054 Sim max: 1.0
ClusterLoss debug: loss_val=2.942088, ne_loss=0.114662, total=3.056750
Sim min: 0.004779625218361616 Sim max: 1.0
ClusterLoss debug: loss_val=2.916594, ne_loss=0.059151, total=2.975744
Sim min: 0.00316847232170403 Sim max: 1.0
ClusterLoss debug: loss_val=2.906882, ne_loss=0.066152, total=2.973034
Sim min: 0.0036168296355754137 Sim max: 1.0
ClusterLoss debug: loss_val=2.917383, ne_loss=0.096162, total=3.013545
Sim min: 0.005038667935878038 Sim max: 1.0
ClusterLoss debug: loss_val=2.926777, ne_loss=0.185865, total=3.112642
Sim min: 0.00630133505910635 Sim max: 1.0
ClusterLoss debug: loss_val=2.900767, ne_loss=0.062878, total=2.963645
Sim min: 0.004777505062520504 Sim max: 1.0
ClusterLoss debug: loss_val=2.922204, ne_loss=0.076843, total=2.999046
Sim min: 0.004712004214525223 Sim max: 1.0
ClusterLoss debug: loss_val=2.905727, ne_loss=0.091205, total=2.996932
Train Epoch: 1 [16768/18637 (45%)]	Instance Loss: 2.444979	Cluster Loss: 2.996932 	sec/iter: 0.7418
Sim min: 0.0021729622967541218 Sim max: 1.0
ClusterLoss debug: loss_val=2.913257, ne_loss=0.106165, total=3.019422
Sim min: 0.004646623507142067 Sim max: 1.0
ClusterLoss debug: loss_val=2.889351, ne_loss=0.043322, total=2.932673
Sim min: 0.0038816393353044987 Sim max: 1.0
ClusterLoss debug: loss_val=2.908578, ne_loss=0.184179, total=3.092758
Sim min: 0.003190890420228243 Sim max: 1.0
ClusterLoss debug: loss_val=2.915932, ne_loss=0.060835, total=2.976767
Sim min: 0.0028704500291496515 Sim max: 1.0
ClusterLoss debug: loss_val=2.900982, ne_loss=0.161418, total=3.062400
Sim min: 0.006794600747525692 Sim max: 1.0
ClusterLoss debug: loss_val=2.896467, ne_loss=0.070616, total=2.967084
Sim min: 0.008755666203796864 Sim max: 1.0
ClusterLoss debug: loss_val=2.926113, ne_loss=0.056136, total=2.982249
Sim min: 0.00463279290124774 Sim max: 1.0
ClusterLoss debug: loss_val=2.931411, ne_loss=0.071452, total=3.002863
Sim min: 0.0027182598132640123 Sim max: 1.0
ClusterLoss debug: loss_val=2.906514, ne_loss=0.152410, total=3.058924
Sim min: 0.005557610187679529 Sim max: 1.0
ClusterLoss debug: loss_val=2.901300, ne_loss=0.131159, total=3.032459
Train Epoch: 1 [18048/18637 (48%)]	Instance Loss: 2.654162	Cluster Loss: 3.032459 	sec/iter: 0.7321
Sim min: 0.00436428003013134 Sim max: 1.0
ClusterLoss debug: loss_val=2.938066, ne_loss=0.050816, total=2.988883
Sim min: 0.005258495919406414 Sim max: 1.0
ClusterLoss debug: loss_val=2.918684, ne_loss=0.097878, total=3.016562
Sim min: 0.004345193039625883 Sim max: 1.0
ClusterLoss debug: loss_val=2.898066, ne_loss=0.130518, total=3.028584
Sim min: 0.007429899647831917 Sim max: 1.0
ClusterLoss debug: loss_val=2.892439, ne_loss=0.054570, total=2.947009
Sim min: 0.004435381852090359 Sim max: 1.0
ClusterLoss debug: loss_val=2.892154, ne_loss=0.093142, total=2.985297
Sim min: 0.004304332192987204 Sim max: 1.0
ClusterLoss debug: loss_val=2.919808, ne_loss=0.190002, total=3.109811
Sim min: 0.001495655975304544 Sim max: 1.0
ClusterLoss debug: loss_val=2.903723, ne_loss=0.118029, total=3.021752
Sim min: 0.003220517421141267 Sim max: 1.0
ClusterLoss debug: loss_val=2.915389, ne_loss=0.073991, total=2.989381
Sim min: 0.0037414690013974905 Sim max: 1.0
ClusterLoss debug: loss_val=2.885450, ne_loss=0.118320, total=3.003770
Sim min: 0.0035478181671351194 Sim max: 1.0
ClusterLoss debug: loss_val=2.908286, ne_loss=0.030193, total=2.938479
Train Epoch: 1 [19328/18637 (52%)]	Instance Loss: 3.000717	Cluster Loss: 2.938479 	sec/iter: 0.7207
Sim min: 0.002560465829446912 Sim max: 1.0
ClusterLoss debug: loss_val=2.900084, ne_loss=0.060712, total=2.960796
Sim min: 0.002924499334767461 Sim max: 1.0
ClusterLoss debug: loss_val=2.878669, ne_loss=0.061956, total=2.940625
Sim min: 0.0027211043052375317 Sim max: 1.0
ClusterLoss debug: loss_val=2.888651, ne_loss=0.050280, total=2.938931
Sim min: 0.0023699207231402397 Sim max: 1.0
ClusterLoss debug: loss_val=2.899865, ne_loss=0.081488, total=2.981353
Sim min: 0.002963863778859377 Sim max: 1.0
ClusterLoss debug: loss_val=2.919390, ne_loss=0.154631, total=3.074021
Sim min: 0.002507721073925495 Sim max: 1.0
ClusterLoss debug: loss_val=2.879986, ne_loss=0.069168, total=2.949154
Sim min: 0.003728561569005251 Sim max: 1.0
ClusterLoss debug: loss_val=2.892443, ne_loss=0.158516, total=3.050959
Sim min: 0.002701586578041315 Sim max: 1.0
ClusterLoss debug: loss_val=2.891389, ne_loss=0.066288, total=2.957677
Sim min: 0.0029100626707077026 Sim max: 1.0
ClusterLoss debug: loss_val=2.900706, ne_loss=0.106667, total=3.007373
Sim min: 0.0032363198697566986 Sim max: 1.0
ClusterLoss debug: loss_val=2.897943, ne_loss=0.069103, total=2.967046
Train Epoch: 1 [20608/18637 (55%)]	Instance Loss: 2.562057	Cluster Loss: 2.967046 	sec/iter: 0.7129
Sim min: 0.0015949354274198413 Sim max: 1.0
ClusterLoss debug: loss_val=2.890887, ne_loss=0.075102, total=2.965989
Sim min: 0.002863258123397827 Sim max: 1.0
ClusterLoss debug: loss_val=2.931283, ne_loss=0.060924, total=2.992208
Sim min: 0.005103179253637791 Sim max: 1.0
ClusterLoss debug: loss_val=2.921531, ne_loss=0.101582, total=3.023113
Sim min: 0.003780750557780266 Sim max: 1.0
ClusterLoss debug: loss_val=2.928965, ne_loss=0.153670, total=3.082636
Sim min: 0.003076551714912057 Sim max: 1.0
ClusterLoss debug: loss_val=2.898143, ne_loss=0.062225, total=2.960368
Sim min: 0.003497056197375059 Sim max: 1.0
ClusterLoss debug: loss_val=2.894163, ne_loss=0.105985, total=3.000148
Sim min: 0.0037085700314491987 Sim max: 1.0
ClusterLoss debug: loss_val=2.885882, ne_loss=0.127017, total=3.012900
Sim min: 0.0026923136319965124 Sim max: 1.0
ClusterLoss debug: loss_val=2.904539, ne_loss=0.133649, total=3.038188
Sim min: 0.002440295647829771 Sim max: 1.0
ClusterLoss debug: loss_val=2.904140, ne_loss=0.090203, total=2.994343
Sim min: 0.005803423468023539 Sim max: 1.0
ClusterLoss debug: loss_val=2.903013, ne_loss=0.064559, total=2.967572
Train Epoch: 1 [21888/18637 (59%)]	Instance Loss: 2.419475	Cluster Loss: 2.967572 	sec/iter: 0.7052
Sim min: 0.005213701631873846 Sim max: 1.0
ClusterLoss debug: loss_val=2.898667, ne_loss=0.036079, total=2.934746
Sim min: 0.003717961721122265 Sim max: 1.0
ClusterLoss debug: loss_val=2.910950, ne_loss=0.195230, total=3.106181
Sim min: 0.0038022140506654978 Sim max: 1.0
ClusterLoss debug: loss_val=2.893085, ne_loss=0.094153, total=2.987238
Sim min: 0.003431677585467696 Sim max: 1.0
ClusterLoss debug: loss_val=2.875173, ne_loss=0.062997, total=2.938170
Sim min: 0.003310257103294134 Sim max: 1.0
ClusterLoss debug: loss_val=2.903515, ne_loss=0.106857, total=3.010372
Sim min: 0.0063627418130636215 Sim max: 1.0
ClusterLoss debug: loss_val=2.928062, ne_loss=0.096963, total=3.025025
Sim min: 0.0024240107741206884 Sim max: 1.0
ClusterLoss debug: loss_val=2.879202, ne_loss=0.083742, total=2.962944
Sim min: 0.002461714670062065 Sim max: 1.0
ClusterLoss debug: loss_val=2.886513, ne_loss=0.089582, total=2.976095
Sim min: 0.004311286378651857 Sim max: 1.0
ClusterLoss debug: loss_val=2.932199, ne_loss=0.118392, total=3.050591
Sim min: 0.0017192792147397995 Sim max: 1.0
ClusterLoss debug: loss_val=2.911178, ne_loss=0.148641, total=3.059819
Train Epoch: 1 [23168/18637 (62%)]	Instance Loss: 2.880527	Cluster Loss: 3.059819 	sec/iter: 0.6974
Sim min: 0.003008659929037094 Sim max: 1.0
ClusterLoss debug: loss_val=2.906695, ne_loss=0.040980, total=2.947675
Sim min: 0.0021011964417994022 Sim max: 1.0
ClusterLoss debug: loss_val=2.902214, ne_loss=0.133755, total=3.035969
Sim min: 0.0018058824352920055 Sim max: 1.0
ClusterLoss debug: loss_val=2.881653, ne_loss=0.071766, total=2.953419
Sim min: 0.005301395431160927 Sim max: 1.0
ClusterLoss debug: loss_val=2.904845, ne_loss=0.069233, total=2.974079
Sim min: 0.005166034679859877 Sim max: 1.0
ClusterLoss debug: loss_val=2.910745, ne_loss=0.087359, total=2.998104
Sim min: 0.003075119573622942 Sim max: 1.0
ClusterLoss debug: loss_val=2.889872, ne_loss=0.040260, total=2.930132
Sim min: 0.0022255440708249807 Sim max: 1.0
ClusterLoss debug: loss_val=2.896040, ne_loss=0.139933, total=3.035973
Sim min: 0.0024498910643160343 Sim max: 1.0
ClusterLoss debug: loss_val=2.905152, ne_loss=0.055951, total=2.961103
Sim min: 0.003142883302643895 Sim max: 1.0
ClusterLoss debug: loss_val=2.873345, ne_loss=0.154761, total=3.028106
Sim min: 0.0033775283955037594 Sim max: 1.0
ClusterLoss debug: loss_val=2.892536, ne_loss=0.092448, total=2.984984
Train Epoch: 1 [24448/18637 (65%)]	Instance Loss: 2.218422	Cluster Loss: 2.984984 	sec/iter: 0.6898
Sim min: 0.002392682246863842 Sim max: 1.0
ClusterLoss debug: loss_val=2.883096, ne_loss=0.131538, total=3.014634
Sim min: 0.003628204809501767 Sim max: 1.0
ClusterLoss debug: loss_val=2.898846, ne_loss=0.095418, total=2.994264
Sim min: 0.0026595222298055887 Sim max: 1.0
ClusterLoss debug: loss_val=2.927604, ne_loss=0.154703, total=3.082307
Sim min: 0.0038181126583367586 Sim max: 1.0
ClusterLoss debug: loss_val=2.898833, ne_loss=0.074574, total=2.973408
Sim min: 0.0038922715466469526 Sim max: 1.0
ClusterLoss debug: loss_val=2.903527, ne_loss=0.101515, total=3.005042
Sim min: 0.004819944500923157 Sim max: 1.0
ClusterLoss debug: loss_val=2.910742, ne_loss=0.104930, total=3.015671
Sim min: 0.0033460005652159452 Sim max: 1.0
ClusterLoss debug: loss_val=2.910923, ne_loss=0.215738, total=3.126661
Sim min: 0.0022252602502703667 Sim max: 1.0
ClusterLoss debug: loss_val=2.882956, ne_loss=0.044191, total=2.927147
Sim min: 0.002604384208098054 Sim max: 1.0
ClusterLoss debug: loss_val=2.902359, ne_loss=0.119845, total=3.022204
Sim min: 0.004673172254115343 Sim max: 1.0
ClusterLoss debug: loss_val=2.905730, ne_loss=0.168854, total=3.074584
Train Epoch: 1 [25728/18637 (69%)]	Instance Loss: 2.314756	Cluster Loss: 3.074584 	sec/iter: 0.6828
Sim min: 0.0027735468465834856 Sim max: 1.0
ClusterLoss debug: loss_val=2.909053, ne_loss=0.136659, total=3.045713
Sim min: 0.0024209311231970787 Sim max: 1.0
ClusterLoss debug: loss_val=2.900813, ne_loss=0.074728, total=2.975542
Sim min: 0.005085086449980736 Sim max: 1.0
ClusterLoss debug: loss_val=2.920516, ne_loss=0.105814, total=3.026331
Sim min: 0.0017945630243048072 Sim max: 1.0
ClusterLoss debug: loss_val=2.883687, ne_loss=0.094007, total=2.977695
Sim min: 0.003729610238224268 Sim max: 1.0
ClusterLoss debug: loss_val=2.912176, ne_loss=0.037205, total=2.949381
Sim min: 0.002711585257202387 Sim max: 1.0
ClusterLoss debug: loss_val=2.909147, ne_loss=0.133144, total=3.042291
Sim min: 0.001902468502521515 Sim max: 1.0
ClusterLoss debug: loss_val=2.899527, ne_loss=0.048190, total=2.947717
Sim min: 0.001831538276746869 Sim max: 1.0
ClusterLoss debug: loss_val=2.898553, ne_loss=0.059147, total=2.957700
Sim min: 0.0031106339301913977 Sim max: 1.0
ClusterLoss debug: loss_val=2.889143, ne_loss=0.130009, total=3.019152
Sim min: 0.0025268110912293196 Sim max: 1.0
ClusterLoss debug: loss_val=2.893658, ne_loss=0.153964, total=3.047621
Train Epoch: 1 [27008/18637 (72%)]	Instance Loss: 2.547536	Cluster Loss: 3.047621 	sec/iter: 0.6797
Sim min: 0.0038452502340078354 Sim max: 1.0
ClusterLoss debug: loss_val=2.888074, ne_loss=0.077405, total=2.965479
Sim min: 0.002035477664321661 Sim max: 1.0
ClusterLoss debug: loss_val=2.903477, ne_loss=0.125823, total=3.029301
Sim min: 0.002348892157897353 Sim max: 1.0
ClusterLoss debug: loss_val=2.894438, ne_loss=0.149615, total=3.044052
Sim min: 0.0050720153376460075 Sim max: 1.0
ClusterLoss debug: loss_val=2.941506, ne_loss=0.186680, total=3.128186
Sim min: 0.00367616955190897 Sim max: 1.0
ClusterLoss debug: loss_val=2.906195, ne_loss=0.101764, total=3.007959
Sim min: 0.0019157389178872108 Sim max: 1.0
ClusterLoss debug: loss_val=2.890811, ne_loss=0.084749, total=2.975560
Sim min: 0.0026149072218686342 Sim max: 1.0
ClusterLoss debug: loss_val=2.891098, ne_loss=0.103780, total=2.994879
Sim min: 0.0033683101646602154 Sim max: 1.0
ClusterLoss debug: loss_val=2.895018, ne_loss=0.033647, total=2.928665
Sim min: 0.006163672544062138 Sim max: 1.0
ClusterLoss debug: loss_val=2.917018, ne_loss=0.044231, total=2.961249
Sim min: 0.0026673590764403343 Sim max: 1.0
ClusterLoss debug: loss_val=2.907251, ne_loss=0.101649, total=3.008900
Train Epoch: 1 [28288/18637 (76%)]	Instance Loss: 2.581759	Cluster Loss: 3.008900 	sec/iter: 0.6770
Sim min: 0.0019963409285992384 Sim max: 1.0
ClusterLoss debug: loss_val=2.878252, ne_loss=0.074888, total=2.953140
Sim min: 0.0036250136326998472 Sim max: 1.0
ClusterLoss debug: loss_val=2.899882, ne_loss=0.077785, total=2.977667
Sim min: 0.0034942664206027985 Sim max: 1.0
ClusterLoss debug: loss_val=2.906351, ne_loss=0.116659, total=3.023010
Sim min: 0.004089028108865023 Sim max: 1.0
ClusterLoss debug: loss_val=2.928380, ne_loss=0.078858, total=3.007239
Sim min: 0.0027524475008249283 Sim max: 1.0
ClusterLoss debug: loss_val=2.918876, ne_loss=0.167030, total=3.085906
Sim min: 0.004078736994415522 Sim max: 1.0
ClusterLoss debug: loss_val=2.896116, ne_loss=0.208734, total=3.104850
Sim min: 0.003480043960735202 Sim max: 1.0
ClusterLoss debug: loss_val=2.904811, ne_loss=0.079075, total=2.983886
Sim min: 0.0030193179845809937 Sim max: 1.0
ClusterLoss debug: loss_val=2.878821, ne_loss=0.031856, total=2.910677
Sim min: 0.002004241105169058 Sim max: 1.0
ClusterLoss debug: loss_val=2.894184, ne_loss=0.026530, total=2.920713
Sim min: 0.0031871520914137363 Sim max: 1.0
ClusterLoss debug: loss_val=2.912877, ne_loss=0.102292, total=3.015169
Train Epoch: 1 [29568/18637 (79%)]	Instance Loss: 2.696650	Cluster Loss: 3.015169 	sec/iter: 0.6742
Sim min: 0.0036771020386368036 Sim max: 1.0
ClusterLoss debug: loss_val=2.900954, ne_loss=0.120103, total=3.021057
Sim min: 0.0028044970240443945 Sim max: 1.0
ClusterLoss debug: loss_val=2.928349, ne_loss=0.113702, total=3.042051
Sim min: 0.0030590295791625977 Sim max: 1.0
ClusterLoss debug: loss_val=2.938296, ne_loss=0.155521, total=3.093817
Sim min: 0.001965222181752324 Sim max: 1.0
ClusterLoss debug: loss_val=2.904435, ne_loss=0.109431, total=3.013866
Sim min: 0.001792755676433444 Sim max: 1.0
ClusterLoss debug: loss_val=2.888082, ne_loss=0.118352, total=3.006435
Sim min: 0.0017440615920349956 Sim max: 1.0
ClusterLoss debug: loss_val=2.920151, ne_loss=0.085654, total=3.005805
Sim min: 0.00346402102150023 Sim max: 1.0
ClusterLoss debug: loss_val=2.909700, ne_loss=0.129909, total=3.039608
Sim min: 0.002848957432433963 Sim max: 1.0
ClusterLoss debug: loss_val=2.895138, ne_loss=0.077739, total=2.972877
Sim min: 0.002562639769166708 Sim max: 1.0
ClusterLoss debug: loss_val=2.916111, ne_loss=0.090988, total=3.007099
Sim min: 0.002350045833736658 Sim max: 1.0
ClusterLoss debug: loss_val=2.909206, ne_loss=0.071316, total=2.980522
Train Epoch: 1 [30848/18637 (83%)]	Instance Loss: 2.395151	Cluster Loss: 2.980522 	sec/iter: 0.6721
Sim min: 0.0034400124568492174 Sim max: 1.0
ClusterLoss debug: loss_val=2.901865, ne_loss=0.100729, total=3.002594
Sim min: 0.00648927316069603 Sim max: 1.0
ClusterLoss debug: loss_val=2.903881, ne_loss=0.119498, total=3.023378
Sim min: 0.0022607266437262297 Sim max: 1.0
ClusterLoss debug: loss_val=2.886941, ne_loss=0.096899, total=2.983840
Sim min: 0.00336342491209507 Sim max: 1.0
ClusterLoss debug: loss_val=2.894514, ne_loss=0.102894, total=2.997408
Sim min: 0.0044755395501852036 Sim max: 1.0
ClusterLoss debug: loss_val=2.924869, ne_loss=0.063735, total=2.988604
Sim min: 0.004225653596222401 Sim max: 1.0
ClusterLoss debug: loss_val=2.896847, ne_loss=0.190181, total=3.087028
Sim min: 0.0014554352965205908 Sim max: 1.0
ClusterLoss debug: loss_val=2.903628, ne_loss=0.153193, total=3.056821
Sim min: 0.0025457877200096846 Sim max: 1.0
ClusterLoss debug: loss_val=2.883115, ne_loss=0.105971, total=2.989085
Sim min: 0.0008784615783952177 Sim max: 1.0
ClusterLoss debug: loss_val=2.886389, ne_loss=0.200753, total=3.087143
Sim min: 0.002968794899061322 Sim max: 1.0
ClusterLoss debug: loss_val=2.895750, ne_loss=0.066706, total=2.962456
Train Epoch: 1 [32128/18637 (86%)]	Instance Loss: 2.285249	Cluster Loss: 2.962456 	sec/iter: 0.6690
Sim min: 0.0016418006271123886 Sim max: 1.0
ClusterLoss debug: loss_val=2.885906, ne_loss=0.062984, total=2.948890
Sim min: 0.0032230655197054148 Sim max: 1.0
ClusterLoss debug: loss_val=2.914163, ne_loss=0.074235, total=2.988398
Sim min: 0.0013352527748793364 Sim max: 1.0
ClusterLoss debug: loss_val=2.878637, ne_loss=0.081989, total=2.960626
Sim min: 0.0012623582733795047 Sim max: 1.0
ClusterLoss debug: loss_val=2.924502, ne_loss=0.166501, total=3.091003
Sim min: 0.0026446266565471888 Sim max: 1.0
ClusterLoss debug: loss_val=2.879766, ne_loss=0.060673, total=2.940439
Sim min: 0.0017825239337980747 Sim max: 1.0
ClusterLoss debug: loss_val=2.884840, ne_loss=0.074069, total=2.958909
Sim min: 0.0020849264692515135 Sim max: 1.0
ClusterLoss debug: loss_val=2.916121, ne_loss=0.068662, total=2.984783
Sim min: 0.0023737018927931786 Sim max: 1.0
ClusterLoss debug: loss_val=2.897459, ne_loss=0.054273, total=2.951731
Sim min: 0.001799879246391356 Sim max: 1.0
ClusterLoss debug: loss_val=2.876347, ne_loss=0.123070, total=2.999417
Sim min: 0.0033324076794087887 Sim max: 1.0
ClusterLoss debug: loss_val=2.915167, ne_loss=0.047737, total=2.962904
Train Epoch: 1 [33408/18637 (89%)]	Instance Loss: 2.110216	Cluster Loss: 2.962904 	sec/iter: 0.6658
Sim min: 0.004790003877133131 Sim max: 1.0
ClusterLoss debug: loss_val=2.891845, ne_loss=0.072118, total=2.963963
Sim min: 0.0017100023105740547 Sim max: 1.0
ClusterLoss debug: loss_val=2.887913, ne_loss=0.075923, total=2.963836
Sim min: 0.003013862296938896 Sim max: 1.0
ClusterLoss debug: loss_val=2.888571, ne_loss=0.082137, total=2.970707
Sim min: 0.0015219314955174923 Sim max: 1.0
ClusterLoss debug: loss_val=2.891461, ne_loss=0.052218, total=2.943679
Sim min: 0.0026977192610502243 Sim max: 1.0
ClusterLoss debug: loss_val=2.887449, ne_loss=0.139536, total=3.026985
Sim min: 0.0042556338012218475 Sim max: 1.0
ClusterLoss debug: loss_val=2.883168, ne_loss=0.155548, total=3.038716
Sim min: 0.004982983227819204 Sim max: 1.0
ClusterLoss debug: loss_val=2.914678, ne_loss=0.075285, total=2.989963
Sim min: 0.0017112073255702853 Sim max: 1.0
ClusterLoss debug: loss_val=2.885061, ne_loss=0.066295, total=2.951356
Sim min: 0.002879893407225609 Sim max: 1.0
ClusterLoss debug: loss_val=2.888911, ne_loss=0.107188, total=2.996099
Sim min: 0.0014471285976469517 Sim max: 1.0
ClusterLoss debug: loss_val=2.880217, ne_loss=0.101382, total=2.981599
Train Epoch: 1 [34688/18637 (93%)]	Instance Loss: 2.380929	Cluster Loss: 2.981599 	sec/iter: 0.6624
Sim min: 0.0034439454320818186 Sim max: 1.0
ClusterLoss debug: loss_val=2.882051, ne_loss=0.035908, total=2.917959
Sim min: 0.004019135143607855 Sim max: 1.0
ClusterLoss debug: loss_val=2.891471, ne_loss=0.107710, total=2.999181
Sim min: 0.0030727835837751627 Sim max: 1.0
ClusterLoss debug: loss_val=2.900696, ne_loss=0.102431, total=3.003127
Sim min: 0.0046559846960008144 Sim max: 1.0
ClusterLoss debug: loss_val=2.884314, ne_loss=0.083190, total=2.967504
Sim min: 0.0016003675991669297 Sim max: 1.0
ClusterLoss debug: loss_val=2.908149, ne_loss=0.139185, total=3.047334
Sim min: 0.003097986802458763 Sim max: 1.0
ClusterLoss debug: loss_val=2.882197, ne_loss=0.074006, total=2.956203
Sim min: 0.0031277271918952465 Sim max: 1.0
ClusterLoss debug: loss_val=2.912503, ne_loss=0.104023, total=3.016526
Sim min: 0.0018180381739512086 Sim max: 1.0
ClusterLoss debug: loss_val=2.891010, ne_loss=0.088925, total=2.979935
Sim min: 0.0025514590088278055 Sim max: 1.0
ClusterLoss debug: loss_val=2.872401, ne_loss=0.165843, total=3.038244
Sim min: 0.0033961685840040445 Sim max: 1.0
ClusterLoss debug: loss_val=2.909903, ne_loss=0.082393, total=2.992296
Train Epoch: 1 [35968/18637 (96%)]	Instance Loss: 2.742770	Cluster Loss: 2.992296 	sec/iter: 0.6613
Sim min: 0.0015399426920339465 Sim max: 1.0
ClusterLoss debug: loss_val=2.888419, ne_loss=0.121943, total=3.010361
Sim min: 0.001522422069683671 Sim max: 1.0
ClusterLoss debug: loss_val=2.880492, ne_loss=0.138870, total=3.019362
Sim min: 0.001727823168039322 Sim max: 1.0
ClusterLoss debug: loss_val=2.886516, ne_loss=0.057405, total=2.943921
Sim min: 0.002968088025227189 Sim max: 1.0
ClusterLoss debug: loss_val=2.894086, ne_loss=0.090715, total=2.984801
Sim min: 0.0012846908066421747 Sim max: 1.0
ClusterLoss debug: loss_val=2.899547, ne_loss=0.112677, total=3.012223
Sim min: 0.002375756623223424 Sim max: 1.0
ClusterLoss debug: loss_val=2.907783, ne_loss=0.042588, total=2.950372
Sim min: 0.0024423017166554928 Sim max: 1.0
ClusterLoss debug: loss_val=2.903863, ne_loss=0.121396, total=3.025259
Sim min: 0.001849023625254631 Sim max: 1.0
ClusterLoss debug: loss_val=2.913108, ne_loss=0.095024, total=3.008132
Sim min: 0.007298827171325684 Sim max: 1.0
ClusterLoss debug: loss_val=2.904756, ne_loss=0.111711, total=3.016468
Sim min: 0.0028447096701711416 Sim max: 1.0
ClusterLoss debug: loss_val=2.911096, ne_loss=0.176877, total=3.087974
Train Epoch: 1 [37248/18637 (100%)]	Instance Loss: 2.774014	Cluster Loss: 3.087974 	sec/iter: 0.6582
Sim min: 0.005283145699650049 Sim max: 1.0
ClusterLoss debug: loss_val=3.032378, ne_loss=0.569868, total=3.602247
Train Epoch: 1 [37274/18637 (100%)]	Instance Loss: 1.975099	Cluster Loss: 3.602247 	sec/iter: 0.6567
Model saved to model_epoch_1.pth
请输入数字1以继续运行: 
===== 分类评估 =====
Epoch 1 - Loss: 3.1062, Acc: 4.49%
Recall:     0.0800 0.0000 0.0000 0.0500 0.0000 0.0100 0.0000 0.0000 0.1400 0.2400 0.0000 0.0000 0.0000 0.0000 0.0000 0.0141 0.3900 0.0100 0.0000 0.0000 0.0000
Precision:  0.0367 0.0000 0.0000 0.0153 0.0000 0.0556 0.0000 0.0000 0.0586 0.2105 0.0000 0.0000 0.0000 0.0000 0.0000 0.0076 0.1105 0.0075 0.0000 0.0000 0.0000
F1-score:   0.0503 0.0000 0.0000 0.0234 0.0000 0.0169 0.0000 0.0000 0.0826 0.2243 0.0000 0.0000 0.0000 0.0000 0.0000 0.0099 0.1722 0.0086 0.0000 0.0000 0.0000

===== 聚类评估 =====
混淆矩阵:
 [[ 1  0  0  1  0  0  0  0  0  0  0  0  0  0  2 96  0  0  0  0  0]
 [ 2  0  0  2 16  0  0  0  0  0  0  0  0  0  0  6 74  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  3 49  0  0  0  0 48]
 [ 0  0  0  2  0  0 69  0  0  0  0  0  0  0  4  7  0  0  0  0 18]
 [33  0  0  0  0  0  0  0  0  0  0  0  0  0  2 23  0  0  0 41  1]
 [31  0  0  2  0  0  0  1  0  0  0  0  0  0  1 62  1  0  0  0  2]
 [ 0 16  0 23  0  0  1  0 13  0  6  3 30  0  0  0  0  0  7  0  1]
 [ 0  0  0  0  0  0  0  0  0 11  0  0  0  0 89  0  0  0  0  0  0]
 [68  0  0 13  2  0  0  0  0  0  0  0  0  1  0 14  0  0  0  0  2]
 [ 4  0  0  0  0 73  0  0  0  0  0  0  0  0  0 22  0  0  0  1  0]
 [ 0  0  0  2  0  6  0  1  0  0  0  0  0  0  0 91  0  0  0  0  0]
 [ 0  0 25 59  0  0  7  0  0  0  0  0  0  0  2  1  0  2  0  0  4]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0 90  9  0  0  0  0  0  1]
 [ 0  0  0  1  0  0  0 29  0  0  0  0  0  0 55 14  0  0  0  0  1]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  2 78 16  0  0  0  0  1]
 [22  0  0 15  0  0  0  0  0  0  0  0  0  0  2 29  0  0  0  0  3]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  2 94  2  0  0  0  1]
 [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0 92  2  0  0  1  1]
 [ 0  0  0  1  0  0  1  0  0  0  0  0  0  0  0 39  0  0  0 59  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 99  0  0  0  0  0]
 [ 0  0  0  0 66  0  1  0  0  0  0  0  0  0  0  1 31  0  0  0  1]]
代价矩阵:
 [[160. 159. 161. 161. 128. 130. 161. 161.  93. 157. 161. 161. 161. 161.
  161. 139. 161. 161. 161. 161. 161.]
 [ 16.  16.  16.  16.  16.  16.   0.  16.  16.  16.  16.  16.  16.  16.
   16.  16.  16.  16.  16.  16.  16.]
 [ 25.  25.  25.  25.  25.  25.  25.  25.  25.  25.  25.   0.  25.  25.
   25.  25.  25.  25.  25.  25.  25.]
 [121. 120. 122. 120. 122. 120.  99. 122. 109. 122. 120.  63. 122. 121.
  122. 107. 121. 122. 121. 122. 122.]
 [ 88.  72.  88.  88.  88.  88.  88.  88.  86.  88.  88.  88.  88.  88.
   88.  88.  88.  84.  88.  88.  22.]
 [ 79.  79.  79.  79.  79.  79.  79.  79.  79.   6.  73.  79.  79.  79.
   79.  79.  79.  79.  79.  79.  79.]
 [ 79.  79.  79.  10.  79.  79.  78.  79.  79.  79.  79.  72.  79.  79.
   79.  79.  79.  79.  78.  79.  78.]
 [ 34.  34.  34.  34.  34.  33.  34.  34.  34.  34.  33.  34.  34.   5.
   31.  34.  34.  34.  34.  34.  34.]
 [ 13.  13.  13.  13.  13.  13.   0.  13.  13.  13.  13.  13.  13.  13.
   13.  13.  13.  13.  13.  13.  13.]
 [ 11.  11.  11.  11.  11.  11.  11.   0.  11.  11.  11.  11.  11.  11.
   11.  11.  11.  11.  11.  11.  11.]
 [  6.   6.   6.   6.   6.   6.   0.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.]
 [  3.   3.   3.   3.   3.   3.   0.   3.   3.   3.   3.   3.   3.   3.
    3.   3.   3.   3.   3.   3.   3.]
 [ 30.  30.  30.  30.  30.  30.   0.  30.  30.  30.  30.  30.  30.  30.
   30.  30.  30.  30.  30.  30.  30.]
 [ 93.  93.  93.  93.  93.  93.  93.  93.  92.  93.  93.  93.   3.  93.
   91.  93.  93.  93.  93.  93.  93.]
 [248. 250. 247. 246. 248. 249. 250. 161. 250. 250. 250. 248. 241. 195.
  172. 248. 248. 250. 250. 249. 250.]
 [659. 749. 706. 748. 732. 693. 755. 755. 741. 733. 664. 754. 755. 741.
  739. 726. 661. 663. 716. 656. 754.]
 [110.  36. 110. 110. 110. 109. 110. 110. 110. 110. 110. 110. 110. 110.
  110. 110. 108. 108. 110. 110.  79.]
 [  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [  7.   7.   7.   7.   7.   7.   0.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.]
 [102. 102. 102. 102.  61. 102. 102. 102. 102. 101. 102. 102. 102. 102.
  102. 102. 102. 101.  43. 102. 102.]
 [ 85.  85.  37.  67.  84.  83.  84.  85.  83.  85.  85.  81.  84.  84.
   84.  82.  84.  84.  85.  85.  84.]]
最佳映射关系: [ 8.  0. 17. 11. 20.  9.  3. 13.  4.  7. 10.  5.  6. 12. 14. 19.  1. 16.
 15. 18.  2.]
聚类分配的最大索引: 20

=== 实例级嵌入 ===
NMI: 0.5705  ARI: 0.2011
F-score: 0.3159  Silhouette: 0.3080
Adjusted Acc: 0.4119
混淆矩阵:
 [[20  0  1  1  0  3  0  0  0  4  0  0  0  0  0  5  4 23 26 13  0]
 [ 8  1  0  6 12  0  0  0  0  0 38 32  0  0  0  1  0  0  0  1  1]
 [31  5  0  2  1 17  1 14  2  0  0  0  0  0  0  3  0  4 12  6  2]
 [ 5 25  0  3  0  0 21  0 32  1  1  0  0  0  0  0  0  0  1  0 11]
 [ 0  0  0  1  5  1  0 28  2  0  0  0  0  0  0  3 21 15 14  9  1]
 [ 7  2  7 10  2  6  1  0  2 11  0  7  0  0  0 10  4 19  7  4  1]
 [ 0  2  0  0  0  0 23  0 37  0  0  0  0  0 38  0  0  0  0  0  0]
 [ 0  6  0  0  0  0  0  0 36  0  0  0  0  0  9  0  0  0  0  0 49]
 [ 3  0 21 13  1  4 18  0  1  0  2  0  0  0  0 10  0  3 19  4  1]
 [12  0 66  1  0  0  0  0  9  0  0  0  0  0  0  0  0  0 11  0  1]
 [14  1 75  0  1  0  1  0  0  1  0  0  0  0  0  3  0  1  3  0  0]
 [ 1  5  0  1  0  0 26  0 12  0  0  0 27  0  0  0  0  0  0  0 28]
 [ 0 27  0  6 10  0  0  0 29  6  4  8  0  0  0  8  1  0  0  0  1]
 [ 4  0  0  1  2  2  3  0  2 52  0  0  0 15  0  6  4  3  3  1  2]
 [ 4  1  0  0  2  9  0  0  4 58  0  0  0  0  0  6  1  8  2  2  3]
 [ 0  1  0  3  0 12 12  0  3  1  0  0  0  0  0  1  0  0 18  0 20]
 [ 4  2  0 48  4  0  0  1  0  1  0  0  0  1  0  5  2  0 13 19  0]
 [ 5  0 29  1  2 21  0  1  1  1  0  0  0  4  0 18  7  1  7  2  0]
 [ 0  1  0  0  0 35  0 56  0  1  0  0  0  0  0  0  0  1  3  3  0]
 [ 5  0  1  3  0 23  3  0  0  0  0  0  0  0  0 19 30  9  2  2  3]
 [ 0  1  0  1 23  0  0  0  0 22 31  0  0 18  0  1  0  0  0  2  1]]
代价矩阵:
 [[103. 115.  92. 118. 123. 116. 123. 123. 120. 111. 109. 122. 123. 119.
  119. 123. 119. 118. 123. 118. 123.]
 [ 80.  79.  75.  55.  80.  78.  78.  74.  80.  80.  79.  75.  53.  80.
   79.  79.  78.  80.  79.  80.  79.]
 [199. 200. 200. 200. 200. 193. 200. 200. 179. 134. 125. 200. 200. 200.
  200. 200. 200. 171. 200. 199. 200.]
 [100.  95.  99.  98. 100.  91. 101. 101.  88. 100. 101. 100.  95. 100.
  101.  98.  53. 100. 101.  98. 100.]
 [ 65.  53.  64.  65.  60.  63.  65.  65.  64.  65.  64.  65.  55.  63.
   63.  65.  61.  63.  65.  65.  42.]
 [130. 133. 116. 133. 132. 127. 133. 133. 129. 133. 133. 133. 133. 131.
  124. 121. 133. 112.  98. 110. 133.]
 [109. 109. 108.  88. 109. 108.  86. 109.  91. 109. 108.  83. 109. 106.
  109.  97. 109. 109. 109. 106. 109.]
 [100. 100.  86. 100.  72. 100. 100. 100. 100. 100. 100. 100. 100. 100.
  100. 100.  99.  99.  44. 100. 100.]
 [172. 172. 170. 140. 170. 170. 135. 136. 171. 163. 172. 160. 143. 170.
  168. 169. 172. 171. 172. 172. 172.]
 [155. 159. 159. 158. 159. 148. 159. 159. 159. 159. 158. 159. 153. 107.
  101. 158. 158. 158. 158. 159. 137.]
 [ 76.  38.  76.  75.  76.  76.  76.  76.  74.  76.  76.  76.  72.  76.
   76.  76.  76.  76.  76.  76.  45.]
 [ 47.  15.  47.  47.  47.  40.  47.  47.  47.  47.  47.  47.  39.  47.
   47.  47.  47.  47.  47.  47.  47.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 38.  38.  38.  38.  38.  38.  38.  38.  38.  38.  38.  38.  38.  23.
   38.  38.  37.  34.  38.  38.  20.]
 [ 47.  47.  47.  47.  47.  47.   9.  38.  47.  47.  47.  47.  47.  47.
   47.  47.  47.  47.  47.  47.  47.]
 [ 94.  98.  96.  99.  96.  89.  99.  99.  89.  99.  96.  99.  91.  93.
   93.  98.  94.  81.  99.  80.  98.]
 [ 70.  74.  74.  74.  53.  70.  74.  74.  74.  74.  74.  74.  73.  70.
   73.  74.  72.  67.  74.  44.  74.]
 [ 64.  87.  83.  87.  72.  68.  87.  87.  84.  87.  86.  87.  87.  84.
   79.  87.  87.  86.  86.  78.  87.]
 [115. 141. 129. 140. 127. 134. 141. 141. 122. 130. 138. 141. 141. 138.
  139. 123. 128. 134. 138. 139. 141.]
 [ 55.  67.  62.  68.  59.  64.  68.  68.  64.  68.  68.  68.  68.  67.
   66.  68.  49.  66.  65.  66.  66.]
 [125. 124. 123. 114. 124. 124. 125.  76. 124. 124. 125.  97. 124. 123.
  122. 105. 125. 125. 125. 122. 124.]]
最佳映射关系: [ 2. 12. 10. 16.  9. 15.  8. 18.  3. 14. 20.  1. 11. 13.  6. 17. 19.  5.
  0.  4.  7.]
聚类分配的最大索引: 20

=== 簇级嵌入 ===
NMI: 0.4296  ARI: 0.1982
F-score: 0.2406  Silhouette: 0.5381
Adjusted Acc: 0.3143

总耗时: 25.35s
分类准确率: 4.49%
簇级嵌入NMI: 0.4296
请输入数字2以继续运行: Sim min: 0.0030475023668259382 Sim max: 1.0
ClusterLoss debug: loss_val=2.903965, ne_loss=0.076265, total=2.980230
Train Epoch: 2 [128/18637 (0%)]	Instance Loss: 2.610332	Cluster Loss: 2.980230 	sec/iter: 0.8461
Sim min: 0.003701778594404459 Sim max: 1.0
ClusterLoss debug: loss_val=2.893809, ne_loss=0.150571, total=3.044380
Sim min: 0.004016484599560499 Sim max: 1.0
ClusterLoss debug: loss_val=2.948654, ne_loss=0.167263, total=3.115917
Sim min: 0.001644811825826764 Sim max: 1.0
ClusterLoss debug: loss_val=2.892239, ne_loss=0.115873, total=3.008112
Sim min: 0.005243224091827869 Sim max: 1.0
ClusterLoss debug: loss_val=2.931387, ne_loss=0.099725, total=3.031112
Sim min: 0.003592129796743393 Sim max: 1.0
ClusterLoss debug: loss_val=2.913710, ne_loss=0.173827, total=3.087538
Sim min: 0.0032074383925646544 Sim max: 1.0
ClusterLoss debug: loss_val=2.902945, ne_loss=0.067403, total=2.970348
Sim min: 0.0012538052396848798 Sim max: 1.0
ClusterLoss debug: loss_val=2.878757, ne_loss=0.170734, total=3.049492
Sim min: 0.002345886779949069 Sim max: 1.0
ClusterLoss debug: loss_val=2.876241, ne_loss=0.075346, total=2.951586
Sim min: 0.003034418448805809 Sim max: 1.0
ClusterLoss debug: loss_val=2.887781, ne_loss=0.079741, total=2.967522
Sim min: 0.0021149066742509604 Sim max: 1.0
ClusterLoss debug: loss_val=2.892541, ne_loss=0.100195, total=2.992736
Train Epoch: 2 [1408/18637 (4%)]	Instance Loss: 2.316574	Cluster Loss: 2.992736 	sec/iter: 0.8053
Sim min: 0.005473946686834097 Sim max: 1.0
ClusterLoss debug: loss_val=2.898222, ne_loss=0.104634, total=3.002856
Sim min: 0.0026241925079375505 Sim max: 1.0
ClusterLoss debug: loss_val=2.896112, ne_loss=0.047320, total=2.943432
Sim min: 0.003007588442414999 Sim max: 1.0
ClusterLoss debug: loss_val=2.885188, ne_loss=0.094706, total=2.979894
Sim min: 0.002538365079089999 Sim max: 1.0
ClusterLoss debug: loss_val=2.903026, ne_loss=0.086698, total=2.989724
Sim min: 0.001847487990744412 Sim max: 1.0
ClusterLoss debug: loss_val=2.882158, ne_loss=0.054217, total=2.936375
Sim min: 0.00285544921644032 Sim max: 1.0
ClusterLoss debug: loss_val=2.882484, ne_loss=0.083065, total=2.965549
Sim min: 0.005374742206186056 Sim max: 1.0
ClusterLoss debug: loss_val=2.917860, ne_loss=0.148319, total=3.066180
Sim min: 0.0034956741146743298 Sim max: 1.0
ClusterLoss debug: loss_val=2.887998, ne_loss=0.043829, total=2.931827
Sim min: 0.0019155869958922267 Sim max: 1.0
ClusterLoss debug: loss_val=2.886626, ne_loss=0.086823, total=2.973449
Sim min: 0.005671211518347263 Sim max: 1.0
ClusterLoss debug: loss_val=2.912640, ne_loss=0.148290, total=3.060930
Train Epoch: 2 [2688/18637 (7%)]	Instance Loss: 2.207542	Cluster Loss: 3.060930 	sec/iter: 0.7886
Sim min: 0.0033772923052310944 Sim max: 1.0
ClusterLoss debug: loss_val=2.899824, ne_loss=0.060770, total=2.960594
Sim min: 0.0037082061171531677 Sim max: 1.0
ClusterLoss debug: loss_val=2.889875, ne_loss=0.101337, total=2.991211
Sim min: 0.004821639973670244 Sim max: 1.0
ClusterLoss debug: loss_val=2.904613, ne_loss=0.061306, total=2.965919
Sim min: 0.0015172669664025307 Sim max: 1.0
ClusterLoss debug: loss_val=2.881457, ne_loss=0.113933, total=2.995390
Sim min: 0.0019479120383039117 Sim max: 1.0
ClusterLoss debug: loss_val=2.887889, ne_loss=0.086804, total=2.974693
Sim min: 0.0012826384045183659 Sim max: 1.0
ClusterLoss debug: loss_val=2.890236, ne_loss=0.091154, total=2.981390
Sim min: 0.0044479151256382465 Sim max: 1.0
ClusterLoss debug: loss_val=2.905644, ne_loss=0.182080, total=3.087724
Sim min: 0.003402178641408682 Sim max: 1.0
ClusterLoss debug: loss_val=2.891535, ne_loss=0.116701, total=3.008236
Sim min: 0.0013202500995248556 Sim max: 1.0
ClusterLoss debug: loss_val=2.885957, ne_loss=0.114546, total=3.000504
Sim min: 0.0014274620916694403 Sim max: 1.0
ClusterLoss debug: loss_val=2.869001, ne_loss=0.064792, total=2.933794
Train Epoch: 2 [3968/18637 (11%)]	Instance Loss: 2.039626	Cluster Loss: 2.933794 	sec/iter: 0.7823
Sim min: 0.0015917940763756633 Sim max: 1.0
ClusterLoss debug: loss_val=2.899386, ne_loss=0.063688, total=2.963075
Sim min: 0.0027664355002343655 Sim max: 1.0
ClusterLoss debug: loss_val=2.886212, ne_loss=0.062665, total=2.948876
Sim min: 0.0027986590284854174 Sim max: 1.0
ClusterLoss debug: loss_val=2.901172, ne_loss=0.108076, total=3.009248
Sim min: 0.0019924233201891184 Sim max: 1.0
ClusterLoss debug: loss_val=2.888822, ne_loss=0.098784, total=2.987606
Sim min: 0.0015797432279214263 Sim max: 1.0
ClusterLoss debug: loss_val=2.898021, ne_loss=0.145063, total=3.043084
Sim min: 0.0009306506253778934 Sim max: 1.0
ClusterLoss debug: loss_val=2.906071, ne_loss=0.109748, total=3.015820
Sim min: 0.0012430297210812569 Sim max: 1.0
ClusterLoss debug: loss_val=2.890447, ne_loss=0.114463, total=3.004910
Sim min: 0.0011443247785791755 Sim max: 1.0
ClusterLoss debug: loss_val=2.891961, ne_loss=0.098731, total=2.990692
Sim min: 0.0010944706154987216 Sim max: 1.0
ClusterLoss debug: loss_val=2.874801, ne_loss=0.079921, total=2.954723
Sim min: 0.0016649779863655567 Sim max: 1.0
ClusterLoss debug: loss_val=2.892665, ne_loss=0.116950, total=3.009614
Train Epoch: 2 [5248/18637 (14%)]	Instance Loss: 2.572591	Cluster Loss: 3.009614 	sec/iter: 0.7834
Sim min: 0.0018033790402114391 Sim max: 1.0
ClusterLoss debug: loss_val=2.906164, ne_loss=0.113820, total=3.019983
Sim min: 0.000923668616451323 Sim max: 1.0
ClusterLoss debug: loss_val=2.913278, ne_loss=0.120228, total=3.033506
Sim min: 0.002418546238914132 Sim max: 1.0
ClusterLoss debug: loss_val=2.890532, ne_loss=0.114523, total=3.005055
Sim min: 0.00148371874820441 Sim max: 1.0
ClusterLoss debug: loss_val=2.896236, ne_loss=0.096730, total=2.992967
Sim min: 0.0032022083178162575 Sim max: 1.0
ClusterLoss debug: loss_val=2.885878, ne_loss=0.141001, total=3.026879
Sim min: 0.003755605313926935 Sim max: 1.0
ClusterLoss debug: loss_val=2.888379, ne_loss=0.103567, total=2.991946
Sim min: 0.001520030666142702 Sim max: 1.0
ClusterLoss debug: loss_val=2.894754, ne_loss=0.047262, total=2.942016
Sim min: 0.004157093819230795 Sim max: 1.0
ClusterLoss debug: loss_val=2.893125, ne_loss=0.082562, total=2.975687
Sim min: 0.003771013580262661 Sim max: 1.0
ClusterLoss debug: loss_val=2.893877, ne_loss=0.104844, total=2.998720
Sim min: 0.002747565507888794 Sim max: 1.0
ClusterLoss debug: loss_val=2.900523, ne_loss=0.169459, total=3.069983
Train Epoch: 2 [6528/18637 (17%)]	Instance Loss: 2.595156	Cluster Loss: 3.069983 	sec/iter: 0.7753
Sim min: 0.0033794259652495384 Sim max: 1.0
ClusterLoss debug: loss_val=2.886765, ne_loss=0.108437, total=2.995202
Sim min: 0.001734612393192947 Sim max: 1.0
ClusterLoss debug: loss_val=2.879535, ne_loss=0.100253, total=2.979788
Sim min: 0.0025406053755432367 Sim max: 1.0
ClusterLoss debug: loss_val=2.877100, ne_loss=0.217197, total=3.094297
Sim min: 0.00297812488861382 Sim max: 1.0
ClusterLoss debug: loss_val=2.897708, ne_loss=0.200061, total=3.097769
Sim min: 0.0010347162606194615 Sim max: 1.0
ClusterLoss debug: loss_val=2.872276, ne_loss=0.169580, total=3.041856
Sim min: 0.0018631428247317672 Sim max: 1.0
ClusterLoss debug: loss_val=2.901256, ne_loss=0.106722, total=3.007978
Sim min: 0.0016500656493008137 Sim max: 1.0
ClusterLoss debug: loss_val=2.905153, ne_loss=0.116279, total=3.021432
Sim min: 0.0017696097493171692 Sim max: 1.0
ClusterLoss debug: loss_val=2.883928, ne_loss=0.101704, total=2.985632
Sim min: 0.0026377318426966667 Sim max: 1.0
ClusterLoss debug: loss_val=2.888095, ne_loss=0.143050, total=3.031145
Sim min: 0.007894113659858704 Sim max: 1.0
ClusterLoss debug: loss_val=2.919773, ne_loss=0.248464, total=3.168237
Train Epoch: 2 [7808/18637 (21%)]	Instance Loss: 2.228153	Cluster Loss: 3.168237 	sec/iter: 0.7801
Sim min: 0.0024538987781852484 Sim max: 1.0
ClusterLoss debug: loss_val=2.908027, ne_loss=0.111679, total=3.019706
Sim min: 0.0021029231138527393 Sim max: 1.0
ClusterLoss debug: loss_val=2.879604, ne_loss=0.083452, total=2.963057
Sim min: 0.002193632535636425 Sim max: 1.0
ClusterLoss debug: loss_val=2.885115, ne_loss=0.026074, total=2.911189
Sim min: 0.001198124373331666 Sim max: 1.0
ClusterLoss debug: loss_val=2.890685, ne_loss=0.062685, total=2.953370
Sim min: 0.002954628085717559 Sim max: 1.0
ClusterLoss debug: loss_val=2.926926, ne_loss=0.128285, total=3.055211
Sim min: 0.0014796354807913303 Sim max: 1.0
ClusterLoss debug: loss_val=2.887731, ne_loss=0.129713, total=3.017444
Sim min: 0.0012416638201102614 Sim max: 1.0
ClusterLoss debug: loss_val=2.880389, ne_loss=0.103781, total=2.984170
Sim min: 0.0015601629856973886 Sim max: 1.0
ClusterLoss debug: loss_val=2.900557, ne_loss=0.104479, total=3.005036
Sim min: 0.002259500091895461 Sim max: 1.0
ClusterLoss debug: loss_val=2.875198, ne_loss=0.053978, total=2.929176
Sim min: 0.001937576220370829 Sim max: 1.0
ClusterLoss debug: loss_val=2.885317, ne_loss=0.107561, total=2.992878
Train Epoch: 2 [9088/18637 (24%)]	Instance Loss: 2.635925	Cluster Loss: 2.992878 	sec/iter: 0.7757
Sim min: 0.0029920609667897224 Sim max: 1.0
ClusterLoss debug: loss_val=2.911885, ne_loss=0.059747, total=2.971633
Sim min: 0.0017500515095889568 Sim max: 1.0
ClusterLoss debug: loss_val=2.909750, ne_loss=0.293168, total=3.202918
Sim min: 0.003462277352809906 Sim max: 1.0
ClusterLoss debug: loss_val=2.913083, ne_loss=0.104967, total=3.018049
Sim min: 0.0013862663181498647 Sim max: 1.0
ClusterLoss debug: loss_val=2.873631, ne_loss=0.152314, total=3.025944
Sim min: 0.0017961484845727682 Sim max: 1.0
ClusterLoss debug: loss_val=2.883566, ne_loss=0.100402, total=2.983968
Sim min: 0.0012393001234158874 Sim max: 1.0
ClusterLoss debug: loss_val=2.908116, ne_loss=0.206046, total=3.114162
Sim min: 0.004032726399600506 Sim max: 1.0
ClusterLoss debug: loss_val=2.906185, ne_loss=0.274349, total=3.180534
Sim min: 0.002128913998603821 Sim max: 1.0
ClusterLoss debug: loss_val=2.891887, ne_loss=0.071460, total=2.963347
Sim min: 0.003320462303236127 Sim max: 1.0
ClusterLoss debug: loss_val=2.896030, ne_loss=0.115285, total=3.011315
Sim min: 0.0028083950746804476 Sim max: 1.0
ClusterLoss debug: loss_val=2.903325, ne_loss=0.171655, total=3.074980
Train Epoch: 2 [10368/18637 (28%)]	Instance Loss: 2.230512	Cluster Loss: 3.074980 	sec/iter: 0.7773
Sim min: 0.0029049862641841173 Sim max: 1.0
ClusterLoss debug: loss_val=2.909154, ne_loss=0.126727, total=3.035882
Sim min: 0.002368804533034563 Sim max: 1.0
ClusterLoss debug: loss_val=2.907377, ne_loss=0.052043, total=2.959420
Sim min: 0.0005756820319220424 Sim max: 1.0
ClusterLoss debug: loss_val=2.870502, ne_loss=0.101788, total=2.972289
Sim min: 0.002886468544602394 Sim max: 1.0
ClusterLoss debug: loss_val=2.881595, ne_loss=0.052644, total=2.934239
Sim min: 0.0014248499646782875 Sim max: 1.0
ClusterLoss debug: loss_val=2.880741, ne_loss=0.090019, total=2.970760
Sim min: 0.0036690181586891413 Sim max: 1.0
ClusterLoss debug: loss_val=2.899360, ne_loss=0.122882, total=3.022242
Sim min: 0.00335717317648232 Sim max: 1.0
ClusterLoss debug: loss_val=2.897481, ne_loss=0.067068, total=2.964549
Sim min: 0.0015632221475243568 Sim max: 1.0
ClusterLoss debug: loss_val=2.902826, ne_loss=0.084594, total=2.987420
Sim min: 0.00241106073372066 Sim max: 1.0
ClusterLoss debug: loss_val=2.870306, ne_loss=0.108776, total=2.979081
Sim min: 0.0018651080317795277 Sim max: 1.0
ClusterLoss debug: loss_val=2.909873, ne_loss=0.065545, total=2.975418
Train Epoch: 2 [11648/18637 (31%)]	Instance Loss: 2.364521	Cluster Loss: 2.975418 	sec/iter: 0.7723
Sim min: 0.0032073676120489836 Sim max: 1.0
ClusterLoss debug: loss_val=2.918143, ne_loss=0.079363, total=2.997506
Sim min: 0.0020189720671623945 Sim max: 1.0
ClusterLoss debug: loss_val=2.871124, ne_loss=0.103206, total=2.974330
Sim min: 0.002961042569950223 Sim max: 1.0
ClusterLoss debug: loss_val=2.872410, ne_loss=0.147474, total=3.019885
Sim min: 0.002942930907011032 Sim max: 1.0
ClusterLoss debug: loss_val=2.896074, ne_loss=0.172636, total=3.068709
Sim min: 0.002087031491100788 Sim max: 1.0
ClusterLoss debug: loss_val=2.881052, ne_loss=0.163353, total=3.044406
Sim min: 0.0025437523145228624 Sim max: 1.0
ClusterLoss debug: loss_val=2.891743, ne_loss=0.040014, total=2.931756
Sim min: 0.0017378190532326698 Sim max: 1.0
ClusterLoss debug: loss_val=2.894653, ne_loss=0.133048, total=3.027701
Sim min: 0.0027190495748072863 Sim max: 1.0
ClusterLoss debug: loss_val=2.901990, ne_loss=0.099091, total=3.001081
Sim min: 0.0019708022009581327 Sim max: 1.0
ClusterLoss debug: loss_val=2.890455, ne_loss=0.093068, total=2.983524
Sim min: 0.002206052653491497 Sim max: 1.0
ClusterLoss debug: loss_val=2.880749, ne_loss=0.077532, total=2.958281
Train Epoch: 2 [12928/18637 (35%)]	Instance Loss: 2.571945	Cluster Loss: 2.958281 	sec/iter: 0.7718
Sim min: 0.0032222929876297712 Sim max: 1.0
ClusterLoss debug: loss_val=2.888708, ne_loss=0.139589, total=3.028297
Sim min: 0.0015391906490549445 Sim max: 1.0
ClusterLoss debug: loss_val=2.897845, ne_loss=0.103675, total=3.001520
Sim min: 0.0009588028187863529 Sim max: 1.0
ClusterLoss debug: loss_val=2.881761, ne_loss=0.056131, total=2.937892
Sim min: 0.0029980610124766827 Sim max: 1.0
ClusterLoss debug: loss_val=2.895602, ne_loss=0.158007, total=3.053609
Sim min: 0.002196338726207614 Sim max: 1.0
ClusterLoss debug: loss_val=2.900690, ne_loss=0.094846, total=2.995536
Sim min: 0.0037416487466543913 Sim max: 1.0
ClusterLoss debug: loss_val=2.906932, ne_loss=0.136940, total=3.043873
Sim min: 0.0032703580800443888 Sim max: 1.0
ClusterLoss debug: loss_val=2.898152, ne_loss=0.200393, total=3.098545
Sim min: 0.0013559495564550161 Sim max: 1.0
ClusterLoss debug: loss_val=2.869537, ne_loss=0.104442, total=2.973979
Sim min: 0.0007663142750971019 Sim max: 1.0
ClusterLoss debug: loss_val=2.891746, ne_loss=0.142664, total=3.034410
Sim min: 0.0014088156167417765 Sim max: 1.0
ClusterLoss debug: loss_val=2.888319, ne_loss=0.057016, total=2.945335
Train Epoch: 2 [14208/18637 (38%)]	Instance Loss: 2.249226	Cluster Loss: 2.945335 	sec/iter: 0.7774
Sim min: 0.0016296339454129338 Sim max: 1.0
ClusterLoss debug: loss_val=2.881554, ne_loss=0.053458, total=2.935012
Sim min: 0.0033842253033071756 Sim max: 1.0
ClusterLoss debug: loss_val=2.899105, ne_loss=0.056753, total=2.955858
Sim min: 0.002371749607846141 Sim max: 1.0
ClusterLoss debug: loss_val=2.884894, ne_loss=0.080324, total=2.965218
Sim min: 0.002073003211989999 Sim max: 1.0
ClusterLoss debug: loss_val=2.901401, ne_loss=0.091287, total=2.992688
Sim min: 0.002826129784807563 Sim max: 1.0
ClusterLoss debug: loss_val=2.895065, ne_loss=0.051538, total=2.946603
Sim min: 0.0010226914891973138 Sim max: 1.0
ClusterLoss debug: loss_val=2.889716, ne_loss=0.105123, total=2.994839
Sim min: 0.0033126806374639273 Sim max: 1.0
ClusterLoss debug: loss_val=2.881653, ne_loss=0.116762, total=2.998414
Sim min: 0.003963287454098463 Sim max: 1.0
ClusterLoss debug: loss_val=2.907131, ne_loss=0.074693, total=2.981825
Sim min: 0.0038617842365056276 Sim max: 1.0
ClusterLoss debug: loss_val=2.910757, ne_loss=0.062328, total=2.973085
Sim min: 0.004499782342463732 Sim max: 1.0
ClusterLoss debug: loss_val=2.879342, ne_loss=0.064996, total=2.944338
Train Epoch: 2 [15488/18637 (41%)]	Instance Loss: 2.234999	Cluster Loss: 2.944338 	sec/iter: 0.7885
Sim min: 0.0014915956417098641 Sim max: 1.0
ClusterLoss debug: loss_val=2.887752, ne_loss=0.097004, total=2.984756
Sim min: 0.0023761738557368517 Sim max: 1.0
ClusterLoss debug: loss_val=2.879848, ne_loss=0.020082, total=2.899929
Sim min: 0.0016948648262768984 Sim max: 1.0
ClusterLoss debug: loss_val=2.873528, ne_loss=0.141018, total=3.014546
Sim min: 0.0017535864608362317 Sim max: 1.0
ClusterLoss debug: loss_val=2.907638, ne_loss=0.084182, total=2.991820
Sim min: 0.0018318173242732882 Sim max: 1.0
ClusterLoss debug: loss_val=2.879605, ne_loss=0.062617, total=2.942222
Sim min: 0.001135290483944118 Sim max: 1.0
ClusterLoss debug: loss_val=2.890476, ne_loss=0.109395, total=2.999872
Sim min: 0.0019176916684955359 Sim max: 1.0
ClusterLoss debug: loss_val=2.925007, ne_loss=0.164333, total=3.089339
Sim min: 0.003882011165842414 Sim max: 1.0
ClusterLoss debug: loss_val=2.890235, ne_loss=0.148697, total=3.038932
Sim min: 0.0022324149031192064 Sim max: 1.0
ClusterLoss debug: loss_val=2.892050, ne_loss=0.129237, total=3.021287
Sim min: 0.002730024280026555 Sim max: 1.0
ClusterLoss debug: loss_val=2.913478, ne_loss=0.146186, total=3.059664
Train Epoch: 2 [16768/18637 (45%)]	Instance Loss: 2.148380	Cluster Loss: 3.059664 	sec/iter: 0.8398
Sim min: 0.0019318383419886231 Sim max: 1.0
ClusterLoss debug: loss_val=2.896396, ne_loss=0.172002, total=3.068398
Sim min: 0.0021756920032203197 Sim max: 1.0
ClusterLoss debug: loss_val=2.905602, ne_loss=0.122189, total=3.027791
Sim min: 0.002796637360006571 Sim max: 1.0
ClusterLoss debug: loss_val=2.888342, ne_loss=0.202711, total=3.091053
Sim min: 0.003696557367220521 Sim max: 1.0
ClusterLoss debug: loss_val=2.882245, ne_loss=0.161608, total=3.043853
Sim min: 0.001955899642780423 Sim max: 1.0
ClusterLoss debug: loss_val=2.889292, ne_loss=0.107046, total=2.996338
Sim min: 0.0024366797879338264 Sim max: 1.0
ClusterLoss debug: loss_val=2.907523, ne_loss=0.062418, total=2.969941
Sim min: 0.0014501528348773718 Sim max: 1.0
ClusterLoss debug: loss_val=2.917563, ne_loss=0.113621, total=3.031184
Sim min: 0.0013412086991593242 Sim max: 1.0
ClusterLoss debug: loss_val=2.870139, ne_loss=0.147365, total=3.017504
Sim min: 0.0029506629798561335 Sim max: 1.0
ClusterLoss debug: loss_val=2.883632, ne_loss=0.156523, total=3.040154
Sim min: 0.004321565851569176 Sim max: 1.0
ClusterLoss debug: loss_val=2.885615, ne_loss=0.093225, total=2.978840
Train Epoch: 2 [18048/18637 (48%)]	Instance Loss: 2.018933	Cluster Loss: 2.978840 	sec/iter: 0.8636
Sim min: 0.0025145127438008785 Sim max: 1.0
ClusterLoss debug: loss_val=2.904443, ne_loss=0.066417, total=2.970859
Sim min: 0.0012620402267202735 Sim max: 1.0
ClusterLoss debug: loss_val=2.904665, ne_loss=0.137726, total=3.042391
Sim min: 0.001832180772908032 Sim max: 1.0
ClusterLoss debug: loss_val=2.913407, ne_loss=0.163831, total=3.077237
Sim min: 0.0021307743154466152 Sim max: 1.0
ClusterLoss debug: loss_val=2.899911, ne_loss=0.216501, total=3.116412
Sim min: 0.003979463130235672 Sim max: 1.0
ClusterLoss debug: loss_val=2.923267, ne_loss=0.097438, total=3.020705
Sim min: 0.0016543446108698845 Sim max: 1.0
ClusterLoss debug: loss_val=2.886739, ne_loss=0.108388, total=2.995127
Sim min: 0.0037852886598557234 Sim max: 1.0
ClusterLoss debug: loss_val=2.917530, ne_loss=0.114400, total=3.031930
Sim min: 0.002925001084804535 Sim max: 1.0
ClusterLoss debug: loss_val=2.901280, ne_loss=0.077490, total=2.978770
Sim min: 0.0037751002237200737 Sim max: 1.0
ClusterLoss debug: loss_val=2.900186, ne_loss=0.076976, total=2.977162
Sim min: 0.001140189589932561 Sim max: 1.0
ClusterLoss debug: loss_val=2.897542, ne_loss=0.113920, total=3.011462
Train Epoch: 2 [19328/18637 (52%)]	Instance Loss: 2.406195	Cluster Loss: 3.011462 	sec/iter: 0.8870
Sim min: 0.004649501759558916 Sim max: 1.0
ClusterLoss debug: loss_val=2.922149, ne_loss=0.191301, total=3.113451
Sim min: 0.004470700863748789 Sim max: 1.0
ClusterLoss debug: loss_val=2.904656, ne_loss=0.077164, total=2.981820
Sim min: 0.0021343003027141094 Sim max: 1.0
ClusterLoss debug: loss_val=2.906513, ne_loss=0.117708, total=3.024221
Sim min: 0.0028181835077703 Sim max: 1.0
ClusterLoss debug: loss_val=2.892892, ne_loss=0.104783, total=2.997674
Sim min: 0.0019157563801854849 Sim max: 1.0
ClusterLoss debug: loss_val=2.896215, ne_loss=0.107113, total=3.003328
Sim min: 0.004025091417133808 Sim max: 1.0
ClusterLoss debug: loss_val=2.899675, ne_loss=0.042454, total=2.942129
Sim min: 0.002030865754932165 Sim max: 1.0
ClusterLoss debug: loss_val=2.917048, ne_loss=0.064376, total=2.981424
Sim min: 0.0010405067587271333 Sim max: 1.0
ClusterLoss debug: loss_val=2.889726, ne_loss=0.116520, total=3.006246
Sim min: 0.00235066469758749 Sim max: 1.0
ClusterLoss debug: loss_val=2.890675, ne_loss=0.128346, total=3.019021
Sim min: 0.0012283865362405777 Sim max: 1.0
ClusterLoss debug: loss_val=2.872466, ne_loss=0.123049, total=2.995516
Train Epoch: 2 [20608/18637 (55%)]	Instance Loss: 2.196366	Cluster Loss: 2.995516 	sec/iter: 0.9108
Sim min: 0.0026094578206539154 Sim max: 1.0
ClusterLoss debug: loss_val=2.891776, ne_loss=0.195958, total=3.087735
Sim min: 0.0019539042841643095 Sim max: 1.0
ClusterLoss debug: loss_val=2.895240, ne_loss=0.134790, total=3.030030
Sim min: 0.0015222918009385467 Sim max: 1.0
ClusterLoss debug: loss_val=2.870459, ne_loss=0.037937, total=2.908396
Sim min: 0.0020709570962935686 Sim max: 1.0
ClusterLoss debug: loss_val=2.911764, ne_loss=0.110482, total=3.022245
Sim min: 0.003473611082881689 Sim max: 1.0
ClusterLoss debug: loss_val=2.919012, ne_loss=0.177556, total=3.096569
Sim min: 0.002243516966700554 Sim max: 1.0
ClusterLoss debug: loss_val=2.918129, ne_loss=0.122840, total=3.040969
Sim min: 0.0010237527312710881 Sim max: 1.0
ClusterLoss debug: loss_val=2.893824, ne_loss=0.157577, total=3.051401
Sim min: 0.002978754695504904 Sim max: 1.0
ClusterLoss debug: loss_val=2.892063, ne_loss=0.128120, total=3.020183
Sim min: 0.002360030310228467 Sim max: 1.0
ClusterLoss debug: loss_val=2.909323, ne_loss=0.152092, total=3.061416
Sim min: 0.0016324027674272656 Sim max: 1.0
ClusterLoss debug: loss_val=2.900421, ne_loss=0.190260, total=3.090680
Train Epoch: 2 [21888/18637 (59%)]	Instance Loss: 2.463997	Cluster Loss: 3.090680 	sec/iter: 0.9343
Sim min: 0.0015250672586262226 Sim max: 1.0
ClusterLoss debug: loss_val=2.898084, ne_loss=0.096620, total=2.994704
Sim min: 0.002322007203474641 Sim max: 1.0
ClusterLoss debug: loss_val=2.903347, ne_loss=0.092021, total=2.995368
Sim min: 0.002690984169021249 Sim max: 1.0
ClusterLoss debug: loss_val=2.907561, ne_loss=0.116802, total=3.024363
Sim min: 0.0022611194290220737 Sim max: 1.0
ClusterLoss debug: loss_val=2.901438, ne_loss=0.124520, total=3.025958
Sim min: 0.003877433715388179 Sim max: 1.0
ClusterLoss debug: loss_val=2.879587, ne_loss=0.092907, total=2.972495
Sim min: 0.0015487285563722253 Sim max: 1.0
ClusterLoss debug: loss_val=2.919027, ne_loss=0.082953, total=3.001980
Sim min: 0.0019778148271143436 Sim max: 1.0
ClusterLoss debug: loss_val=2.878971, ne_loss=0.104498, total=2.983469
Sim min: 0.003761869855225086 Sim max: 1.0
ClusterLoss debug: loss_val=2.911961, ne_loss=0.130775, total=3.042736
Sim min: 0.002330367686226964 Sim max: 1.0
ClusterLoss debug: loss_val=2.878049, ne_loss=0.029432, total=2.907482
Sim min: 0.0021247887052595615 Sim max: 1.0
ClusterLoss debug: loss_val=2.901063, ne_loss=0.094321, total=2.995383
Train Epoch: 2 [23168/18637 (62%)]	Instance Loss: 2.506750	Cluster Loss: 2.995383 	sec/iter: 0.9498
Sim min: 0.0014041863614693284 Sim max: 1.0
ClusterLoss debug: loss_val=2.902980, ne_loss=0.109415, total=3.012395
Sim min: 0.0009797016391530633 Sim max: 1.0
ClusterLoss debug: loss_val=2.873908, ne_loss=0.219105, total=3.093013
Sim min: 0.0034467102959752083 Sim max: 1.0
ClusterLoss debug: loss_val=2.892582, ne_loss=0.100238, total=2.992819
Sim min: 0.0019996457267552614 Sim max: 1.0
ClusterLoss debug: loss_val=2.900188, ne_loss=0.098680, total=2.998868
Sim min: 0.002922528423368931 Sim max: 1.0
ClusterLoss debug: loss_val=2.898749, ne_loss=0.063145, total=2.961894
Sim min: 0.0018863913137465715 Sim max: 1.0
ClusterLoss debug: loss_val=2.891432, ne_loss=0.051193, total=2.942624
Sim min: 0.001967191928997636 Sim max: 1.0
ClusterLoss debug: loss_val=2.888641, ne_loss=0.154937, total=3.043578
Sim min: 0.002970782807096839 Sim max: 1.0
ClusterLoss debug: loss_val=2.895724, ne_loss=0.110664, total=3.006388
Sim min: 0.001625633449293673 Sim max: 1.0
ClusterLoss debug: loss_val=2.886135, ne_loss=0.080851, total=2.966986
Sim min: 0.0014163439627736807 Sim max: 1.0
ClusterLoss debug: loss_val=2.902580, ne_loss=0.110959, total=3.013539
Train Epoch: 2 [24448/18637 (65%)]	Instance Loss: 2.163017	Cluster Loss: 3.013539 	sec/iter: 0.9623
Sim min: 0.001408094889484346 Sim max: 1.0
ClusterLoss debug: loss_val=2.868510, ne_loss=0.081379, total=2.949889
Sim min: 0.0007344778277911246 Sim max: 1.0
ClusterLoss debug: loss_val=2.889364, ne_loss=0.119790, total=3.009153
Sim min: 0.002769173588603735 Sim max: 1.0
ClusterLoss debug: loss_val=2.911565, ne_loss=0.149065, total=3.060630
Sim min: 0.0012124780332669616 Sim max: 1.0
ClusterLoss debug: loss_val=2.889493, ne_loss=0.116845, total=3.006338
Sim min: 0.002052183961495757 Sim max: 1.0
ClusterLoss debug: loss_val=2.899791, ne_loss=0.068874, total=2.968665
Sim min: 0.0030370003078132868 Sim max: 1.0
ClusterLoss debug: loss_val=2.916454, ne_loss=0.085925, total=3.002379
Sim min: 0.0012646547984331846 Sim max: 1.0
ClusterLoss debug: loss_val=2.892159, ne_loss=0.106627, total=2.998786
Sim min: 0.002057908335700631 Sim max: 1.0
ClusterLoss debug: loss_val=2.882950, ne_loss=0.069032, total=2.951982
Sim min: 0.0017549830954521894 Sim max: 1.0
ClusterLoss debug: loss_val=2.890494, ne_loss=0.119306, total=3.009799
Sim min: 0.002570601413026452 Sim max: 1.0
ClusterLoss debug: loss_val=2.896683, ne_loss=0.090806, total=2.987489
Train Epoch: 2 [25728/18637 (69%)]	Instance Loss: 2.299180	Cluster Loss: 2.987489 	sec/iter: 0.9747
Sim min: 0.0019357416313141584 Sim max: 1.0
ClusterLoss debug: loss_val=2.890894, ne_loss=0.082087, total=2.972981
Sim min: 0.0021637720055878162 Sim max: 1.0
ClusterLoss debug: loss_val=2.888624, ne_loss=0.117964, total=3.006588
Sim min: 0.0019165099365636706 Sim max: 1.0
ClusterLoss debug: loss_val=2.903114, ne_loss=0.072210, total=2.975324
Sim min: 0.0015198455657809973 Sim max: 1.0
ClusterLoss debug: loss_val=2.921680, ne_loss=0.109599, total=3.031279
Sim min: 0.002965988125652075 Sim max: 1.0
ClusterLoss debug: loss_val=2.907976, ne_loss=0.082620, total=2.990596
Sim min: 0.0017385863466188312 Sim max: 1.0
ClusterLoss debug: loss_val=2.898762, ne_loss=0.063670, total=2.962432
Sim min: 0.0006961135659366846 Sim max: 1.0
ClusterLoss debug: loss_val=2.876095, ne_loss=0.140641, total=3.016736
Sim min: 0.0021038935519754887 Sim max: 1.0
ClusterLoss debug: loss_val=2.904624, ne_loss=0.068812, total=2.973436
Sim min: 0.001024537836201489 Sim max: 1.0
ClusterLoss debug: loss_val=2.897003, ne_loss=0.137754, total=3.034757
Sim min: 0.0019136969931423664 Sim max: 1.0
ClusterLoss debug: loss_val=2.894253, ne_loss=0.067050, total=2.961303
Train Epoch: 2 [27008/18637 (72%)]	Instance Loss: 2.700037	Cluster Loss: 2.961303 	sec/iter: 0.9713
Sim min: 0.001257540425285697 Sim max: 1.0
ClusterLoss debug: loss_val=2.895218, ne_loss=0.164955, total=3.060173
Sim min: 0.0010717972181737423 Sim max: 1.0
ClusterLoss debug: loss_val=2.878553, ne_loss=0.084151, total=2.962704
Sim min: 0.002077817916870117 Sim max: 1.0
ClusterLoss debug: loss_val=2.887351, ne_loss=0.080974, total=2.968325
Sim min: 0.004999998956918716 Sim max: 1.0
ClusterLoss debug: loss_val=2.883147, ne_loss=0.163675, total=3.046822
Sim min: 0.0016605424461886287 Sim max: 1.0
ClusterLoss debug: loss_val=2.881517, ne_loss=0.141966, total=3.023483
Sim min: 0.001591873704455793 Sim max: 1.0
ClusterLoss debug: loss_val=2.872796, ne_loss=0.073507, total=2.946304
Sim min: 0.0009827640606090426 Sim max: 1.0
ClusterLoss debug: loss_val=2.875977, ne_loss=0.105868, total=2.981845
Sim min: 0.002444448182359338 Sim max: 1.0
ClusterLoss debug: loss_val=2.874704, ne_loss=0.103571, total=2.978275
Sim min: 0.001856344286352396 Sim max: 1.0
ClusterLoss debug: loss_val=2.885998, ne_loss=0.081949, total=2.967946
Sim min: 0.0013510941062122583 Sim max: 1.0
ClusterLoss debug: loss_val=2.864851, ne_loss=0.031145, total=2.895996
Train Epoch: 2 [28288/18637 (76%)]	Instance Loss: 2.263284	Cluster Loss: 2.895996 	sec/iter: 0.9615
Sim min: 0.002212587045505643 Sim max: 1.0
ClusterLoss debug: loss_val=2.872952, ne_loss=0.049933, total=2.922884
Sim min: 0.0014940450200811028 Sim max: 1.0
ClusterLoss debug: loss_val=2.875942, ne_loss=0.106674, total=2.982616
Sim min: 0.0018542492762207985 Sim max: 1.0
ClusterLoss debug: loss_val=2.882776, ne_loss=0.089605, total=2.972381
Sim min: 0.0006601333734579384 Sim max: 1.0
ClusterLoss debug: loss_val=2.897897, ne_loss=0.133601, total=3.031498
Sim min: 0.0021995045244693756 Sim max: 1.0
ClusterLoss debug: loss_val=2.880298, ne_loss=0.098262, total=2.978560
Sim min: 0.0022941974457353354 Sim max: 1.0
ClusterLoss debug: loss_val=2.874628, ne_loss=0.078346, total=2.952974
Sim min: 0.0026887475978583097 Sim max: 1.0
ClusterLoss debug: loss_val=2.879182, ne_loss=0.067326, total=2.946508
Sim min: 0.0025936781894415617 Sim max: 1.0
ClusterLoss debug: loss_val=2.895690, ne_loss=0.083872, total=2.979562
Sim min: 0.0026241159066557884 Sim max: 1.0
ClusterLoss debug: loss_val=2.903473, ne_loss=0.085743, total=2.989216
Sim min: 0.0035994809586554766 Sim max: 1.0
ClusterLoss debug: loss_val=2.882994, ne_loss=0.086989, total=2.969983
Train Epoch: 2 [29568/18637 (79%)]	Instance Loss: 2.353641	Cluster Loss: 2.969983 	sec/iter: 0.9748
Sim min: 0.001997642917558551 Sim max: 1.0
ClusterLoss debug: loss_val=2.879304, ne_loss=0.090608, total=2.969912
Sim min: 0.0016567636048421264 Sim max: 1.0
ClusterLoss debug: loss_val=2.874730, ne_loss=0.049490, total=2.924220
Sim min: 0.0017007438000291586 Sim max: 1.0
ClusterLoss debug: loss_val=2.935857, ne_loss=0.151267, total=3.087124
Sim min: 0.0016076178289949894 Sim max: 1.0
ClusterLoss debug: loss_val=2.867528, ne_loss=0.050529, total=2.918057
Sim min: 0.001537992968223989 Sim max: 1.0
ClusterLoss debug: loss_val=2.870845, ne_loss=0.074153, total=2.944998
Sim min: 0.002100505633279681 Sim max: 1.0
ClusterLoss debug: loss_val=2.905475, ne_loss=0.087695, total=2.993170
Sim min: 0.000783948868047446 Sim max: 1.0
ClusterLoss debug: loss_val=2.882707, ne_loss=0.081479, total=2.964186
Sim min: 0.0031152828596532345 Sim max: 1.0
ClusterLoss debug: loss_val=2.905656, ne_loss=0.084840, total=2.990495
Sim min: 0.000583435466978699 Sim max: 1.0
ClusterLoss debug: loss_val=2.883330, ne_loss=0.121591, total=3.004921
Sim min: 0.0013237814418971539 Sim max: 1.0
ClusterLoss debug: loss_val=2.869587, ne_loss=0.138391, total=3.007978
Train Epoch: 2 [30848/18637 (83%)]	Instance Loss: 2.741343	Cluster Loss: 3.007978 	sec/iter: 0.9863
Sim min: 0.0010677214013412595 Sim max: 1.0
ClusterLoss debug: loss_val=2.863774, ne_loss=0.114333, total=2.978107
Sim min: 0.0014485681895166636 Sim max: 1.0
ClusterLoss debug: loss_val=2.881093, ne_loss=0.105761, total=2.986854
Sim min: 0.001186571316793561 Sim max: 1.0
ClusterLoss debug: loss_val=2.883212, ne_loss=0.150511, total=3.033723
Sim min: 0.001795770600438118 Sim max: 1.0
ClusterLoss debug: loss_val=2.893802, ne_loss=0.081530, total=2.975331
Sim min: 0.00146363431122154 Sim max: 1.0
ClusterLoss debug: loss_val=2.867313, ne_loss=0.073217, total=2.940530
Sim min: 0.0015270139556378126 Sim max: 1.0
ClusterLoss debug: loss_val=2.888905, ne_loss=0.175942, total=3.064847
Sim min: 0.0044523682445287704 Sim max: 1.0
ClusterLoss debug: loss_val=2.902197, ne_loss=0.066414, total=2.968611
Sim min: 0.0013536957558244467 Sim max: 1.0
ClusterLoss debug: loss_val=2.874993, ne_loss=0.086530, total=2.961524
Sim min: 0.0007197179365903139 Sim max: 1.0
ClusterLoss debug: loss_val=2.864952, ne_loss=0.121411, total=2.986363
Sim min: 0.0013506552204489708 Sim max: 1.0
ClusterLoss debug: loss_val=2.879664, ne_loss=0.069170, total=2.948834
Train Epoch: 2 [32128/18637 (86%)]	Instance Loss: 2.440843	Cluster Loss: 2.948834 	sec/iter: 0.9943
Sim min: 0.001001341617666185 Sim max: 1.0
ClusterLoss debug: loss_val=2.869966, ne_loss=0.091609, total=2.961574
Sim min: 0.001364559167996049 Sim max: 1.0
ClusterLoss debug: loss_val=2.885550, ne_loss=0.063704, total=2.949254
Sim min: 0.003427334362640977 Sim max: 1.0
ClusterLoss debug: loss_val=2.893800, ne_loss=0.064324, total=2.958124
Sim min: 0.002454013330861926 Sim max: 1.0
ClusterLoss debug: loss_val=2.910358, ne_loss=0.078430, total=2.988788
Sim min: 0.0027318214997649193 Sim max: 1.0
ClusterLoss debug: loss_val=2.895457, ne_loss=0.077412, total=2.972869
Sim min: 0.002880530431866646 Sim max: 1.0
ClusterLoss debug: loss_val=2.891222, ne_loss=0.091482, total=2.982704
Sim min: 0.0009630114072933793 Sim max: 1.0
ClusterLoss debug: loss_val=2.892946, ne_loss=0.095043, total=2.987988
Sim min: 0.0017370579298585653 Sim max: 1.0
ClusterLoss debug: loss_val=2.902521, ne_loss=0.119571, total=3.022092
Sim min: 0.0013310471549630165 Sim max: 1.0
ClusterLoss debug: loss_val=2.877892, ne_loss=0.087651, total=2.965542
Sim min: 0.003579049604013562 Sim max: 1.0
ClusterLoss debug: loss_val=2.890322, ne_loss=0.145602, total=3.035924
Train Epoch: 2 [33408/18637 (89%)]	Instance Loss: 2.384426	Cluster Loss: 3.035924 	sec/iter: 0.9961
Sim min: 0.0024029852356761694 Sim max: 1.0
ClusterLoss debug: loss_val=2.890968, ne_loss=0.253864, total=3.144832
Sim min: 0.0023454732727259398 Sim max: 1.0
ClusterLoss debug: loss_val=2.895635, ne_loss=0.143584, total=3.039220
Sim min: 0.001482963445596397 Sim max: 1.0
ClusterLoss debug: loss_val=2.915688, ne_loss=0.197120, total=3.112808
Sim min: 0.0036457283422350883 Sim max: 1.0
ClusterLoss debug: loss_val=2.891009, ne_loss=0.078675, total=2.969683
Sim min: 0.001827381202019751 Sim max: 1.0
ClusterLoss debug: loss_val=2.896022, ne_loss=0.088923, total=2.984945
Sim min: 0.0008482043049298227 Sim max: 1.0
ClusterLoss debug: loss_val=2.872436, ne_loss=0.050577, total=2.923013
Sim min: 0.0015865098685026169 Sim max: 1.0
ClusterLoss debug: loss_val=2.917382, ne_loss=0.112803, total=3.030185
Sim min: 0.0011130203492939472 Sim max: 1.0
ClusterLoss debug: loss_val=2.885411, ne_loss=0.120545, total=3.005956
Sim min: 0.0018402095884084702 Sim max: 1.0
ClusterLoss debug: loss_val=2.872874, ne_loss=0.095767, total=2.968640
Sim min: 0.0026462606620043516 Sim max: 1.0
ClusterLoss debug: loss_val=2.887668, ne_loss=0.117063, total=3.004731
Train Epoch: 2 [34688/18637 (93%)]	Instance Loss: 2.456692	Cluster Loss: 3.004731 	sec/iter: 1.0122
Sim min: 0.0011069343890994787 Sim max: 1.0
ClusterLoss debug: loss_val=2.886499, ne_loss=0.174923, total=3.061422
Sim min: 0.003813238115981221 Sim max: 1.0
ClusterLoss debug: loss_val=2.896044, ne_loss=0.102337, total=2.998380
Sim min: 0.000958712596911937 Sim max: 1.0
ClusterLoss debug: loss_val=2.883408, ne_loss=0.101867, total=2.985276
Sim min: 0.0016910885460674763 Sim max: 1.0
ClusterLoss debug: loss_val=2.913965, ne_loss=0.090969, total=3.004935
Sim min: 0.001350337639451027 Sim max: 1.0
ClusterLoss debug: loss_val=2.880303, ne_loss=0.077613, total=2.957916
Sim min: 0.0023079963866621256 Sim max: 1.0
ClusterLoss debug: loss_val=2.909012, ne_loss=0.138788, total=3.047801
Sim min: 0.0019258639076724648 Sim max: 1.0
ClusterLoss debug: loss_val=2.900160, ne_loss=0.107477, total=3.007637
Sim min: 0.0018556186696514487 Sim max: 1.0
ClusterLoss debug: loss_val=2.878269, ne_loss=0.124414, total=3.002683
Sim min: 0.001706679118797183 Sim max: 1.0
ClusterLoss debug: loss_val=2.877464, ne_loss=0.094042, total=2.971505
Sim min: 0.0016060058260336518 Sim max: 1.0
ClusterLoss debug: loss_val=2.887489, ne_loss=0.101358, total=2.988847
Train Epoch: 2 [35968/18637 (96%)]	Instance Loss: 2.838056	Cluster Loss: 2.988847 	sec/iter: 1.0369
Sim min: 0.0020491373725235462 Sim max: 1.0
ClusterLoss debug: loss_val=2.899766, ne_loss=0.065541, total=2.965307
Sim min: 0.001986046554520726 Sim max: 1.0
ClusterLoss debug: loss_val=2.895453, ne_loss=0.100360, total=2.995813
Sim min: 0.001275468384847045 Sim max: 1.0
ClusterLoss debug: loss_val=2.891134, ne_loss=0.138312, total=3.029446
Sim min: 0.0033610269892960787 Sim max: 1.0
ClusterLoss debug: loss_val=2.897714, ne_loss=0.124264, total=3.021978
Sim min: 0.0015014295931905508 Sim max: 1.0
ClusterLoss debug: loss_val=2.890613, ne_loss=0.113597, total=3.004210
Sim min: 0.0020106234587728977 Sim max: 1.0
ClusterLoss debug: loss_val=2.888590, ne_loss=0.044259, total=2.932849
Sim min: 0.0010799055453389883 Sim max: 1.0
ClusterLoss debug: loss_val=2.867954, ne_loss=0.082406, total=2.950360
Sim min: 0.0012407554313540459 Sim max: 1.0
ClusterLoss debug: loss_val=2.914130, ne_loss=0.077548, total=2.991678
Sim min: 0.0013996611814945936 Sim max: 1.0
ClusterLoss debug: loss_val=2.908829, ne_loss=0.108806, total=3.017635
Sim min: 0.001488624606281519 Sim max: 1.0
ClusterLoss debug: loss_val=2.879766, ne_loss=0.074129, total=2.953895
Train Epoch: 2 [37248/18637 (100%)]	Instance Loss: 2.552980	Cluster Loss: 2.953895 	sec/iter: 1.0513
Sim min: 0.0007565871928818524 Sim max: 1.0
ClusterLoss debug: loss_val=2.951395, ne_loss=0.399626, total=3.351021
Train Epoch: 2 [37274/18637 (100%)]	Instance Loss: 0.840675	Cluster Loss: 3.351021 	sec/iter: 1.0484
Model saved to model_epoch_2.pth
请输入数字1以继续运行: 
===== 分类评估 =====
Epoch 2 - Loss: 3.1009, Acc: 4.49%
Recall:     0.0800 0.0000 0.0000 0.0500 0.0000 0.0100 0.0000 0.0000 0.1400 0.2400 0.0000 0.0000 0.0000 0.0000 0.0000 0.0141 0.3900 0.0100 0.0000 0.0000 0.0000
Precision:  0.0365 0.0000 0.0000 0.0154 0.0000 0.0526 0.0000 0.0000 0.0588 0.2124 0.0000 0.0000 0.0000 0.0000 0.0000 0.0075 0.1105 0.0065 0.0000 0.0000 0.0000
F1-score:   0.0502 0.0000 0.0000 0.0236 0.0000 0.0168 0.0000 0.0000 0.0828 0.2254 0.0000 0.0000 0.0000 0.0000 0.0000 0.0098 0.1722 0.0079 0.0000 0.0000 0.0000

===== 聚类评估 =====
混淆矩阵:
 [[ 1  0  0  0  0  0  0  0  1  0  0  0  2  0  0 96  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  2  0  0 12 85  0  0  0  0]
 [ 1  0  1  0  0  0  0  1 17  0  0  0  5  0  0 75  0  0  0  0  0]
 [ 0  0  0  0  0  0 69  0  1  0  0  0  1  0  0 25  0  0  0  4  0]
 [ 2  0  0  0  0  0  0  0 61  0  0  0  2  1  0 34  0  0  0  0  0]
 [ 4  0  0  0  0  0  0  0  2  0  0  1  3  0  0 85  5  0  0  0  0]
 [ 0 22  0  0  0 12  1  0  0  0 37  0 21  0  3  3  0  0  0  0  1]
 [ 0  0  0 11  0  0  0  0  0  0  0  0  0 89  0  0  0  0  0  0  0]
 [ 2  0  0  0  1  0  0  0  0  0  0  0  5  0  0 86  6  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0 71  0  0  0 27  0  0  0  0  0]
 [ 0  0  0  0  1  0  0  0  0  1  0 58  1  0  0 39  0  0  0  0  0]
 [ 1  0 25  0  0  0  0  0  1  0  0  0 66  0  0  5  0  0  2  0  0]
 [ 0  0  0  0 49  0  0  0  0  0  0  0  1  0  0  8  0 42  0  0  0]
 [ 0  0  0  0  3  0  0  0  0 30  0  0  4  0  0 63  0  0  0  0  0]
 [ 0  0  0  0 63  0  0  0  0  2  0  0  0  0  0 34  0  1  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0 51  0  0 18  0  0  0  0  0]
 [95  0  0  0  0  0  0  0  0  0  0  0  2  0  0  2  1  0  0  0  0]
 [94  0  0  0  0  0  0  0  2  0  0  0  0  0  0  4  0  0  0  0  0]
 [43  0  0  0  0  0  0  0 54  0  0  0  1  0  0  2  0  0  0  0  0]
 [84  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0]
 [ 1  0  0  0  0  0  1 92  0  0  0  0  1  0  0  2  3  0  0  0  0]]
代价矩阵:
 [[327. 328. 327. 328. 326. 324. 328. 328. 326. 328. 328. 327. 328. 328.
  328. 328. 233. 234. 285. 244. 327.]
 [ 22.  22.  22.  22.  22.  22.   0.  22.  22.  22.  22.  22.  22.  22.
   22.  22.  22.  22.  22.  22.  22.]
 [ 26.  26.  25.  26.  26.  26.  26.  26.  26.  26.  26.   1.  26.  26.
   26.  26.  26.  26.  26.  26.  26.]
 [ 11.  11.  11.  11.  11.  11.  11.   0.  11.  11.  11.  11.  11.  11.
   11.  11.  11.  11.  11.  11.  11.]
 [117. 117. 117. 117. 117. 117. 117. 117. 116. 117. 116. 117.  68. 114.
   54. 117. 117. 117. 117. 117. 117.]
 [ 12.  12.  12.  12.  12.  12.   0.  12.  12.  12.  12.  12.  12.  12.
   12.  12.  12.  12.  12.  12.  12.]
 [ 71.  71.  71.   2.  71.  71.  70.  71.  71.  71.  71.  71.  71.  71.
   71.  71.  71.  71.  71.  71.  70.]
 [ 94.  93.  93.  94.  94.  94.  94.  94.  94.  94.  94.  94.  94.  94.
   94.  94.  94.  94.  94.  94.   2.]
 [142. 143. 126. 142.  82. 141. 143. 143. 143. 141. 143. 142. 143. 143.
  143. 141. 143. 141.  89. 143. 143.]
 [ 33.  33.  33.  33.  33.  33.  33.  33.  33.  33.  32.  33.  33.   3.
   31.  33.  33.  33.  33.  33.  33.]
 [ 37.  37.  37.  37.  37.  37.   0.  37.  37.  37.  37.  37.  37.  37.
   37.  37.  37.  37.  37.  37.  37.]
 [130. 130. 130. 130. 130. 129. 130. 130. 130.  59.  72. 130. 130. 130.
  130. 130. 130. 130. 130. 130. 130.]
 [166. 166. 163. 167. 166. 165. 147. 168. 163. 168. 167. 102. 167. 164.
  168. 117. 166. 168. 167. 168. 167.]
 [ 90.  90.  90.  90.  89.  90.  90.   1.  90.  90.  90.  90.  90.  90.
   90.  90.  90.  90.  90.  90.  90.]
 [  3.   3.   3.   3.   3.   3.   0.   3.   3.   3.   3.   3.   3.   3.
    3.   3.   3.   3.   3.   3.   3.]
 [540. 624. 561. 611. 602. 551. 633. 636. 550. 609. 597. 631. 628. 573.
  602. 618. 634. 632. 634. 620. 634.]
 [100.  15. 100. 100. 100.  95. 100. 100.  94. 100. 100. 100. 100. 100.
  100. 100.  99. 100. 100. 100.  97.]
 [ 43.  43.  43.  43.  43.  43.  43.  43.  43.  43.  43.  43.   1.  43.
   42.  43.  43.  43.  43.  43.  43.]
 [  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [  4.   4.   4.   0.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.
    4.   4.   4.   4.   4.   4.   4.]
 [  1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.]]
最佳映射关系: [16. 19. 11. 18. 14. 17.  3. 20.  4. 13.  6.  9. 15.  7.  2.  0.  1. 12.
 10.  8.  5.]
聚类分配的最大索引: 20

=== 实例级嵌入 ===
NMI: 0.6135  ARI: 0.2534
F-score: 0.3569  Silhouette: 0.2831
Adjusted Acc: 0.4375
混淆矩阵:
 [[ 0  1  0  0  0  0  2 27  0  4  1  0  4  0  0  1 32  0 28  0  0]
 [38  6  1  1  0  0  0  0  0  0  6  0  0  0 37  0  9  0  2  0  0]
 [ 0  3  3  2  0  0 21 14  9  0  8  0  0  2  0  0 30  0  2  4  2]
 [ 1  1 27 14  0  1  0  0  0  1  0  0  1  0  0  0  6  2  0 28 18]
 [ 0  1  0  1  0  0  9 14 33  0  8  0 21  0  0  0  3  0  8  2  0]
 [ 0  9  2  1  0  0 21  7  0 11  3  0  4  0  7  7 10  1 14  2  1]
 [ 0  0  2  0  0 38  0  0  0  0  0  0  0  0  0  0  0  7  0 30 23]
 [ 0  0  5 46  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0 37  1]
 [ 2 12  0  1 12  0  4 19  3  0  1  0  0  0  0  9  5  0 13  1 18]
 [ 0  1  0  0 31  0  0 22  0  0  0  0  0  0  0 35  1  0  0 10  0]
 [ 0  0  1  0  8  0  0  6  0  1  1  0  0  0  0 67 11  0  4  0  1]
 [ 0  1  5 24  0  0  0  0  0  0  0 27  0  0  0  0  1  0  0 16 26]
 [ 4  6  9  1  0  0  0  0  0  6  0  0  1  0 18  0  0 43  8  4  0]
 [ 0  1  0  2  0  0  3  3  0 52  2  0  4 15  0  0  5  0  8  1  4]
 [ 0  2  0  0  0  0 13  2  0 58  2  0  1  1  0  0  4  0  9  8  0]
 [ 0  2  1 20  0  0 12 18  0  1  1  0  0  0  0  0  0  0  1  3 12]
 [ 0 53  1  0  0  0  0 13  3  1 11  0  4  0  0  0 11  0  2  1  0]
 [ 0  2  0  0  0  0 21  7  1  3  4  0  5  2  0 29  8  0 17  1  0]
 [ 0  0  1  0  0  0 36  3 57  1  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  4  0  3  0  0 29  2  0  1  0  0 32  0  0  1  7  0 18  0  3]
 [31  1  1  1  0  0  0  0  1  8 35  0  0 22  0  0  0  0  0  0  0]]
代价矩阵:
 [[ 76.  38.  76.  75.  76.  76.  76.  76.  74.  76.  76.  76.  72.  76.
   76.  76.  76.  76.  76.  76.  45.]
 [105. 100. 103. 105. 105.  97. 106. 106.  94. 105. 106. 105. 100. 105.
  104. 104.  53. 104. 106. 102. 105.]
 [ 59.  58.  56.  32.  59.  57.  57.  54.  59.  59.  58.  54.  50.  59.
   59.  58.  58.  59.  58.  59.  58.]
 [117. 116. 115. 103. 116. 116. 117.  71. 116. 117. 117.  93. 116. 115.
  117.  97. 117. 117. 117. 114. 116.]
 [ 51.  51.  51.  51.  51.  51.  51.  51.  39.  20.  43.  51.  51.  51.
   51.  51.  51.  51.  51.  51.  51.]
 [ 39.  39.  39.  38.  39.  39.   1.  39.  39.  39.  39.  39.  39.  39.
   39.  39.  39.  39.  39.  39.  39.]
 [169. 171. 150. 171. 162. 150. 171. 171. 167. 171. 171. 171. 171. 168.
  158. 159. 171. 150. 135. 142. 171.]
 [130. 157. 143. 157. 143. 150. 157. 157. 138. 135. 151. 157. 157. 154.
  155. 139. 144. 150. 154. 155. 157.]
 [107. 107.  98. 107.  74. 107. 107. 107. 104. 107. 107. 107. 107. 107.
  107. 107. 104. 106.  50. 107. 106.]
 [144. 148. 148. 147. 148. 137. 148. 148. 148. 148. 147. 148. 142.  96.
   90. 147. 147. 145. 147. 147. 140.]
 [ 82.  77.  75.  83.  75.  80.  83.  83.  82.  83.  82.  83.  83.  81.
   81.  82.  72.  79.  83.  83.  48.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 73.  77.  77.  76.  56.  73.  77.  77.  77.  77.  77.  77.  76.  73.
   76.  77.  73.  72.  77.  45.  77.]
 [ 42.  42.  40.  42.  42.  42.  42.  42.  42.  42.  42.  42.  42.  27.
   41.  42.  42.  40.  42.  42.  20.]
 [ 62.  25.  62.  62.  62.  55.  62.  62.  62.  62.  62.  62.  44.  62.
   62.  62.  62.  62.  62.  62.  62.]
 [148. 149. 149. 149. 149. 142. 149. 149. 140. 114.  82. 149. 149. 149.
  149. 149. 149. 120. 149. 148. 149.]
 [113. 136. 115. 139. 142. 135. 145. 145. 140. 144. 134. 144. 145. 140.
  141. 145. 134. 137. 143. 138. 145.]
 [ 64.  64.  64.  62.  64.  63.  57.  53.  64.  64.  64.  64.  21.  64.
   64.  64.  64.  64.  64.  64.  64.]
 [106. 132. 132. 134. 126. 120. 134. 134. 121. 134. 130. 134. 126. 126.
  125. 133. 132. 117. 134. 116. 134.]
 [148. 148. 144. 120. 146. 146. 118. 111. 147. 138. 148. 132. 144. 147.
  140. 145. 147. 147. 148. 148. 148.]
 [109. 109. 107.  91. 109. 108.  86. 108.  91. 109. 108.  83. 109. 105.
  109.  97. 109. 109. 109. 106. 109.]]
最佳映射关系: [ 1. 16.  3. 15.  9.  6. 17.  4. 18. 14. 20. 11. 19. 13.  5. 10.  2. 12.
  0.  7.  8.]
聚类分配的最大索引: 20

=== 簇级嵌入 ===
NMI: 0.4376  ARI: 0.1960
F-score: 0.2383  Silhouette: 0.5421
Adjusted Acc: 0.3361

总耗时: 57.03s
分类准确率: 4.49%
簇级嵌入NMI: 0.4376
请输入数字2以继续运行: Sim min: 0.0030850728508085012 Sim max: 1.0
ClusterLoss debug: loss_val=2.881789, ne_loss=0.190884, total=3.072673
Train Epoch: 3 [128/18637 (0%)]	Instance Loss: 2.275083	Cluster Loss: 3.072673 	sec/iter: 1.4354
Sim min: 0.00213629100471735 Sim max: 1.0
ClusterLoss debug: loss_val=2.890550, ne_loss=0.094443, total=2.984993
Sim min: 0.0007994050392881036 Sim max: 1.0
ClusterLoss debug: loss_val=2.898989, ne_loss=0.188191, total=3.087180
Sim min: 0.0010827131336554885 Sim max: 1.0
ClusterLoss debug: loss_val=2.869694, ne_loss=0.115727, total=2.985422
Sim min: 0.004264038521796465 Sim max: 1.0
ClusterLoss debug: loss_val=2.923062, ne_loss=0.105793, total=3.028855
Sim min: 0.002247433178126812 Sim max: 1.0
ClusterLoss debug: loss_val=2.880659, ne_loss=0.105208, total=2.985868
Sim min: 0.0020398253109306097 Sim max: 1.0
ClusterLoss debug: loss_val=2.876063, ne_loss=0.094306, total=2.970369
Sim min: 0.002737987320870161 Sim max: 1.0
ClusterLoss debug: loss_val=2.881924, ne_loss=0.072575, total=2.954499
Sim min: 0.0025936674792319536 Sim max: 1.0
ClusterLoss debug: loss_val=2.903768, ne_loss=0.075692, total=2.979460
Sim min: 0.002106522209942341 Sim max: 1.0
ClusterLoss debug: loss_val=2.873686, ne_loss=0.082044, total=2.955730
Sim min: 0.0007859091856516898 Sim max: 1.0
ClusterLoss debug: loss_val=2.868888, ne_loss=0.161597, total=3.030484
Train Epoch: 3 [1408/18637 (4%)]	Instance Loss: 2.187721	Cluster Loss: 3.030484 	sec/iter: 1.4112
Sim min: 0.0027681950014084578 Sim max: 1.0
ClusterLoss debug: loss_val=2.890017, ne_loss=0.130485, total=3.020502
Sim min: 0.004753593355417252 Sim max: 1.0
ClusterLoss debug: loss_val=2.880344, ne_loss=0.145490, total=3.025834
Sim min: 0.004942459985613823 Sim max: 1.0
ClusterLoss debug: loss_val=2.882607, ne_loss=0.063128, total=2.945735
Sim min: 0.0025362407322973013 Sim max: 1.0
ClusterLoss debug: loss_val=2.887079, ne_loss=0.156215, total=3.043294
Sim min: 0.0027960254810750484 Sim max: 1.0
ClusterLoss debug: loss_val=2.908556, ne_loss=0.266595, total=3.175151
Sim min: 0.0009817661484703422 Sim max: 1.0
ClusterLoss debug: loss_val=2.885015, ne_loss=0.106981, total=2.991996
Sim min: 0.0022908931132405996 Sim max: 1.0
ClusterLoss debug: loss_val=2.882269, ne_loss=0.075345, total=2.957614
Sim min: 0.003088545985519886 Sim max: 1.0
ClusterLoss debug: loss_val=2.884027, ne_loss=0.077933, total=2.961960
Sim min: 0.002114905510097742 Sim max: 1.0
ClusterLoss debug: loss_val=2.877083, ne_loss=0.111361, total=2.988443
Sim min: 0.001833826070651412 Sim max: 1.0
ClusterLoss debug: loss_val=2.886420, ne_loss=0.163772, total=3.050192
Train Epoch: 3 [2688/18637 (7%)]	Instance Loss: 2.538195	Cluster Loss: 3.050192 	sec/iter: 1.1819
Sim min: 0.0021317852661013603 Sim max: 1.0
ClusterLoss debug: loss_val=2.875567, ne_loss=0.091345, total=2.966913
Sim min: 0.0021165222860872746 Sim max: 1.0
ClusterLoss debug: loss_val=2.876512, ne_loss=0.076217, total=2.952729
Sim min: 0.0026141915004700422 Sim max: 1.0
ClusterLoss debug: loss_val=2.896531, ne_loss=0.069794, total=2.966325
Sim min: 0.0007854358991608024 Sim max: 1.0
ClusterLoss debug: loss_val=2.867621, ne_loss=0.117744, total=2.985364
Sim min: 0.0010030966950580478 Sim max: 1.0
ClusterLoss debug: loss_val=2.861888, ne_loss=0.065426, total=2.927314
Sim min: 0.001596743706613779 Sim max: 1.0
ClusterLoss debug: loss_val=2.898785, ne_loss=0.111537, total=3.010322
Sim min: 0.002865756396204233 Sim max: 1.0
ClusterLoss debug: loss_val=2.872633, ne_loss=0.040009, total=2.912642
Sim min: 0.0013353517279028893 Sim max: 1.0
ClusterLoss debug: loss_val=2.884701, ne_loss=0.128977, total=3.013678
Sim min: 0.0030205890070647 Sim max: 1.0
ClusterLoss debug: loss_val=2.877456, ne_loss=0.101720, total=2.979176
Sim min: 0.0017534756334498525 Sim max: 1.0
ClusterLoss debug: loss_val=2.880701, ne_loss=0.119321, total=3.000022
Train Epoch: 3 [3968/18637 (11%)]	Instance Loss: 1.973350	Cluster Loss: 3.000022 	sec/iter: 1.0766
Sim min: 0.0030810637399554253 Sim max: 1.0
ClusterLoss debug: loss_val=2.876235, ne_loss=0.049763, total=2.925998
Sim min: 0.0014021798269823194 Sim max: 1.0
ClusterLoss debug: loss_val=2.863541, ne_loss=0.089829, total=2.953370
Sim min: 0.0008709539542905986 Sim max: 1.0
ClusterLoss debug: loss_val=2.866806, ne_loss=0.125702, total=2.992507
Sim min: 0.002986546605825424 Sim max: 1.0
ClusterLoss debug: loss_val=2.896106, ne_loss=0.118067, total=3.014173
Sim min: 0.0012799625983461738 Sim max: 1.0
ClusterLoss debug: loss_val=2.902209, ne_loss=0.116385, total=3.018594
Sim min: 0.002030828967690468 Sim max: 1.0
ClusterLoss debug: loss_val=2.867644, ne_loss=0.201083, total=3.068727
Sim min: 0.001614292967133224 Sim max: 1.0
ClusterLoss debug: loss_val=2.881668, ne_loss=0.105836, total=2.987504
Sim min: 0.003121845191344619 Sim max: 1.0
ClusterLoss debug: loss_val=2.894242, ne_loss=0.058653, total=2.952895
Sim min: 0.0023458267096430063 Sim max: 1.0
ClusterLoss debug: loss_val=2.882625, ne_loss=0.076121, total=2.958746
Sim min: 0.0020680970046669245 Sim max: 1.0
ClusterLoss debug: loss_val=2.878634, ne_loss=0.089468, total=2.968102
Train Epoch: 3 [5248/18637 (14%)]	Instance Loss: 2.201424	Cluster Loss: 2.968102 	sec/iter: 1.0129
Sim min: 0.0012806284939870238 Sim max: 1.0
ClusterLoss debug: loss_val=2.887841, ne_loss=0.052022, total=2.939864
Sim min: 0.0024301784578710794 Sim max: 1.0
ClusterLoss debug: loss_val=2.876417, ne_loss=0.072572, total=2.948989
Sim min: 0.001368907862342894 Sim max: 1.0
ClusterLoss debug: loss_val=2.863985, ne_loss=0.056206, total=2.920191
Sim min: 0.003202477702870965 Sim max: 1.0
ClusterLoss debug: loss_val=2.872801, ne_loss=0.062509, total=2.935310
Sim min: 0.0020481611136347055 Sim max: 1.0
ClusterLoss debug: loss_val=2.910100, ne_loss=0.042829, total=2.952929
Sim min: 0.0014097634702920914 Sim max: 1.0
ClusterLoss debug: loss_val=2.878219, ne_loss=0.137267, total=3.015487
Sim min: 0.002045250963419676 Sim max: 1.0
ClusterLoss debug: loss_val=2.904491, ne_loss=0.090323, total=2.994815
Sim min: 0.0013645899016410112 Sim max: 1.0
ClusterLoss debug: loss_val=2.876271, ne_loss=0.167154, total=3.043425
Sim min: 0.001390438643284142 Sim max: 1.0
ClusterLoss debug: loss_val=2.891223, ne_loss=0.161505, total=3.052728
Sim min: 0.0023111600894480944 Sim max: 1.0
ClusterLoss debug: loss_val=2.881578, ne_loss=0.135572, total=3.017151
Train Epoch: 3 [6528/18637 (17%)]	Instance Loss: 2.263489	Cluster Loss: 3.017151 	sec/iter: 0.9767
Sim min: 0.0024193241260945797 Sim max: 1.0
ClusterLoss debug: loss_val=2.880370, ne_loss=0.113757, total=2.994128
Sim min: 0.001465348294004798 Sim max: 1.0
ClusterLoss debug: loss_val=2.871042, ne_loss=0.081409, total=2.952451
Sim min: 0.0024457944091409445 Sim max: 1.0
ClusterLoss debug: loss_val=2.869803, ne_loss=0.024597, total=2.894399
Sim min: 0.0015398156829178333 Sim max: 1.0
ClusterLoss debug: loss_val=2.873939, ne_loss=0.172158, total=3.046096
Sim min: 0.003634546883404255 Sim max: 1.0
ClusterLoss debug: loss_val=2.881830, ne_loss=0.153553, total=3.035383
Sim min: 0.0013937129406258464 Sim max: 1.0
ClusterLoss debug: loss_val=2.879056, ne_loss=0.120864, total=2.999920
Sim min: 0.0013324272586032748 Sim max: 1.0
ClusterLoss debug: loss_val=2.893530, ne_loss=0.224183, total=3.117713
Sim min: 0.0012506318744271994 Sim max: 1.0
ClusterLoss debug: loss_val=2.870136, ne_loss=0.151356, total=3.021492
Sim min: 0.0020121054258197546 Sim max: 1.0
ClusterLoss debug: loss_val=2.880276, ne_loss=0.098082, total=2.978359
Sim min: 0.0016321003204211593 Sim max: 1.0
ClusterLoss debug: loss_val=2.874445, ne_loss=0.113735, total=2.988180
Train Epoch: 3 [7808/18637 (21%)]	Instance Loss: 2.312746	Cluster Loss: 2.988180 	sec/iter: 0.9587
Sim min: 0.0013711329083889723 Sim max: 1.0
ClusterLoss debug: loss_val=2.889384, ne_loss=0.140540, total=3.029924
Sim min: 0.0010109605500474572 Sim max: 1.0
ClusterLoss debug: loss_val=2.874751, ne_loss=0.059292, total=2.934043
Sim min: 0.0005950977792963386 Sim max: 1.0
ClusterLoss debug: loss_val=2.890455, ne_loss=0.176353, total=3.066808
Sim min: 0.001377986161969602 Sim max: 1.0
ClusterLoss debug: loss_val=2.871001, ne_loss=0.127992, total=2.998993
Sim min: 0.002202001167461276 Sim max: 1.0
ClusterLoss debug: loss_val=2.873408, ne_loss=0.060644, total=2.934052
Sim min: 0.0013016110751777887 Sim max: 1.0
ClusterLoss debug: loss_val=2.873658, ne_loss=0.144475, total=3.018132
Sim min: 0.0016002263873815536 Sim max: 1.0
ClusterLoss debug: loss_val=2.890694, ne_loss=0.085048, total=2.975742
Sim min: 0.0016346046468243003 Sim max: 1.0
ClusterLoss debug: loss_val=2.875880, ne_loss=0.133257, total=3.009137
Sim min: 0.0010713515803217888 Sim max: 1.0
ClusterLoss debug: loss_val=2.880492, ne_loss=0.142993, total=3.023486
Sim min: 0.002068069763481617 Sim max: 1.0
ClusterLoss debug: loss_val=2.883866, ne_loss=0.148885, total=3.032752
Train Epoch: 3 [9088/18637 (24%)]	Instance Loss: 1.978664	Cluster Loss: 3.032752 	sec/iter: 0.9381
Sim min: 0.0011982147116214037 Sim max: 1.0
ClusterLoss debug: loss_val=2.874817, ne_loss=0.129498, total=3.004315
Sim min: 0.0013116155751049519 Sim max: 1.0
ClusterLoss debug: loss_val=2.872287, ne_loss=0.143040, total=3.015327
Sim min: 0.0013291692594066262 Sim max: 1.0
ClusterLoss debug: loss_val=2.879255, ne_loss=0.062595, total=2.941850
Sim min: 0.002068960340693593 Sim max: 1.0
ClusterLoss debug: loss_val=2.891374, ne_loss=0.111674, total=3.003048
Sim min: 0.0012859056005254388 Sim max: 1.0
ClusterLoss debug: loss_val=2.862987, ne_loss=0.071092, total=2.934079
Sim min: 0.0013761769514530897 Sim max: 1.0
ClusterLoss debug: loss_val=2.886933, ne_loss=0.165819, total=3.052752
Sim min: 0.0006915719714015722 Sim max: 1.0
ClusterLoss debug: loss_val=2.887386, ne_loss=0.111348, total=2.998734
Sim min: 0.002895354526117444 Sim max: 1.0
ClusterLoss debug: loss_val=2.880680, ne_loss=0.074811, total=2.955492
Sim min: 0.004356924910098314 Sim max: 1.0
ClusterLoss debug: loss_val=2.889074, ne_loss=0.125366, total=3.014440
Sim min: 0.0020515015348792076 Sim max: 1.0
ClusterLoss debug: loss_val=2.887755, ne_loss=0.130279, total=3.018035
Train Epoch: 3 [10368/18637 (28%)]	Instance Loss: 2.103556	Cluster Loss: 3.018035 	sec/iter: 0.9205
Sim min: 0.0008483669371344149 Sim max: 1.0
ClusterLoss debug: loss_val=2.868707, ne_loss=0.036091, total=2.904799
Sim min: 0.004446505568921566 Sim max: 1.0
ClusterLoss debug: loss_val=2.898338, ne_loss=0.104584, total=3.002922
Sim min: 0.0012370473705232143 Sim max: 1.0
ClusterLoss debug: loss_val=2.858833, ne_loss=0.130277, total=2.989110
Sim min: 0.0023038347717374563 Sim max: 1.0
ClusterLoss debug: loss_val=2.876075, ne_loss=0.143869, total=3.019944
Sim min: 0.0015074892435222864 Sim max: 1.0
ClusterLoss debug: loss_val=2.867369, ne_loss=0.071635, total=2.939004
Sim min: 0.0029484881088137627 Sim max: 1.0
ClusterLoss debug: loss_val=2.876155, ne_loss=0.122009, total=2.998164
Sim min: 0.0024984136689454317 Sim max: 1.0
ClusterLoss debug: loss_val=2.901467, ne_loss=0.191099, total=3.092566
Sim min: 0.001646258868277073 Sim max: 1.0
ClusterLoss debug: loss_val=2.869874, ne_loss=0.103774, total=2.973648
Sim min: 0.0011376702459529042 Sim max: 1.0
ClusterLoss debug: loss_val=2.888752, ne_loss=0.151903, total=3.040655
Sim min: 0.0039241560734808445 Sim max: 1.0
ClusterLoss debug: loss_val=2.892268, ne_loss=0.096598, total=2.988866
Train Epoch: 3 [11648/18637 (31%)]	Instance Loss: 2.325357	Cluster Loss: 2.988866 	sec/iter: 0.9044
Sim min: 0.0011340891942381859 Sim max: 1.0
ClusterLoss debug: loss_val=2.876441, ne_loss=0.076401, total=2.952842
Sim min: 0.002195817418396473 Sim max: 1.0
ClusterLoss debug: loss_val=2.880620, ne_loss=0.107097, total=2.987717
Sim min: 0.0007196941878646612 Sim max: 1.0
ClusterLoss debug: loss_val=2.866602, ne_loss=0.081587, total=2.948189
Sim min: 0.0017313997959718108 Sim max: 1.0
ClusterLoss debug: loss_val=2.864252, ne_loss=0.094621, total=2.958873
Sim min: 0.0013354156399145722 Sim max: 1.0
ClusterLoss debug: loss_val=2.873926, ne_loss=0.122005, total=2.995931
Sim min: 0.0018971222452819347 Sim max: 1.0
ClusterLoss debug: loss_val=2.887386, ne_loss=0.113614, total=3.000999
Sim min: 0.002765212906524539 Sim max: 1.0
ClusterLoss debug: loss_val=2.899850, ne_loss=0.061301, total=2.961151
Sim min: 0.0024130435194820166 Sim max: 1.0
ClusterLoss debug: loss_val=2.872718, ne_loss=0.083843, total=2.956561
Sim min: 0.0009260494261980057 Sim max: 1.0
ClusterLoss debug: loss_val=2.901018, ne_loss=0.097618, total=2.998636
Sim min: 0.0010546771809458733 Sim max: 1.0
ClusterLoss debug: loss_val=2.867945, ne_loss=0.095639, total=2.963584
Train Epoch: 3 [12928/18637 (35%)]	Instance Loss: 2.277909	Cluster Loss: 2.963584 	sec/iter: 0.8915
Sim min: 0.00182643486186862 Sim max: 1.0
ClusterLoss debug: loss_val=2.873968, ne_loss=0.099274, total=2.973242
Sim min: 0.00141704804264009 Sim max: 1.0
ClusterLoss debug: loss_val=2.909555, ne_loss=0.156263, total=3.065818
Sim min: 0.001382954535074532 Sim max: 1.0
ClusterLoss debug: loss_val=2.878273, ne_loss=0.156539, total=3.034812
Sim min: 0.0025430573150515556 Sim max: 1.0
ClusterLoss debug: loss_val=2.917998, ne_loss=0.311257, total=3.229254
Sim min: 0.0014716199366375804 Sim max: 1.0
ClusterLoss debug: loss_val=2.889495, ne_loss=0.158019, total=3.047513
Sim min: 0.002343983156606555 Sim max: 1.0
ClusterLoss debug: loss_val=2.876553, ne_loss=0.117767, total=2.994320
Sim min: 0.0017209547804668546 Sim max: 1.0
ClusterLoss debug: loss_val=2.891533, ne_loss=0.148343, total=3.039875
Sim min: 0.0014902726979926229 Sim max: 1.0
ClusterLoss debug: loss_val=2.877581, ne_loss=0.201308, total=3.078889
Sim min: 0.0018939413130283356 Sim max: 1.0
ClusterLoss debug: loss_val=2.895111, ne_loss=0.101231, total=2.996342
Sim min: 0.0024080469738692045 Sim max: 1.0
ClusterLoss debug: loss_val=2.884049, ne_loss=0.084382, total=2.968431
Train Epoch: 3 [14208/18637 (38%)]	Instance Loss: 2.114304	Cluster Loss: 2.968431 	sec/iter: 0.8848
Sim min: 0.0020775552839040756 Sim max: 1.0
ClusterLoss debug: loss_val=2.885631, ne_loss=0.056100, total=2.941730
Sim min: 0.0014023584080860019 Sim max: 1.0
ClusterLoss debug: loss_val=2.874034, ne_loss=0.076560, total=2.950594
Sim min: 0.0009319421369582415 Sim max: 1.0
ClusterLoss debug: loss_val=2.879668, ne_loss=0.141398, total=3.021066
Sim min: 0.0008578798733651638 Sim max: 1.0
ClusterLoss debug: loss_val=2.875163, ne_loss=0.144407, total=3.019570
Sim min: 0.0015330929309129715 Sim max: 1.0
ClusterLoss debug: loss_val=2.861839, ne_loss=0.143095, total=3.004935
Sim min: 0.0013412517728284001 Sim max: 1.0
ClusterLoss debug: loss_val=2.888911, ne_loss=0.074759, total=2.963671
Sim min: 0.0011686960933730006 Sim max: 1.0
ClusterLoss debug: loss_val=2.897418, ne_loss=0.068037, total=2.965455
Sim min: 0.0016353446990251541 Sim max: 1.0
ClusterLoss debug: loss_val=2.887459, ne_loss=0.061305, total=2.948763
Sim min: 0.0012707130517810583 Sim max: 1.0
ClusterLoss debug: loss_val=2.889036, ne_loss=0.087541, total=2.976577
Sim min: 0.0013051626738160849 Sim max: 1.0
ClusterLoss debug: loss_val=2.881852, ne_loss=0.186416, total=3.068268
Train Epoch: 3 [15488/18637 (41%)]	Instance Loss: 2.242569	Cluster Loss: 3.068268 	sec/iter: 0.8779
Sim min: 0.002304117428138852 Sim max: 1.0
ClusterLoss debug: loss_val=2.884974, ne_loss=0.121501, total=3.006475
Sim min: 0.0008427015854977071 Sim max: 1.0
ClusterLoss debug: loss_val=2.888405, ne_loss=0.119462, total=3.007867
Sim min: 0.000895023753400892 Sim max: 1.0
ClusterLoss debug: loss_val=2.885223, ne_loss=0.130303, total=3.015527
Sim min: 0.0027427268214523792 Sim max: 1.0
ClusterLoss debug: loss_val=2.882092, ne_loss=0.071854, total=2.953945
Sim min: 0.0031526328530162573 Sim max: 1.0
ClusterLoss debug: loss_val=2.909185, ne_loss=0.161324, total=3.070508
Sim min: 0.002093696966767311 Sim max: 1.0
ClusterLoss debug: loss_val=2.896115, ne_loss=0.165627, total=3.061742
Sim min: 0.001524807303212583 Sim max: 1.0
ClusterLoss debug: loss_val=2.877662, ne_loss=0.070481, total=2.948142
Sim min: 0.002370137255638838 Sim max: 1.0
ClusterLoss debug: loss_val=2.883081, ne_loss=0.081530, total=2.964611
Sim min: 0.00202267081476748 Sim max: 1.0
ClusterLoss debug: loss_val=2.909369, ne_loss=0.089906, total=2.999275
Sim min: 0.0018899965798482299 Sim max: 1.0
ClusterLoss debug: loss_val=2.874592, ne_loss=0.101833, total=2.976425
Train Epoch: 3 [16768/18637 (45%)]	Instance Loss: 2.155641	Cluster Loss: 2.976425 	sec/iter: 0.8711
Sim min: 0.0021535593550652266 Sim max: 1.0
ClusterLoss debug: loss_val=2.891724, ne_loss=0.099509, total=2.991234
Sim min: 0.0015691418666392565 Sim max: 1.0
ClusterLoss debug: loss_val=2.878151, ne_loss=0.087637, total=2.965788
Sim min: 0.0028335056267678738 Sim max: 1.0
ClusterLoss debug: loss_val=2.876905, ne_loss=0.097656, total=2.974561
Sim min: 0.0019205124117434025 Sim max: 1.0
ClusterLoss debug: loss_val=2.871499, ne_loss=0.089302, total=2.960800
Sim min: 0.0009305145358666778 Sim max: 1.0
ClusterLoss debug: loss_val=2.878839, ne_loss=0.140144, total=3.018983
Sim min: 0.0012494416441768408 Sim max: 1.0
ClusterLoss debug: loss_val=2.878382, ne_loss=0.104672, total=2.983054
Sim min: 0.0010591604514047503 Sim max: 1.0
ClusterLoss debug: loss_val=2.894307, ne_loss=0.148129, total=3.042435
Sim min: 0.0013621438993141055 Sim max: 1.0
ClusterLoss debug: loss_val=2.893567, ne_loss=0.190382, total=3.083949
Sim min: 0.0009549646056257188 Sim max: 1.0
ClusterLoss debug: loss_val=2.870702, ne_loss=0.195681, total=3.066383
Sim min: 0.002084923442453146 Sim max: 1.0
ClusterLoss debug: loss_val=2.878169, ne_loss=0.148777, total=3.026946
Train Epoch: 3 [18048/18637 (48%)]	Instance Loss: 2.260429	Cluster Loss: 3.026946 	sec/iter: 0.8630
Sim min: 0.003704192116856575 Sim max: 1.0
ClusterLoss debug: loss_val=2.870120, ne_loss=0.102227, total=2.972347
Sim min: 0.0011281849583610892 Sim max: 1.0
ClusterLoss debug: loss_val=2.891130, ne_loss=0.145195, total=3.036325
Sim min: 0.0018181252526119351 Sim max: 1.0
ClusterLoss debug: loss_val=2.904044, ne_loss=0.097166, total=3.001209
Sim min: 0.0015368303284049034 Sim max: 1.0
ClusterLoss debug: loss_val=2.911670, ne_loss=0.233623, total=3.145293
Sim min: 0.001902722637169063 Sim max: 1.0
ClusterLoss debug: loss_val=2.883994, ne_loss=0.132177, total=3.016171
Sim min: 0.00283172819763422 Sim max: 1.0
ClusterLoss debug: loss_val=2.899132, ne_loss=0.189991, total=3.089123
Sim min: 0.001381221110932529 Sim max: 1.0
ClusterLoss debug: loss_val=2.898386, ne_loss=0.046623, total=2.945009
Sim min: 0.001600314280949533 Sim max: 1.0
ClusterLoss debug: loss_val=2.876552, ne_loss=0.041218, total=2.917770
Sim min: 0.002028390299528837 Sim max: 1.0
ClusterLoss debug: loss_val=2.900357, ne_loss=0.103665, total=3.004022
Sim min: 0.002583731198683381 Sim max: 1.0
ClusterLoss debug: loss_val=2.887067, ne_loss=0.121276, total=3.008343
Train Epoch: 3 [19328/18637 (52%)]	Instance Loss: 2.279065	Cluster Loss: 3.008343 	sec/iter: 0.8587
Sim min: 0.0006723476690240204 Sim max: 1.0
ClusterLoss debug: loss_val=2.870729, ne_loss=0.071683, total=2.942412
Sim min: 0.00248628668487072 Sim max: 1.0
ClusterLoss debug: loss_val=2.910489, ne_loss=0.109625, total=3.020114
Sim min: 0.0015357447555288672 Sim max: 1.0
ClusterLoss debug: loss_val=2.901208, ne_loss=0.136282, total=3.037490
Sim min: 0.0006761823897249997 Sim max: 1.0
ClusterLoss debug: loss_val=2.880431, ne_loss=0.093175, total=2.973606
Sim min: 0.0014198842691257596 Sim max: 1.0
ClusterLoss debug: loss_val=2.897009, ne_loss=0.151710, total=3.048719
Sim min: 0.00280081108212471 Sim max: 1.0
ClusterLoss debug: loss_val=2.901043, ne_loss=0.057487, total=2.958529
Sim min: 0.0019545999821275473 Sim max: 1.0
ClusterLoss debug: loss_val=2.910994, ne_loss=0.127685, total=3.038679
Sim min: 0.001247560023330152 Sim max: 1.0
ClusterLoss debug: loss_val=2.890676, ne_loss=0.118822, total=3.009498
Sim min: 0.001204446773044765 Sim max: 1.0
ClusterLoss debug: loss_val=2.901627, ne_loss=0.137671, total=3.039298
Sim min: 0.0010345132322981954 Sim max: 1.0
ClusterLoss debug: loss_val=2.868185, ne_loss=0.073630, total=2.941815
Train Epoch: 3 [20608/18637 (55%)]	Instance Loss: 2.278443	Cluster Loss: 2.941815 	sec/iter: 0.8537
Sim min: 0.00158945273142308 Sim max: 1.0
ClusterLoss debug: loss_val=2.873408, ne_loss=0.077141, total=2.950548
Sim min: 0.0011368173873052 Sim max: 1.0
ClusterLoss debug: loss_val=2.885693, ne_loss=0.076381, total=2.962075
Sim min: 0.0008218474686145782 Sim max: 1.0
ClusterLoss debug: loss_val=2.886772, ne_loss=0.053610, total=2.940382
Sim min: 0.0031946415547281504 Sim max: 1.0
ClusterLoss debug: loss_val=2.889530, ne_loss=0.111041, total=3.000571
Sim min: 0.0019905222579836845 Sim max: 1.0
ClusterLoss debug: loss_val=2.873127, ne_loss=0.097259, total=2.970386
Sim min: 0.000976579962298274 Sim max: 1.0
ClusterLoss debug: loss_val=2.886636, ne_loss=0.124902, total=3.011538
Sim min: 0.0012240345822647214 Sim max: 1.0
ClusterLoss debug: loss_val=2.878580, ne_loss=0.075954, total=2.954533
Sim min: 0.001505754073150456 Sim max: 1.0
ClusterLoss debug: loss_val=2.889143, ne_loss=0.105494, total=2.994637
Sim min: 0.0013380569871515036 Sim max: 1.0
ClusterLoss debug: loss_val=2.877338, ne_loss=0.039168, total=2.916506
Sim min: 0.0009278418147005141 Sim max: 1.0
ClusterLoss debug: loss_val=2.872646, ne_loss=0.059809, total=2.932456
Train Epoch: 3 [21888/18637 (59%)]	Instance Loss: 2.403417	Cluster Loss: 2.932456 	sec/iter: 0.8535
Sim min: 0.000867081864271313 Sim max: 1.0
ClusterLoss debug: loss_val=2.878486, ne_loss=0.101267, total=2.979753
Sim min: 0.0012318575754761696 Sim max: 1.0
ClusterLoss debug: loss_val=2.878589, ne_loss=0.056329, total=2.934918
Sim min: 0.005031963344663382 Sim max: 1.0
ClusterLoss debug: loss_val=2.892799, ne_loss=0.132463, total=3.025262
Sim min: 0.0017859938088804483 Sim max: 1.0
ClusterLoss debug: loss_val=2.887418, ne_loss=0.135121, total=3.022538
Sim min: 0.003253570757806301 Sim max: 1.0
ClusterLoss debug: loss_val=2.886563, ne_loss=0.066163, total=2.952726
Sim min: 0.0013537649065256119 Sim max: 1.0
ClusterLoss debug: loss_val=2.866399, ne_loss=0.049979, total=2.916378
Sim min: 0.003246013540774584 Sim max: 1.0
ClusterLoss debug: loss_val=2.895879, ne_loss=0.069288, total=2.965167
Sim min: 0.0015434575034305453 Sim max: 1.0
ClusterLoss debug: loss_val=2.874574, ne_loss=0.117286, total=2.991861
Sim min: 0.0017725310754030943 Sim max: 1.0
ClusterLoss debug: loss_val=2.899099, ne_loss=0.145375, total=3.044474
Sim min: 0.0020820065401494503 Sim max: 1.0
ClusterLoss debug: loss_val=2.858515, ne_loss=0.104341, total=2.962856
Train Epoch: 3 [23168/18637 (62%)]	Instance Loss: 2.223034	Cluster Loss: 2.962856 	sec/iter: 0.8514
Sim min: 0.0018312707543373108 Sim max: 1.0
ClusterLoss debug: loss_val=2.894446, ne_loss=0.049177, total=2.943623
Sim min: 0.003515502903610468 Sim max: 1.0
ClusterLoss debug: loss_val=2.905198, ne_loss=0.107040, total=3.012238
Sim min: 0.0018939871806651354 Sim max: 1.0
ClusterLoss debug: loss_val=2.880781, ne_loss=0.091773, total=2.972554
Sim min: 0.000844139140099287 Sim max: 1.0
ClusterLoss debug: loss_val=2.875029, ne_loss=0.123478, total=2.998507
Sim min: 0.00335863558575511 Sim max: 1.0
ClusterLoss debug: loss_val=2.885754, ne_loss=0.131719, total=3.017473
Sim min: 0.0018335962668061256 Sim max: 1.0
ClusterLoss debug: loss_val=2.874342, ne_loss=0.085231, total=2.959573
Sim min: 0.0017275692662224174 Sim max: 1.0
ClusterLoss debug: loss_val=2.888855, ne_loss=0.130375, total=3.019231
Sim min: 0.0015096385031938553 Sim max: 1.0
ClusterLoss debug: loss_val=2.875185, ne_loss=0.101689, total=2.976873
Sim min: 0.0006775278015993536 Sim max: 1.0
ClusterLoss debug: loss_val=2.863293, ne_loss=0.058047, total=2.921340
Sim min: 0.0013089877320453525 Sim max: 1.0
ClusterLoss debug: loss_val=2.924973, ne_loss=0.161687, total=3.086659
Train Epoch: 3 [24448/18637 (65%)]	Instance Loss: 2.158914	Cluster Loss: 3.086659 	sec/iter: 0.8473
Sim min: 0.0014879564987495542 Sim max: 1.0
ClusterLoss debug: loss_val=2.885366, ne_loss=0.081560, total=2.966927
Sim min: 0.0008865154813975096 Sim max: 1.0
ClusterLoss debug: loss_val=2.876117, ne_loss=0.124392, total=3.000509
Sim min: 0.0016537616029381752 Sim max: 1.0
ClusterLoss debug: loss_val=2.885107, ne_loss=0.233497, total=3.118604
Sim min: 0.0020412281155586243 Sim max: 1.0
ClusterLoss debug: loss_val=2.885441, ne_loss=0.111613, total=2.997054
Sim min: 0.0015116087161004543 Sim max: 1.0
ClusterLoss debug: loss_val=2.864097, ne_loss=0.153293, total=3.017390
Sim min: 0.001669234363362193 Sim max: 1.0
ClusterLoss debug: loss_val=2.889953, ne_loss=0.128420, total=3.018373
Sim min: 0.0009648413397371769 Sim max: 1.0
ClusterLoss debug: loss_val=2.863896, ne_loss=0.106585, total=2.970481
Sim min: 0.0008536208770237863 Sim max: 1.0
ClusterLoss debug: loss_val=2.888689, ne_loss=0.173380, total=3.062069
Sim min: 0.0012043516617268324 Sim max: 1.0
ClusterLoss debug: loss_val=2.858179, ne_loss=0.075951, total=2.934130
Sim min: 0.001405879040248692 Sim max: 1.0
ClusterLoss debug: loss_val=2.896981, ne_loss=0.118448, total=3.015429
Train Epoch: 3 [25728/18637 (69%)]	Instance Loss: 1.963649	Cluster Loss: 3.015429 	sec/iter: 0.8463
Sim min: 0.001902286079712212 Sim max: 1.0
ClusterLoss debug: loss_val=2.898157, ne_loss=0.139489, total=3.037647
Sim min: 0.0014112176140770316 Sim max: 1.0
ClusterLoss debug: loss_val=2.893323, ne_loss=0.139562, total=3.032885
Sim min: 0.0015201501082628965 Sim max: 1.0
ClusterLoss debug: loss_val=2.872347, ne_loss=0.062617, total=2.934964
Sim min: 0.0010909108677878976 Sim max: 1.0
ClusterLoss debug: loss_val=2.886567, ne_loss=0.081633, total=2.968199
Sim min: 0.0013907564571127295 Sim max: 1.0
ClusterLoss debug: loss_val=2.875853, ne_loss=0.095891, total=2.971744
Sim min: 0.0018086706986650825 Sim max: 1.0
ClusterLoss debug: loss_val=2.897115, ne_loss=0.054242, total=2.951357
Sim min: 0.0016883178614079952 Sim max: 1.0
ClusterLoss debug: loss_val=2.880294, ne_loss=0.108250, total=2.988544
Sim min: 0.0017038576770573854 Sim max: 1.0
ClusterLoss debug: loss_val=2.875015, ne_loss=0.094187, total=2.969202
Sim min: 0.002378723816946149 Sim max: 1.0
ClusterLoss debug: loss_val=2.880213, ne_loss=0.051006, total=2.931219
Sim min: 0.0012061065062880516 Sim max: 1.0
ClusterLoss debug: loss_val=2.867797, ne_loss=0.122148, total=2.989945
Train Epoch: 3 [27008/18637 (72%)]	Instance Loss: 2.518298	Cluster Loss: 2.989945 	sec/iter: 0.8450
Sim min: 0.0018571020336821675 Sim max: 1.0
ClusterLoss debug: loss_val=2.887416, ne_loss=0.111913, total=2.999329
Sim min: 0.0017639824654906988 Sim max: 1.0
ClusterLoss debug: loss_val=2.868623, ne_loss=0.134877, total=3.003500
Sim min: 0.003258906537666917 Sim max: 1.0
ClusterLoss debug: loss_val=2.897891, ne_loss=0.125634, total=3.023525
Sim min: 0.002950601978227496 Sim max: 1.0
ClusterLoss debug: loss_val=2.894872, ne_loss=0.123693, total=3.018565
Sim min: 0.0018054782412946224 Sim max: 1.0
ClusterLoss debug: loss_val=2.903982, ne_loss=0.072392, total=2.976374
Sim min: 0.0010479963384568691 Sim max: 1.0
ClusterLoss debug: loss_val=2.871699, ne_loss=0.076379, total=2.948078
Sim min: 0.0008845094125717878 Sim max: 1.0
ClusterLoss debug: loss_val=2.868350, ne_loss=0.112213, total=2.980563
Sim min: 0.0020821397192776203 Sim max: 1.0
ClusterLoss debug: loss_val=2.878933, ne_loss=0.087086, total=2.966019
Sim min: 0.0016326752956956625 Sim max: 1.0
ClusterLoss debug: loss_val=2.867335, ne_loss=0.073850, total=2.941185
Sim min: 0.0008891496690921485 Sim max: 1.0
ClusterLoss debug: loss_val=2.866499, ne_loss=0.045320, total=2.911819
Train Epoch: 3 [28288/18637 (76%)]	Instance Loss: 2.517756	Cluster Loss: 2.911819 	sec/iter: 0.8431
Sim min: 0.0005537556135095656 Sim max: 1.0
ClusterLoss debug: loss_val=2.884142, ne_loss=0.078037, total=2.962179
Sim min: 0.0014423920074477792 Sim max: 1.0
ClusterLoss debug: loss_val=2.864315, ne_loss=0.125809, total=2.990124
Sim min: 0.0021539018489420414 Sim max: 1.0
ClusterLoss debug: loss_val=2.895119, ne_loss=0.063771, total=2.958890
Sim min: 0.0014879201771691442 Sim max: 1.0
ClusterLoss debug: loss_val=2.872418, ne_loss=0.091761, total=2.964179
Sim min: 0.001190308597870171 Sim max: 1.0
ClusterLoss debug: loss_val=2.876491, ne_loss=0.135152, total=3.011643
Sim min: 0.0017181352013722062 Sim max: 1.0
ClusterLoss debug: loss_val=2.893677, ne_loss=0.117877, total=3.011554
Sim min: 0.0007788193761371076 Sim max: 1.0
ClusterLoss debug: loss_val=2.867533, ne_loss=0.104995, total=2.972528
Sim min: 0.0010278586996719241 Sim max: 1.0
ClusterLoss debug: loss_val=2.866320, ne_loss=0.059878, total=2.926198
Sim min: 0.000941924867220223 Sim max: 1.0
ClusterLoss debug: loss_val=2.899106, ne_loss=0.091756, total=2.990862
Sim min: 0.0009264706168323755 Sim max: 1.0
ClusterLoss debug: loss_val=2.877124, ne_loss=0.163194, total=3.040318
Train Epoch: 3 [29568/18637 (79%)]	Instance Loss: 2.050418	Cluster Loss: 3.040318 	sec/iter: 0.8408
Sim min: 0.0025744244921952486 Sim max: 1.0
ClusterLoss debug: loss_val=2.876131, ne_loss=0.067093, total=2.943224
Sim min: 0.0009372727945446968 Sim max: 1.0
ClusterLoss debug: loss_val=2.909300, ne_loss=0.139819, total=3.049119
Sim min: 0.0020612559746950865 Sim max: 1.0
ClusterLoss debug: loss_val=2.878808, ne_loss=0.098492, total=2.977301
Sim min: 0.00115010526496917 Sim max: 1.0
ClusterLoss debug: loss_val=2.860924, ne_loss=0.082419, total=2.943343
Sim min: 0.0011677101720124483 Sim max: 1.0
ClusterLoss debug: loss_val=2.864458, ne_loss=0.156335, total=3.020793
Sim min: 0.0009513430413790047 Sim max: 1.0
ClusterLoss debug: loss_val=2.880552, ne_loss=0.159155, total=3.039707
Sim min: 0.002198166446760297 Sim max: 1.0
ClusterLoss debug: loss_val=2.856537, ne_loss=0.060912, total=2.917448
Sim min: 0.000636304437648505 Sim max: 1.0
ClusterLoss debug: loss_val=2.879359, ne_loss=0.076404, total=2.955762
Sim min: 0.0014194032410159707 Sim max: 1.0
ClusterLoss debug: loss_val=2.872277, ne_loss=0.113218, total=2.985496
Sim min: 0.0014665101189166307 Sim max: 1.0
ClusterLoss debug: loss_val=2.872907, ne_loss=0.184565, total=3.057472
Train Epoch: 3 [30848/18637 (83%)]	Instance Loss: 2.482228	Cluster Loss: 3.057472 	sec/iter: 0.8393
Sim min: 0.001789118512533605 Sim max: 1.0
ClusterLoss debug: loss_val=2.896053, ne_loss=0.070036, total=2.966089
Sim min: 0.0005492499331012368 Sim max: 1.0
ClusterLoss debug: loss_val=2.861135, ne_loss=0.094839, total=2.955973
Sim min: 0.0011161150177940726 Sim max: 1.0
ClusterLoss debug: loss_val=2.866354, ne_loss=0.090755, total=2.957109
Sim min: 0.0016701995627954602 Sim max: 1.0
ClusterLoss debug: loss_val=2.884248, ne_loss=0.048659, total=2.932907
Sim min: 0.0013806592905893922 Sim max: 1.0
ClusterLoss debug: loss_val=2.895191, ne_loss=0.104139, total=2.999331
Sim min: 0.0017043538391590118 Sim max: 1.0
ClusterLoss debug: loss_val=2.886942, ne_loss=0.122553, total=3.009495
Sim min: 0.0012115720892325044 Sim max: 1.0
ClusterLoss debug: loss_val=2.888752, ne_loss=0.121735, total=3.010487
Sim min: 0.0008467462030239403 Sim max: 1.0
ClusterLoss debug: loss_val=2.898061, ne_loss=0.152027, total=3.050087
Sim min: 0.002190383616834879 Sim max: 1.0
ClusterLoss debug: loss_val=2.872461, ne_loss=0.080555, total=2.953017
Sim min: 0.0011931040789932013 Sim max: 1.0
ClusterLoss debug: loss_val=2.874110, ne_loss=0.062692, total=2.936801
Train Epoch: 3 [32128/18637 (86%)]	Instance Loss: 2.685388	Cluster Loss: 2.936801 	sec/iter: 0.8392
Sim min: 0.0008328787516802549 Sim max: 1.0
ClusterLoss debug: loss_val=2.874893, ne_loss=0.109902, total=2.984796
Sim min: 0.001395190367475152 Sim max: 1.0
ClusterLoss debug: loss_val=2.878399, ne_loss=0.098740, total=2.977139
Sim min: 0.0008153394446708262 Sim max: 1.0
ClusterLoss debug: loss_val=2.891677, ne_loss=0.138632, total=3.030309
Sim min: 0.0005373050225898623 Sim max: 1.0
ClusterLoss debug: loss_val=2.866163, ne_loss=0.088693, total=2.954856
Sim min: 0.0019737256225198507 Sim max: 1.0
ClusterLoss debug: loss_val=2.882946, ne_loss=0.040864, total=2.923810
Sim min: 0.0013689076295122504 Sim max: 1.0
ClusterLoss debug: loss_val=2.875127, ne_loss=0.160413, total=3.035540
Sim min: 0.0015443334123119712 Sim max: 1.0
ClusterLoss debug: loss_val=2.882431, ne_loss=0.045635, total=2.928066
Sim min: 0.0009647398255765438 Sim max: 1.0
ClusterLoss debug: loss_val=2.894414, ne_loss=0.101979, total=2.996392
Sim min: 0.0015765466960147023 Sim max: 1.0
ClusterLoss debug: loss_val=2.903773, ne_loss=0.147059, total=3.050832
Sim min: 0.0012837093090638518 Sim max: 1.0
ClusterLoss debug: loss_val=2.884384, ne_loss=0.081204, total=2.965588
Train Epoch: 3 [33408/18637 (89%)]	Instance Loss: 2.120118	Cluster Loss: 2.965588 	sec/iter: 0.8380
Sim min: 0.00098177301697433 Sim max: 1.0
ClusterLoss debug: loss_val=2.891835, ne_loss=0.223373, total=3.115208
Sim min: 0.0012925032060593367 Sim max: 1.0
ClusterLoss debug: loss_val=2.889125, ne_loss=0.072654, total=2.961778
Sim min: 0.0008386356639675796 Sim max: 1.0
ClusterLoss debug: loss_val=2.871268, ne_loss=0.090324, total=2.961591
Sim min: 0.0009513625409454107 Sim max: 1.0
ClusterLoss debug: loss_val=2.870359, ne_loss=0.089394, total=2.959753
Sim min: 0.0014659697189927101 Sim max: 1.0
ClusterLoss debug: loss_val=2.891747, ne_loss=0.076384, total=2.968131
Sim min: 0.0015711687738075852 Sim max: 1.0
ClusterLoss debug: loss_val=2.920605, ne_loss=0.070679, total=2.991285
Sim min: 0.001183070708066225 Sim max: 1.0
ClusterLoss debug: loss_val=2.865783, ne_loss=0.054832, total=2.920615
Sim min: 0.0009321635006926954 Sim max: 1.0
ClusterLoss debug: loss_val=2.867267, ne_loss=0.107281, total=2.974548
Sim min: 0.0012119871098548174 Sim max: 1.0
ClusterLoss debug: loss_val=2.892494, ne_loss=0.099538, total=2.992032
Sim min: 0.0028636623173952103 Sim max: 1.0
ClusterLoss debug: loss_val=2.902151, ne_loss=0.156961, total=3.059112
Train Epoch: 3 [34688/18637 (93%)]	Instance Loss: 2.073176	Cluster Loss: 3.059112 	sec/iter: 0.8352
Sim min: 0.0004289838543627411 Sim max: 1.0
ClusterLoss debug: loss_val=2.917828, ne_loss=0.173193, total=3.091020
Sim min: 0.0014916841173544526 Sim max: 1.0
ClusterLoss debug: loss_val=2.890442, ne_loss=0.111335, total=3.001777
Sim min: 0.0018324960255995393 Sim max: 1.0
ClusterLoss debug: loss_val=2.868889, ne_loss=0.171838, total=3.040726
Sim min: 0.001657466753385961 Sim max: 1.0
ClusterLoss debug: loss_val=2.882677, ne_loss=0.073411, total=2.956088
Sim min: 0.001734818913973868 Sim max: 1.0
ClusterLoss debug: loss_val=2.855966, ne_loss=0.080301, total=2.936267
Sim min: 0.0008782418444752693 Sim max: 1.0
ClusterLoss debug: loss_val=2.877001, ne_loss=0.209050, total=3.086051
Sim min: 0.00080211361637339 Sim max: 1.0
ClusterLoss debug: loss_val=2.861544, ne_loss=0.079565, total=2.941109
Sim min: 0.0008805047837086022 Sim max: 1.0
ClusterLoss debug: loss_val=2.884878, ne_loss=0.145569, total=3.030447
Sim min: 0.0008357393671758473 Sim max: 1.0
ClusterLoss debug: loss_val=2.875337, ne_loss=0.116023, total=2.991360
Sim min: 0.0015235347673296928 Sim max: 1.0
ClusterLoss debug: loss_val=2.883834, ne_loss=0.079272, total=2.963106
Train Epoch: 3 [35968/18637 (96%)]	Instance Loss: 2.630592	Cluster Loss: 2.963106 	sec/iter: 0.8357
Sim min: 0.0024210382252931595 Sim max: 1.0
ClusterLoss debug: loss_val=2.897314, ne_loss=0.161958, total=3.059272
Sim min: 0.0018908909987658262 Sim max: 1.0
ClusterLoss debug: loss_val=2.890838, ne_loss=0.150129, total=3.040967
Sim min: 0.002488265745341778 Sim max: 1.0
ClusterLoss debug: loss_val=2.890446, ne_loss=0.086819, total=2.977265
Sim min: 0.0012469298671931028 Sim max: 1.0
ClusterLoss debug: loss_val=2.882022, ne_loss=0.192322, total=3.074344
Sim min: 0.002104757120832801 Sim max: 1.0
ClusterLoss debug: loss_val=2.879555, ne_loss=0.115738, total=2.995292
Sim min: 0.0014787650434300303 Sim max: 1.0
ClusterLoss debug: loss_val=2.897005, ne_loss=0.052772, total=2.949777
Sim min: 0.0010902410140261054 Sim max: 1.0
ClusterLoss debug: loss_val=2.881945, ne_loss=0.084539, total=2.966484
Sim min: 0.0011465511051937938 Sim max: 1.0
ClusterLoss debug: loss_val=2.871097, ne_loss=0.089862, total=2.960958
Sim min: 0.0015395090449601412 Sim max: 1.0
ClusterLoss debug: loss_val=2.905237, ne_loss=0.093830, total=2.999067
Sim min: 0.0015022712759673595 Sim max: 1.0
ClusterLoss debug: loss_val=2.906702, ne_loss=0.118758, total=3.025460
Train Epoch: 3 [37248/18637 (100%)]	Instance Loss: 2.512027	Cluster Loss: 3.025460 	sec/iter: 0.8350
Sim min: 0.009274578653275967 Sim max: 1.0
ClusterLoss debug: loss_val=3.110967, ne_loss=0.737398, total=3.848366
Train Epoch: 3 [37274/18637 (100%)]	Instance Loss: 0.815162	Cluster Loss: 3.848366 	sec/iter: 0.8328
Model saved to model_epoch_3.pth
请输入数字1以继续运行: 
===== 分类评估 =====
Epoch 3 - Loss: 3.0985, Acc: 4.54%
Recall:     0.0800 0.0000 0.0000 0.0500 0.0000 0.0100 0.0000 0.0000 0.1400 0.2400 0.0000 0.0000 0.0000 0.0000 0.0100 0.0141 0.3900 0.0100 0.0000 0.0000 0.0000
Precision:  0.0364 0.0000 0.0000 0.0154 0.0000 0.0455 0.0000 0.0000 0.0591 0.2202 0.0000 0.0000 0.0000 0.0000 0.0227 0.0072 0.1111 0.0074 0.0000 0.0000 0.0000
F1-score:   0.0500 0.0000 0.0000 0.0235 0.0000 0.0164 0.0000 0.0000 0.0831 0.2297 0.0000 0.0000 0.0000 0.0000 0.0139 0.0096 0.1729 0.0085 0.0000 0.0000 0.0000

===== 聚类评估 =====
混淆矩阵:
 [[ 1  0  0  1  0  0  0  0  0  2  0  0 96  0  0  0  0  0  0  0  0]
 [28  0  0  1  0 69  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0]
 [ 1  0  0  2  0  0  0  0  0  0  0  0 78  1  0  0  0  0  0 18  0]
 [ 0  0  0  2  0  0  0  0  0  0  0 68 28  0  2  0  0  0  0  0  0]
 [ 2  0  0 14  0  0  0  0  0  0  0  0 55  0  0  0  0  0  0 29  0]
 [88  0  0  3  0  1  0  0  0  1  0  0  3  0  0  0  0  1  0  3  0]
 [ 0  8  0 26  0  0  0 13  4  0  0  1  2  0  0 33  0  0 12  0  1]
 [ 0  0  0  0 11  0  0  0  0  0  0  0 89  0  0  0  0  0  0  0  0]
 [ 6  0  0 71  0  2  0  0  0  0  0  0 21  0  0  0  0  0  0  0  0]
 [ 1  0  0  1  0  0  0  0  0  0  0  0 23  0  0  0  0 74  0  1  0]
 [ 0  0  0  1  0  0  0  0  0  0  1  0 94  0  0  0  0  4  0  0  0]
 [ 1  0 25 60  0  0  0  0  0  0  0  0 10  0  0  0  2  0  0  2  0]
 [ 0  0  0  5  0  0 57  0  0  0  0  0 38  0  0  0  0  0  0  0  0]
 [ 1  0  0  4  0  0  0  0  0 35 28  0 32  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0 65  2  0 33  0  0  0  0  0  0  0  0]
 [ 0  0  0 55  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  1  0]
 [96  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  1  0]
 [97  0  0  1  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 1  0  0  1  0  0  0  0  0  0  0  0 97  0  0  0  0  0  0  1  0]
 [96  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 2  0  0  1  0  3  0  0  0  0  0  0  1 92  0  0  0  0  0  1  0]]
代价矩阵:
 [[420. 393. 420. 421. 419. 333. 421. 421. 415. 420. 421. 420. 421. 420.
  421. 421. 325. 324. 420. 325. 419.]
 [  8.   8.   8.   8.   8.   8.   0.   8.   8.   8.   8.   8.   8.   8.
    8.   8.   8.   8.   8.   8.   8.]
 [ 25.  25.  25.  25.  25.  25.  25.  25.  25.  25.  25.   0.  25.  25.
   25.  25.  25.  25.  25.  25.  25.]
 [248. 248. 247. 247. 235. 246. 223. 249. 178. 248. 248. 189. 244. 245.
  249. 194. 249. 248. 248. 249. 248.]
 [ 11.  11.  11.  11.  11.  11.  11.   0.  11.  11.  11.  11.  11.  11.
   11.  11.  11.  11.  11.  11.  11.]
 [ 75.   6.  75.  75.  75.  74.  75.  75.  73.  75.  75.  75.  75.  75.
   75.  75.  75.  75.  75.  75.  72.]
 [ 57.  57.  57.  57.  57.  57.  57.  57.  57.  57.  57.  57.   0.  57.
   57.  57.  57.  57.  57.  57.  57.]
 [ 13.  13.  13.  13.  13.  13.   0.  13.  13.  13.  13.  13.  13.  13.
   13.  13.  13.  13.  13.  13.  13.]
 [  4.   4.   4.   4.   4.   4.   0.   4.   4.   4.   4.   4.   4.   4.
    4.   4.   4.   4.   4.   4.   4.]
 [101. 103. 103. 103. 103. 102. 103. 103. 103. 103. 103. 103. 103.  68.
   38. 103. 103. 103. 103. 103. 103.]
 [ 31.  31.  31.  31.  31.  31.  31.  31.  31.  31.  30.  31.  31.   3.
   29.  31.  31.  31.  31.  31.  31.]
 [ 69.  69.  69.   1.  69.  69.  68.  69.  69.  69.  69.  69.  69.  69.
   69.  69.  69.  69.  69.  69.  69.]
 [628. 724. 646. 696. 669. 721. 722. 635. 703. 701. 630. 714. 686. 692.
  691. 709. 721. 722. 627. 720. 723.]
 [ 94.  93.  93.  94.  94.  94.  94.  94.  94.  94.  94.  94.  94.  94.
   94.  94.  94.  94.  94.  94.   2.]
 [  2.   2.   2.   0.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [ 33.  33.  33.  33.  33.  33.   0.  33.  33.  33.  33.  33.  33.  33.
   33.  33.  33.  33.  33.  33.  33.]
 [  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [ 79.  79.  79.  79.  79.  78.  79.  79.  79.   5.  75.  79.  79.  79.
   79.  79.  79.  79.  79.  79.  79.]
 [ 12.  12.  12.  12.  12.  12.   0.  12.  12.  12.  12.  12.  12.  12.
   12.  12.  12.  12.  12.  12.  12.]
 [ 58.  57.  40.  58.  29.  55.  58.  58.  58.  57.  58.  56.  58.  58.
   58.  57.  57.  58.  57.  58.  57.]
 [  1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.]]
最佳映射关系: [17.  0. 11.  8.  7.  1. 12. 19.  2. 14. 13.  3. 18. 20.  5.  6. 15.  9.
 10.  4. 16.]
聚类分配的最大索引: 20

=== 实例级嵌入 ===
NMI: 0.5711  ARI: 0.2039
F-score: 0.3296  Silhouette: 0.2398
Adjusted Acc: 0.3940
混淆矩阵:
 [[ 0  0 29  4  5  0  1  1  0  4  0  0  0  0  0 23 33  0  0  0  0]
 [ 0  1  1  0  5  0  0  5  0  0 38  4  0  1  0  0  8  0  0  0 37]
 [13  1 23  0  3  1  0  2  0  0  0  5  0  5  0 13 22  0  9  3  0]
 [ 0 15  2  1  1 15  0  1  0  1  1  0  0 29  2  0  4  0  0 28  0]
 [36  1 14  0  4  0  0 18  0  3  0  5  0  0  0 13  4  0  0  2  0]
 [ 0  1  7 11 12  1  7 10  0  2  0  2  0  2  0 19 11  0  6  2  7]
 [ 0  0  0  0  0 23  0  0  0  0  0  0  0  2 38  0  0  0  0 37  0]
 [ 0 47  0  0  0  1  0  0 11  0  0  0  0  6  0  0  0  0  0 35  0]
 [ 0  1 21  0 14 18  9 12  0  0  2  1  0  0  0  3  3 12  3  1  0]
 [ 0  0 23  0  0  0 35  1  0  0  0  0  0  1  0  0  0 31  0  9  0]
 [ 0  0 10  1  4  1 66  0  0  0  0  1  0  1  0  1  7  8  0  0  0]
 [ 0 24  0  0  0 26  0  1  0  0  0  0 27  7  0  0  1  0  0 14  0]
 [ 0  1  0  6  8  0  0  7 42  0  4  0  0  8  0  0  0  0  0  6 18]
 [ 0  3  3 57  7  3  0  1  0  4  0 12  0  0  0  2  5  0  2  1  0]
 [ 0  1  4 60  8  0  0  0  0  1  0  2  0  1  0  7  2  0  8  6  0]
 [ 0 20 18  1  2 12  0  2  0  0  0  0  0  1  0  0  0  0 12  3  0]
 [ 6  0 14  1 20  0  0 42  0  3  0  2  0  2  0  0 10  0  0  0  0]
 [ 1  0  9  1 18  0 29  2  0  5  0  6  0  0  0  1  6  0 21  1  0]
 [57  0  3  1  0  0  0  0  0  0  0  0  0  1  0  1  2  0 35  0  0]
 [ 0  3  4  0 18  3  1  2  0 31  0  0  0  0  0 14  5  0 19  0  0]
 [ 1  1  0 22  6  0  0  0  0  0 31 38  0  1  0  0  0  0  0  0  0]]
代价矩阵:
 [[114. 114. 101. 114.  78. 114. 114. 114. 114. 114. 114. 114. 114. 114.
  114. 114. 108. 113.  57. 114. 113.]
 [120. 119. 119. 105. 119. 119. 120.  73. 119. 120. 120.  96. 119. 117.
  119. 100. 120. 120. 120. 117. 119.]
 [156. 184. 162. 183. 171. 178. 185. 185. 164. 162. 175. 185. 185. 182.
  181. 167. 171. 176. 182. 181. 185.]
 [162. 166. 166. 165. 166. 155. 166. 166. 166. 166. 165. 166. 160. 109.
  106. 165. 165. 165. 165. 166. 144.]
 [130. 130. 132. 134. 131. 123. 135. 135. 121. 135. 131. 135. 127. 128.
  127. 133. 115. 117. 135. 117. 129.]
 [104. 104. 103.  89. 104. 103.  81. 103.  86. 104. 103.  78. 104. 101.
  104.  92. 104. 104. 104. 101. 104.]
 [147. 148. 148. 148. 148. 141. 148. 148. 139. 113.  82. 148. 148. 148.
  148. 148. 148. 119. 148. 147. 148.]
 [106. 102. 105. 106.  89.  97. 107. 107.  95. 106. 107. 106. 100. 106.
  107. 105.  65. 105. 107. 105. 107.]
 [ 53.  53.  53.  53.  53.  53.  53.  42.  53.  53.  53.  53.  11.  53.
   53.  53.  53.  53.  53.  53.  53.]
 [ 50.  54.  54.  53.  51.  52.  54.  54.  54.  54.  54.  54.  54.  50.
   53.  54.  51.  49.  54.  23.  54.]
 [ 76.  38.  76.  75.  76.  76.  76.  76.  74.  76.  76.  76.  72.  76.
   76.  76.  76.  76.  76.  76.  45.]
 [ 78.  74.  73.  78.  73.  76.  78.  78.  77.  78.  77.  78.  78.  66.
   76.  78.  76.  72.  78.  78.  40.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 68.  67.  63.  39.  68.  66.  66.  62.  68.  67.  67.  61.  60.  68.
   67.  67.  66.  68.  67.  68.  67.]
 [ 40.  40.  40.  38.  40.  40.   2.  40.  40.  40.  40.  40.  40.  40.
   40.  40.  40.  40.  40.  40.  40.]
 [ 74.  97.  84.  97.  84.  78.  97.  97.  94.  97.  96.  97.  97.  95.
   90.  97.  97.  96.  96.  83.  97.]
 [ 90. 115. 101. 119. 119. 112. 123. 123. 120. 123. 116. 122. 123. 118.
  121. 123. 113. 117. 121. 118. 123.]
 [ 51.  51.  51.  51.  51.  51.  51.  51.  39.  20.  43.  51.  51.  51.
   51.  51.  51.  51.  51.  51.  51.]
 [115. 115. 106. 115. 115. 109. 115. 115. 112. 115. 115. 115. 115. 113.
  107. 103. 115.  94.  80.  96. 115.]
 [148. 148. 145. 120. 146. 146. 111. 113. 147. 139. 148. 134. 142. 147.
  142. 145. 148. 147. 148. 148. 148.]
 [ 62.  25.  62.  62.  62.  55.  62.  62.  62.  62.  62.  62.  44.  62.
   62.  62.  62.  62.  62.  62.  62.]]
最佳映射关系: [ 4. 15.  2. 14. 17.  8. 10. 16. 12. 19. 20. 13. 11.  3.  6.  5.  0.  9.
 18.  7.  1.]
聚类分配的最大索引: 20

=== 簇级嵌入 ===
NMI: 0.4365  ARI: 0.1952
F-score: 0.2373  Silhouette: 0.5659
Adjusted Acc: 0.3298

总耗时: 33.08s
分类准确率: 4.54%
簇级嵌入NMI: 0.4365
请输入数字2以继续运行: Sim min: 0.0016573191387578845 Sim max: 1.0
ClusterLoss debug: loss_val=2.857179, ne_loss=0.034990, total=2.892169
Train Epoch: 4 [128/18637 (0%)]	Instance Loss: 2.003277	Cluster Loss: 2.892169 	sec/iter: 0.8842
Sim min: 0.002325331559404731 Sim max: 1.0
ClusterLoss debug: loss_val=2.904524, ne_loss=0.117558, total=3.022082
Sim min: 0.0032248683273792267 Sim max: 1.0
ClusterLoss debug: loss_val=2.894181, ne_loss=0.096310, total=2.990492
Sim min: 0.0028196536004543304 Sim max: 1.0
ClusterLoss debug: loss_val=2.902860, ne_loss=0.144522, total=3.047382
Sim min: 0.0011569744674488902 Sim max: 1.0
ClusterLoss debug: loss_val=2.873056, ne_loss=0.094270, total=2.967326
Sim min: 0.002442001597955823 Sim max: 1.0
ClusterLoss debug: loss_val=2.913081, ne_loss=0.126547, total=3.039628
Sim min: 0.002357853576540947 Sim max: 1.0
ClusterLoss debug: loss_val=2.870681, ne_loss=0.151767, total=3.022448
Sim min: 0.0021195148583501577 Sim max: 1.0
ClusterLoss debug: loss_val=2.886363, ne_loss=0.115681, total=3.002044
Sim min: 0.004383452236652374 Sim max: 1.0
ClusterLoss debug: loss_val=2.902699, ne_loss=0.062622, total=2.965321
Sim min: 0.001740237232297659 Sim max: 1.0
ClusterLoss debug: loss_val=2.888991, ne_loss=0.133553, total=3.022545
Sim min: 0.00119247124530375 Sim max: 1.0
ClusterLoss debug: loss_val=2.881134, ne_loss=0.126296, total=3.007430
Train Epoch: 4 [1408/18637 (4%)]	Instance Loss: 2.281341	Cluster Loss: 3.007430 	sec/iter: 0.8370
Sim min: 0.0015263122040778399 Sim max: 1.0
ClusterLoss debug: loss_val=2.862933, ne_loss=0.053371, total=2.916303
Sim min: 0.0009837225079536438 Sim max: 1.0
ClusterLoss debug: loss_val=2.872278, ne_loss=0.078394, total=2.950672
Sim min: 0.0033580183517187834 Sim max: 1.0
ClusterLoss debug: loss_val=2.884013, ne_loss=0.156609, total=3.040622
Sim min: 0.0007873622234910727 Sim max: 1.0
ClusterLoss debug: loss_val=2.876735, ne_loss=0.102950, total=2.979684
Sim min: 0.0015920513542369008 Sim max: 1.0
ClusterLoss debug: loss_val=2.900235, ne_loss=0.125422, total=3.025657
Sim min: 0.0011946220183745027 Sim max: 1.0
ClusterLoss debug: loss_val=2.863898, ne_loss=0.117354, total=2.981252
Sim min: 0.0014607056509703398 Sim max: 1.0
ClusterLoss debug: loss_val=2.872737, ne_loss=0.134535, total=3.007272
Sim min: 0.0010580996749922633 Sim max: 1.0
ClusterLoss debug: loss_val=2.884681, ne_loss=0.159340, total=3.044022
Sim min: 0.001975548919290304 Sim max: 1.0
ClusterLoss debug: loss_val=2.885334, ne_loss=0.106095, total=2.991429
Sim min: 0.0026028091087937355 Sim max: 1.0
ClusterLoss debug: loss_val=2.899879, ne_loss=0.045884, total=2.945763
Train Epoch: 4 [2688/18637 (7%)]	Instance Loss: 2.162546	Cluster Loss: 2.945763 	sec/iter: 0.8000
Sim min: 0.002105262130498886 Sim max: 1.0
ClusterLoss debug: loss_val=2.897690, ne_loss=0.079864, total=2.977553
Sim min: 0.0014465617714449763 Sim max: 1.0
ClusterLoss debug: loss_val=2.901302, ne_loss=0.123265, total=3.024567
Sim min: 0.0009068335639312863 Sim max: 1.0
ClusterLoss debug: loss_val=2.869873, ne_loss=0.131756, total=3.001629
Sim min: 0.0011643357574939728 Sim max: 1.0
ClusterLoss debug: loss_val=2.875202, ne_loss=0.087982, total=2.963184
Sim min: 0.001619081711396575 Sim max: 1.0
ClusterLoss debug: loss_val=2.881675, ne_loss=0.100706, total=2.982381
Sim min: 0.0014472242910414934 Sim max: 1.0
ClusterLoss debug: loss_val=2.865510, ne_loss=0.086712, total=2.952222
Sim min: 0.0016718520782887936 Sim max: 1.0
ClusterLoss debug: loss_val=2.875810, ne_loss=0.152963, total=3.028774
Sim min: 0.0022043362259864807 Sim max: 1.0
ClusterLoss debug: loss_val=2.894219, ne_loss=0.076153, total=2.970372
Sim min: 0.002226402750238776 Sim max: 1.0
ClusterLoss debug: loss_val=2.886612, ne_loss=0.174072, total=3.060684
Sim min: 0.001191258430480957 Sim max: 1.0
ClusterLoss debug: loss_val=2.876009, ne_loss=0.110120, total=2.986129
Train Epoch: 4 [3968/18637 (11%)]	Instance Loss: 2.254201	Cluster Loss: 2.986129 	sec/iter: 0.7980
Sim min: 0.0021625724621117115 Sim max: 1.0
ClusterLoss debug: loss_val=2.873695, ne_loss=0.041555, total=2.915250
Sim min: 0.000878240680322051 Sim max: 1.0
ClusterLoss debug: loss_val=2.884083, ne_loss=0.103278, total=2.987361
Sim min: 0.0018752421019598842 Sim max: 1.0
ClusterLoss debug: loss_val=2.902131, ne_loss=0.084944, total=2.987075
Sim min: 0.00217425893060863 Sim max: 1.0
ClusterLoss debug: loss_val=2.892544, ne_loss=0.124299, total=3.016843
Sim min: 0.0018163468921557069 Sim max: 1.0
ClusterLoss debug: loss_val=2.880934, ne_loss=0.113344, total=2.994278
Sim min: 0.0020422139205038548 Sim max: 1.0
ClusterLoss debug: loss_val=2.871216, ne_loss=0.097032, total=2.968248
Sim min: 0.0023344627115875483 Sim max: 1.0
ClusterLoss debug: loss_val=2.873575, ne_loss=0.120567, total=2.994143
Sim min: 0.001602682052180171 Sim max: 1.0
ClusterLoss debug: loss_val=2.867788, ne_loss=0.137457, total=3.005244
Sim min: 0.0011774918530136347 Sim max: 1.0
ClusterLoss debug: loss_val=2.871029, ne_loss=0.131792, total=3.002821
Sim min: 0.002375178737565875 Sim max: 1.0
ClusterLoss debug: loss_val=2.880751, ne_loss=0.067996, total=2.948747
Train Epoch: 4 [5248/18637 (14%)]	Instance Loss: 2.073502	Cluster Loss: 2.948747 	sec/iter: 0.8107
Sim min: 0.0032349037937819958 Sim max: 1.0
ClusterLoss debug: loss_val=2.868345, ne_loss=0.054285, total=2.922631
Sim min: 0.00404066639021039 Sim max: 1.0
ClusterLoss debug: loss_val=2.911846, ne_loss=0.085011, total=2.996857
Sim min: 0.00462044682353735 Sim max: 1.0
ClusterLoss debug: loss_val=2.915123, ne_loss=0.214802, total=3.129924
Sim min: 0.0036501807626336813 Sim max: 1.0
ClusterLoss debug: loss_val=2.890931, ne_loss=0.084662, total=2.975593
Sim min: 0.0020575583912432194 Sim max: 1.0
ClusterLoss debug: loss_val=2.915102, ne_loss=0.166136, total=3.081237
Sim min: 0.003461279207840562 Sim max: 1.0
ClusterLoss debug: loss_val=2.889242, ne_loss=0.106424, total=2.995667
Sim min: 0.0018015092937275767 Sim max: 1.0
ClusterLoss debug: loss_val=2.875691, ne_loss=0.078465, total=2.954156
Sim min: 0.0018530685920268297 Sim max: 1.0
ClusterLoss debug: loss_val=2.877007, ne_loss=0.120247, total=2.997254
Sim min: 0.0020310101099312305 Sim max: 1.0
ClusterLoss debug: loss_val=2.872681, ne_loss=0.092768, total=2.965449
Sim min: 0.0023312827106565237 Sim max: 1.0
ClusterLoss debug: loss_val=2.873359, ne_loss=0.076133, total=2.949492
Train Epoch: 4 [6528/18637 (17%)]	Instance Loss: 2.417563	Cluster Loss: 2.949492 	sec/iter: 0.7986
Sim min: 0.003055871929973364 Sim max: 1.0
ClusterLoss debug: loss_val=2.873410, ne_loss=0.079397, total=2.952807
Sim min: 0.0023021912202239037 Sim max: 1.0
ClusterLoss debug: loss_val=2.881654, ne_loss=0.116898, total=2.998551
Sim min: 0.001191181130707264 Sim max: 1.0
ClusterLoss debug: loss_val=2.869112, ne_loss=0.134907, total=3.004019
Sim min: 0.0010935721220448613 Sim max: 1.0
ClusterLoss debug: loss_val=2.873175, ne_loss=0.090807, total=2.963982
Sim min: 0.0016740282298997045 Sim max: 1.0
ClusterLoss debug: loss_val=2.883771, ne_loss=0.143521, total=3.027291
Sim min: 0.001998583087697625 Sim max: 1.0
ClusterLoss debug: loss_val=2.866119, ne_loss=0.064548, total=2.930667
Sim min: 0.002489997772499919 Sim max: 1.0
ClusterLoss debug: loss_val=2.884447, ne_loss=0.100351, total=2.984798
Sim min: 0.003664806019514799 Sim max: 1.0
ClusterLoss debug: loss_val=2.901229, ne_loss=0.089542, total=2.990771
Sim min: 0.0022609552834182978 Sim max: 1.0
ClusterLoss debug: loss_val=2.886818, ne_loss=0.132030, total=3.018848
Sim min: 0.0023941134568303823 Sim max: 1.0
ClusterLoss debug: loss_val=2.871807, ne_loss=0.065866, total=2.937673
Train Epoch: 4 [7808/18637 (21%)]	Instance Loss: 1.853918	Cluster Loss: 2.937673 	sec/iter: 0.8036
Sim min: 0.0013277815887704492 Sim max: 1.0
ClusterLoss debug: loss_val=2.880837, ne_loss=0.065305, total=2.946142
Sim min: 0.0013499121414497495 Sim max: 1.0
ClusterLoss debug: loss_val=2.876177, ne_loss=0.106244, total=2.982421
Sim min: 0.004467517603188753 Sim max: 1.0
ClusterLoss debug: loss_val=2.896528, ne_loss=0.257025, total=3.153553
Sim min: 0.003544845851138234 Sim max: 1.0
ClusterLoss debug: loss_val=2.920403, ne_loss=0.093052, total=3.013455
Sim min: 0.001988035626709461 Sim max: 1.0
ClusterLoss debug: loss_val=2.876134, ne_loss=0.069230, total=2.945364
Sim min: 0.0013857933226972818 Sim max: 1.0
ClusterLoss debug: loss_val=2.881081, ne_loss=0.089356, total=2.970437
Sim min: 0.0019484091317281127 Sim max: 1.0
ClusterLoss debug: loss_val=2.882606, ne_loss=0.093123, total=2.975728
Sim min: 0.0011950258631259203 Sim max: 1.0
ClusterLoss debug: loss_val=2.868182, ne_loss=0.114938, total=2.983120
Sim min: 0.0019112734589725733 Sim max: 1.0
ClusterLoss debug: loss_val=2.890578, ne_loss=0.115506, total=3.006084
Sim min: 0.0030151677783578634 Sim max: 1.0
ClusterLoss debug: loss_val=2.882554, ne_loss=0.073586, total=2.956140
Train Epoch: 4 [9088/18637 (24%)]	Instance Loss: 2.100432	Cluster Loss: 2.956140 	sec/iter: 0.8079
Sim min: 0.0018581920303404331 Sim max: 1.0
ClusterLoss debug: loss_val=2.885251, ne_loss=0.102781, total=2.988031
Sim min: 0.0006038310821168125 Sim max: 1.0
ClusterLoss debug: loss_val=2.886425, ne_loss=0.198445, total=3.084870
Sim min: 0.0006838938570581377 Sim max: 1.0
ClusterLoss debug: loss_val=2.880633, ne_loss=0.160499, total=3.041131
Sim min: 0.0017865034751594067 Sim max: 1.0
ClusterLoss debug: loss_val=2.857899, ne_loss=0.159268, total=3.017167
Sim min: 0.0036907284520566463 Sim max: 1.0
ClusterLoss debug: loss_val=2.890067, ne_loss=0.153470, total=3.043536
Sim min: 0.0010986716952174902 Sim max: 1.0
ClusterLoss debug: loss_val=2.885721, ne_loss=0.251730, total=3.137451
Sim min: 0.0006677826168015599 Sim max: 1.0
ClusterLoss debug: loss_val=2.890491, ne_loss=0.080462, total=2.970953
Sim min: 0.0015848195180296898 Sim max: 1.0
ClusterLoss debug: loss_val=2.906609, ne_loss=0.104507, total=3.011116
Sim min: 0.0028437592554837465 Sim max: 1.0
ClusterLoss debug: loss_val=2.884690, ne_loss=0.132496, total=3.017186
Sim min: 0.003388351993635297 Sim max: 1.0
ClusterLoss debug: loss_val=2.917132, ne_loss=0.128812, total=3.045944
Train Epoch: 4 [10368/18637 (28%)]	Instance Loss: 2.153270	Cluster Loss: 3.045944 	sec/iter: 0.8081
Sim min: 0.003028060542419553 Sim max: 1.0
ClusterLoss debug: loss_val=2.880749, ne_loss=0.067449, total=2.948198
Sim min: 0.0015374937793239951 Sim max: 1.0
ClusterLoss debug: loss_val=2.906670, ne_loss=0.167121, total=3.073791
Sim min: 0.0015033632516860962 Sim max: 1.0
ClusterLoss debug: loss_val=2.914491, ne_loss=0.116709, total=3.031200
Sim min: 0.0012625337112694979 Sim max: 1.0
ClusterLoss debug: loss_val=2.883798, ne_loss=0.068656, total=2.952454
Sim min: 0.0023673775140196085 Sim max: 1.0
ClusterLoss debug: loss_val=2.879187, ne_loss=0.145566, total=3.024752
Sim min: 0.0036379715893417597 Sim max: 1.0
ClusterLoss debug: loss_val=2.883380, ne_loss=0.104887, total=2.988267
Sim min: 0.0037223147228360176 Sim max: 1.0
ClusterLoss debug: loss_val=2.879771, ne_loss=0.129277, total=3.009048
Sim min: 0.0019794877152889967 Sim max: 1.0
ClusterLoss debug: loss_val=2.904596, ne_loss=0.171475, total=3.076071
Sim min: 0.0014998752158135176 Sim max: 1.0
ClusterLoss debug: loss_val=2.881326, ne_loss=0.117050, total=2.998377
Sim min: 0.002024012617766857 Sim max: 1.0
ClusterLoss debug: loss_val=2.874980, ne_loss=0.128618, total=3.003598
Train Epoch: 4 [11648/18637 (31%)]	Instance Loss: 2.290404	Cluster Loss: 3.003598 	sec/iter: 0.8041
Sim min: 0.001841396908275783 Sim max: 1.0
ClusterLoss debug: loss_val=2.854939, ne_loss=0.182046, total=3.036986
Sim min: 0.0018478684360161424 Sim max: 1.0
ClusterLoss debug: loss_val=2.881133, ne_loss=0.071870, total=2.953004
Sim min: 0.002764725824818015 Sim max: 1.0
ClusterLoss debug: loss_val=2.899809, ne_loss=0.131165, total=3.030974
Sim min: 0.0016189776360988617 Sim max: 1.0
ClusterLoss debug: loss_val=2.881484, ne_loss=0.134677, total=3.016161
Sim min: 0.0014143433654680848 Sim max: 1.0
ClusterLoss debug: loss_val=2.865124, ne_loss=0.056451, total=2.921576
Sim min: 0.0028815639670938253 Sim max: 1.0
ClusterLoss debug: loss_val=2.878585, ne_loss=0.181608, total=3.060194
Sim min: 0.0020504898857325315 Sim max: 1.0
ClusterLoss debug: loss_val=2.936018, ne_loss=0.140304, total=3.076321
Sim min: 0.0018655294552445412 Sim max: 1.0
ClusterLoss debug: loss_val=2.878023, ne_loss=0.114214, total=2.992237
Sim min: 0.001102575333788991 Sim max: 1.0
ClusterLoss debug: loss_val=2.895894, ne_loss=0.100269, total=2.996163
Sim min: 0.0024914920795708895 Sim max: 1.0
ClusterLoss debug: loss_val=2.876604, ne_loss=0.128478, total=3.005082
Train Epoch: 4 [12928/18637 (35%)]	Instance Loss: 2.331459	Cluster Loss: 3.005082 	sec/iter: 0.8022
Sim min: 0.001576681388542056 Sim max: 1.0
ClusterLoss debug: loss_val=2.870021, ne_loss=0.095357, total=2.965378
Sim min: 0.001403172267600894 Sim max: 1.0
ClusterLoss debug: loss_val=2.904850, ne_loss=0.158389, total=3.063239
Sim min: 0.0028022590558975935 Sim max: 1.0
ClusterLoss debug: loss_val=2.913443, ne_loss=0.100736, total=3.014178
Sim min: 0.002271086908876896 Sim max: 1.0
ClusterLoss debug: loss_val=2.882569, ne_loss=0.102589, total=2.985158
Sim min: 0.0019239475950598717 Sim max: 1.0
ClusterLoss debug: loss_val=2.899345, ne_loss=0.152398, total=3.051744
Sim min: 0.0021121054887771606 Sim max: 1.0
ClusterLoss debug: loss_val=2.883991, ne_loss=0.035981, total=2.919972
Sim min: 0.0012069958029314876 Sim max: 1.0
ClusterLoss debug: loss_val=2.894775, ne_loss=0.115927, total=3.010702
Sim min: 0.0012264963006600738 Sim max: 1.0
ClusterLoss debug: loss_val=2.869846, ne_loss=0.076403, total=2.946249
Sim min: 0.000545230635907501 Sim max: 1.0
ClusterLoss debug: loss_val=2.870624, ne_loss=0.131784, total=3.002408
Sim min: 0.0015911322552710772 Sim max: 1.0
ClusterLoss debug: loss_val=2.870942, ne_loss=0.108943, total=2.979885
Train Epoch: 4 [14208/18637 (38%)]	Instance Loss: 2.306571	Cluster Loss: 2.979885 	sec/iter: 0.7993
Sim min: 0.0020324676297605038 Sim max: 1.0
ClusterLoss debug: loss_val=2.900525, ne_loss=0.074772, total=2.975296
Sim min: 0.0019890600815415382 Sim max: 1.0
ClusterLoss debug: loss_val=2.899912, ne_loss=0.104595, total=3.004507
Sim min: 0.0013883360661566257 Sim max: 1.0
ClusterLoss debug: loss_val=2.888480, ne_loss=0.149631, total=3.038111
Sim min: 0.0016044164076447487 Sim max: 1.0
ClusterLoss debug: loss_val=2.881244, ne_loss=0.266081, total=3.147326
Sim min: 0.0011458916123956442 Sim max: 1.0
ClusterLoss debug: loss_val=2.878153, ne_loss=0.072396, total=2.950550
Sim min: 0.00044921133667230606 Sim max: 1.0
ClusterLoss debug: loss_val=2.876232, ne_loss=0.045605, total=2.921837
Sim min: 0.0012806920567527413 Sim max: 1.0
ClusterLoss debug: loss_val=2.887167, ne_loss=0.064848, total=2.952015
Sim min: 0.0008766673854552209 Sim max: 1.0
ClusterLoss debug: loss_val=2.866896, ne_loss=0.066603, total=2.933499
Sim min: 0.0009647216647863388 Sim max: 1.0
ClusterLoss debug: loss_val=2.880594, ne_loss=0.092030, total=2.972624
Sim min: 0.0006296630599536002 Sim max: 1.0
ClusterLoss debug: loss_val=2.890154, ne_loss=0.130252, total=3.020407
Train Epoch: 4 [15488/18637 (41%)]	Instance Loss: 2.077037	Cluster Loss: 3.020407 	sec/iter: 0.8029
Sim min: 0.0017334995791316032 Sim max: 1.0
ClusterLoss debug: loss_val=2.885707, ne_loss=0.093094, total=2.978801
Sim min: 0.0013871949631720781 Sim max: 1.0
ClusterLoss debug: loss_val=2.875597, ne_loss=0.127922, total=3.003519
Sim min: 0.0008583869785070419 Sim max: 1.0
ClusterLoss debug: loss_val=2.884699, ne_loss=0.039707, total=2.924406
Sim min: 0.002205789554864168 Sim max: 1.0
ClusterLoss debug: loss_val=2.888230, ne_loss=0.093614, total=2.981844
Sim min: 0.0011481844121590257 Sim max: 1.0
ClusterLoss debug: loss_val=2.886808, ne_loss=0.068017, total=2.954825
Sim min: 0.0017623400781303644 Sim max: 1.0
ClusterLoss debug: loss_val=2.897624, ne_loss=0.078961, total=2.976585
Sim min: 0.0015354208881035447 Sim max: 1.0
ClusterLoss debug: loss_val=2.898697, ne_loss=0.182257, total=3.080954
Sim min: 0.0011019594967365265 Sim max: 1.0
ClusterLoss debug: loss_val=2.868425, ne_loss=0.116110, total=2.984535
Sim min: 0.0007722426671534777 Sim max: 1.0
ClusterLoss debug: loss_val=2.877599, ne_loss=0.110010, total=2.987610
Sim min: 0.0016777270939201117 Sim max: 1.0
ClusterLoss debug: loss_val=2.894835, ne_loss=0.045936, total=2.940771
Train Epoch: 4 [16768/18637 (45%)]	Instance Loss: 2.479056	Cluster Loss: 2.940771 	sec/iter: 0.8026
Sim min: 0.0012902660528197885 Sim max: 1.0
ClusterLoss debug: loss_val=2.902916, ne_loss=0.119167, total=3.022083
Sim min: 0.0014033536426723003 Sim max: 1.0
ClusterLoss debug: loss_val=2.891405, ne_loss=0.124408, total=3.015813
Sim min: 0.0016752268420532346 Sim max: 1.0
ClusterLoss debug: loss_val=2.872569, ne_loss=0.107112, total=2.979681
Sim min: 0.0031813043169677258 Sim max: 1.0
ClusterLoss debug: loss_val=2.889199, ne_loss=0.153765, total=3.042965
Sim min: 0.0022437272127717733 Sim max: 1.0
ClusterLoss debug: loss_val=2.903288, ne_loss=0.101968, total=3.005256
Sim min: 0.0009023752063512802 Sim max: 1.0
ClusterLoss debug: loss_val=2.874619, ne_loss=0.190330, total=3.064950
Sim min: 0.0016571467276662588 Sim max: 1.0
ClusterLoss debug: loss_val=2.885937, ne_loss=0.082424, total=2.968361
Sim min: 0.001502511790022254 Sim max: 1.0
ClusterLoss debug: loss_val=2.882706, ne_loss=0.113252, total=2.995958
Sim min: 0.0017228570068255067 Sim max: 1.0
ClusterLoss debug: loss_val=2.892453, ne_loss=0.062566, total=2.955019
Sim min: 0.00320691941305995 Sim max: 1.0
ClusterLoss debug: loss_val=2.882335, ne_loss=0.137943, total=3.020278
Train Epoch: 4 [18048/18637 (48%)]	Instance Loss: 2.212128	Cluster Loss: 3.020278 	sec/iter: 0.8003
Sim min: 0.0022962221410125494 Sim max: 1.0
ClusterLoss debug: loss_val=2.873532, ne_loss=0.076499, total=2.950031
Sim min: 0.0010287478799000382 Sim max: 1.0
ClusterLoss debug: loss_val=2.897269, ne_loss=0.182754, total=3.080023
Sim min: 0.0024929456412792206 Sim max: 1.0
ClusterLoss debug: loss_val=2.876616, ne_loss=0.092903, total=2.969520
Sim min: 0.0015892739174887538 Sim max: 1.0
ClusterLoss debug: loss_val=2.898417, ne_loss=0.118971, total=3.017388
Sim min: 0.002745572244748473 Sim max: 1.0
ClusterLoss debug: loss_val=2.888596, ne_loss=0.074443, total=2.963039
Sim min: 0.0018934084801003337 Sim max: 1.0
ClusterLoss debug: loss_val=2.900948, ne_loss=0.132046, total=3.032993
Sim min: 0.0024188305251300335 Sim max: 1.0
ClusterLoss debug: loss_val=2.861657, ne_loss=0.136997, total=2.998654
Sim min: 0.0012799468822777271 Sim max: 1.0
ClusterLoss debug: loss_val=2.868846, ne_loss=0.094134, total=2.962980
Sim min: 0.0012186432722955942 Sim max: 1.0
ClusterLoss debug: loss_val=2.867887, ne_loss=0.091304, total=2.959191
Sim min: 0.001246590749360621 Sim max: 1.0
ClusterLoss debug: loss_val=2.878696, ne_loss=0.147032, total=3.025728
Train Epoch: 4 [19328/18637 (52%)]	Instance Loss: 2.311537	Cluster Loss: 3.025728 	sec/iter: 0.8004
Sim min: 0.0008933843928389251 Sim max: 1.0
ClusterLoss debug: loss_val=2.876658, ne_loss=0.138218, total=3.014876
Sim min: 0.001186971552670002 Sim max: 1.0
ClusterLoss debug: loss_val=2.852482, ne_loss=0.121964, total=2.974447
Sim min: 0.0015827477909624577 Sim max: 1.0
ClusterLoss debug: loss_val=2.876958, ne_loss=0.070581, total=2.947540
Sim min: 0.0013271019561216235 Sim max: 1.0
ClusterLoss debug: loss_val=2.873328, ne_loss=0.098569, total=2.971897
Sim min: 0.0007495172903873026 Sim max: 1.0
ClusterLoss debug: loss_val=2.873380, ne_loss=0.117679, total=2.991059
Sim min: 0.0009072394459508359 Sim max: 1.0
ClusterLoss debug: loss_val=2.865110, ne_loss=0.121140, total=2.986249
Sim min: 0.002743492601439357 Sim max: 1.0
ClusterLoss debug: loss_val=2.881664, ne_loss=0.215874, total=3.097538
Sim min: 0.0012647962430492043 Sim max: 1.0
ClusterLoss debug: loss_val=2.868676, ne_loss=0.053629, total=2.922305
Sim min: 0.0014040638925507665 Sim max: 1.0
ClusterLoss debug: loss_val=2.897110, ne_loss=0.090429, total=2.987539
Sim min: 0.001263231853954494 Sim max: 1.0
ClusterLoss debug: loss_val=2.886773, ne_loss=0.246994, total=3.133767
Train Epoch: 4 [20608/18637 (55%)]	Instance Loss: 2.295586	Cluster Loss: 3.133767 	sec/iter: 0.8018
Sim min: 0.0017498346278443933 Sim max: 1.0
ClusterLoss debug: loss_val=2.880535, ne_loss=0.146523, total=3.027058
Sim min: 0.0011772887082770467 Sim max: 1.0
ClusterLoss debug: loss_val=2.880376, ne_loss=0.104328, total=2.984704
Sim min: 0.001763301668688655 Sim max: 1.0
ClusterLoss debug: loss_val=2.899527, ne_loss=0.143291, total=3.042818
Sim min: 0.0015162434428930283 Sim max: 1.0
ClusterLoss debug: loss_val=2.876928, ne_loss=0.120176, total=2.997104
Sim min: 0.0024411692284047604 Sim max: 1.0
ClusterLoss debug: loss_val=2.884128, ne_loss=0.089511, total=2.973639
Sim min: 0.0014287906233221292 Sim max: 1.0
ClusterLoss debug: loss_val=2.883505, ne_loss=0.093525, total=2.977030
Sim min: 0.0009103838237933815 Sim max: 1.0
ClusterLoss debug: loss_val=2.869832, ne_loss=0.128032, total=2.997864
Sim min: 0.0008668444352224469 Sim max: 1.0
ClusterLoss debug: loss_val=2.882059, ne_loss=0.132290, total=3.014349
Sim min: 0.0010969014838337898 Sim max: 1.0
ClusterLoss debug: loss_val=2.864843, ne_loss=0.231225, total=3.096068
Sim min: 0.003811824833974242 Sim max: 1.0
ClusterLoss debug: loss_val=2.887145, ne_loss=0.079731, total=2.966876
Train Epoch: 4 [21888/18637 (59%)]	Instance Loss: 2.511837	Cluster Loss: 2.966876 	sec/iter: 0.8012
Sim min: 0.0010623557027429342 Sim max: 1.0
ClusterLoss debug: loss_val=2.881347, ne_loss=0.105660, total=2.987007
Sim min: 0.002715024398639798 Sim max: 1.0
ClusterLoss debug: loss_val=2.871638, ne_loss=0.123456, total=2.995093
Sim min: 0.0011173683451488614 Sim max: 1.0
ClusterLoss debug: loss_val=2.855878, ne_loss=0.083818, total=2.939697
Sim min: 0.0011398560600355268 Sim max: 1.0
ClusterLoss debug: loss_val=2.873589, ne_loss=0.110944, total=2.984534
Sim min: 0.002747550141066313 Sim max: 1.0
ClusterLoss debug: loss_val=2.880973, ne_loss=0.076058, total=2.957031
Sim min: 0.002318273764103651 Sim max: 1.0
ClusterLoss debug: loss_val=2.881041, ne_loss=0.060442, total=2.941482
Sim min: 0.0011392475571483374 Sim max: 1.0
ClusterLoss debug: loss_val=2.868722, ne_loss=0.135114, total=3.003836
Sim min: 0.00221559451892972 Sim max: 1.0
ClusterLoss debug: loss_val=2.866164, ne_loss=0.121614, total=2.987778
Sim min: 0.0012660232605412602 Sim max: 1.0
ClusterLoss debug: loss_val=2.873961, ne_loss=0.066553, total=2.940515
Sim min: 0.001322961412370205 Sim max: 1.0
ClusterLoss debug: loss_val=2.865001, ne_loss=0.081889, total=2.946889
Train Epoch: 4 [23168/18637 (62%)]	Instance Loss: 2.060613	Cluster Loss: 2.946889 	sec/iter: 0.8037
Sim min: 0.001588708022609353 Sim max: 1.0
ClusterLoss debug: loss_val=2.875433, ne_loss=0.131917, total=3.007350
Sim min: 0.0011069285683333874 Sim max: 1.0
ClusterLoss debug: loss_val=2.896917, ne_loss=0.221863, total=3.118780
Sim min: 0.0008599036955274642 Sim max: 1.0
ClusterLoss debug: loss_val=2.881071, ne_loss=0.087536, total=2.968607
Sim min: 0.001588790095411241 Sim max: 1.0
ClusterLoss debug: loss_val=2.878906, ne_loss=0.102360, total=2.981266
Sim min: 0.0018536187708377838 Sim max: 1.0
ClusterLoss debug: loss_val=2.893784, ne_loss=0.052864, total=2.946648
Sim min: 0.001125649199821055 Sim max: 1.0
ClusterLoss debug: loss_val=2.869514, ne_loss=0.115631, total=2.985145
Sim min: 0.0008372768643312156 Sim max: 1.0
ClusterLoss debug: loss_val=2.888932, ne_loss=0.093068, total=2.982000
Sim min: 0.0021003864239901304 Sim max: 1.0
ClusterLoss debug: loss_val=2.907096, ne_loss=0.087685, total=2.994781
Sim min: 0.0013730860082432628 Sim max: 1.0
ClusterLoss debug: loss_val=2.919029, ne_loss=0.145922, total=3.064951
Sim min: 0.0034076895099133253 Sim max: 1.0
ClusterLoss debug: loss_val=2.857632, ne_loss=0.075311, total=2.932943
Train Epoch: 4 [24448/18637 (65%)]	Instance Loss: 2.198275	Cluster Loss: 2.932943 	sec/iter: 0.8041
Sim min: 0.001515561481937766 Sim max: 1.0
ClusterLoss debug: loss_val=2.911977, ne_loss=0.093150, total=3.005127
Sim min: 0.001055489992722869 Sim max: 1.0
ClusterLoss debug: loss_val=2.853561, ne_loss=0.161757, total=3.015318
Sim min: 0.002226032316684723 Sim max: 1.0
ClusterLoss debug: loss_val=2.896106, ne_loss=0.137132, total=3.033237
Sim min: 0.001207918394356966 Sim max: 1.0
ClusterLoss debug: loss_val=2.876978, ne_loss=0.163421, total=3.040399
Sim min: 0.002236471977084875 Sim max: 1.0
ClusterLoss debug: loss_val=2.874002, ne_loss=0.109786, total=2.983788
Sim min: 0.0020794810261577368 Sim max: 1.0
ClusterLoss debug: loss_val=2.873290, ne_loss=0.102424, total=2.975714
Sim min: 0.0020589244086295366 Sim max: 1.0
ClusterLoss debug: loss_val=2.889204, ne_loss=0.124896, total=3.014100
Sim min: 0.0011737756431102753 Sim max: 1.0
ClusterLoss debug: loss_val=2.867283, ne_loss=0.127854, total=2.995137
Sim min: 0.0020035579800605774 Sim max: 1.0
ClusterLoss debug: loss_val=2.870103, ne_loss=0.154695, total=3.024798
Sim min: 0.0011455599451437593 Sim max: 1.0
ClusterLoss debug: loss_val=2.882879, ne_loss=0.218037, total=3.100916
Train Epoch: 4 [25728/18637 (69%)]	Instance Loss: 2.245551	Cluster Loss: 3.100916 	sec/iter: 0.8074
Sim min: 0.0020534268114715815 Sim max: 1.0
ClusterLoss debug: loss_val=2.868729, ne_loss=0.055371, total=2.924100
Sim min: 0.0025051773991435766 Sim max: 1.0
ClusterLoss debug: loss_val=2.872222, ne_loss=0.145594, total=3.017816
Sim min: 0.0022732566576451063 Sim max: 1.0
ClusterLoss debug: loss_val=2.892379, ne_loss=0.094404, total=2.986783
Sim min: 0.002230849815532565 Sim max: 1.0
ClusterLoss debug: loss_val=2.875198, ne_loss=0.072931, total=2.948128
Sim min: 0.0019332764204591513 Sim max: 1.0
ClusterLoss debug: loss_val=2.891045, ne_loss=0.124542, total=3.015586
Sim min: 0.0028976774774491787 Sim max: 1.0
ClusterLoss debug: loss_val=2.869548, ne_loss=0.148705, total=3.018253
Sim min: 0.0018297219648957253 Sim max: 1.0
ClusterLoss debug: loss_val=2.885558, ne_loss=0.068217, total=2.953776
Sim min: 0.001965927891433239 Sim max: 1.0
ClusterLoss debug: loss_val=2.880213, ne_loss=0.056481, total=2.936694
Sim min: 0.0009093683329410851 Sim max: 1.0
ClusterLoss debug: loss_val=2.868563, ne_loss=0.072353, total=2.940916
Sim min: 0.001106339506804943 Sim max: 1.0
ClusterLoss debug: loss_val=2.868991, ne_loss=0.100542, total=2.969532
Train Epoch: 4 [27008/18637 (72%)]	Instance Loss: 2.091745	Cluster Loss: 2.969532 	sec/iter: 0.8066
Sim min: 0.0011433406034484506 Sim max: 1.0
ClusterLoss debug: loss_val=2.899189, ne_loss=0.098577, total=2.997766
Sim min: 0.0021921757142990828 Sim max: 1.0
ClusterLoss debug: loss_val=2.868537, ne_loss=0.108329, total=2.976866
Sim min: 0.0018523783655837178 Sim max: 1.0
ClusterLoss debug: loss_val=2.868047, ne_loss=0.060697, total=2.928744
Sim min: 0.0013180702226236463 Sim max: 1.0
ClusterLoss debug: loss_val=2.868930, ne_loss=0.060317, total=2.929247
Sim min: 0.002747174585238099 Sim max: 1.0
ClusterLoss debug: loss_val=2.875756, ne_loss=0.050066, total=2.925822
Sim min: 0.0010058274492621422 Sim max: 1.0
ClusterLoss debug: loss_val=2.878910, ne_loss=0.204196, total=3.083105
Sim min: 0.0030714087188243866 Sim max: 1.0
ClusterLoss debug: loss_val=2.869104, ne_loss=0.125623, total=2.994727
Sim min: 0.0011505946749821305 Sim max: 1.0
ClusterLoss debug: loss_val=2.870785, ne_loss=0.144292, total=3.015078
Sim min: 0.001376699423417449 Sim max: 1.0
ClusterLoss debug: loss_val=2.882523, ne_loss=0.081048, total=2.963571
Sim min: 0.0011300017358735204 Sim max: 1.0
ClusterLoss debug: loss_val=2.886702, ne_loss=0.170722, total=3.057423
Train Epoch: 4 [28288/18637 (76%)]	Instance Loss: 2.168468	Cluster Loss: 3.057423 	sec/iter: 0.8131
Sim min: 0.001781645929440856 Sim max: 1.0
ClusterLoss debug: loss_val=2.872845, ne_loss=0.121189, total=2.994034
Sim min: 0.0016878976020962 Sim max: 1.0
ClusterLoss debug: loss_val=2.908128, ne_loss=0.167549, total=3.075677
Sim min: 0.0019692706409841776 Sim max: 1.0
ClusterLoss debug: loss_val=2.915050, ne_loss=0.133789, total=3.048839
Sim min: 0.0007987421122379601 Sim max: 1.0
ClusterLoss debug: loss_val=2.867592, ne_loss=0.072650, total=2.940242
Sim min: 0.0007820589817129076 Sim max: 1.0
ClusterLoss debug: loss_val=2.859761, ne_loss=0.091416, total=2.951177
Sim min: 0.001355634187348187 Sim max: 1.0
ClusterLoss debug: loss_val=2.874372, ne_loss=0.148575, total=3.022947
Sim min: 0.001956094754859805 Sim max: 1.0
ClusterLoss debug: loss_val=2.875761, ne_loss=0.130752, total=3.006513
Sim min: 0.0015840799314901233 Sim max: 1.0
ClusterLoss debug: loss_val=2.858594, ne_loss=0.044079, total=2.902674
Sim min: 0.0019010048126801848 Sim max: 1.0
ClusterLoss debug: loss_val=2.885728, ne_loss=0.209521, total=3.095248
Sim min: 0.0014331901911646128 Sim max: 1.0
ClusterLoss debug: loss_val=2.873060, ne_loss=0.101688, total=2.974748
Train Epoch: 4 [29568/18637 (79%)]	Instance Loss: 2.362483	Cluster Loss: 2.974748 	sec/iter: 0.8159
Sim min: 0.002474755747243762 Sim max: 1.0
ClusterLoss debug: loss_val=2.869707, ne_loss=0.090087, total=2.959794
Sim min: 0.0021423925645649433 Sim max: 1.0
ClusterLoss debug: loss_val=2.880078, ne_loss=0.041907, total=2.921985
Sim min: 0.0005505745066329837 Sim max: 1.0
ClusterLoss debug: loss_val=2.863816, ne_loss=0.222420, total=3.086236
Sim min: 0.0006537868757732213 Sim max: 1.0
ClusterLoss debug: loss_val=2.868242, ne_loss=0.113992, total=2.982234
Sim min: 0.0006732924957759678 Sim max: 1.0
ClusterLoss debug: loss_val=2.871583, ne_loss=0.102311, total=2.973895
Sim min: 0.0013101808726787567 Sim max: 1.0
ClusterLoss debug: loss_val=2.875088, ne_loss=0.074327, total=2.949414
Sim min: 0.0017515006475150585 Sim max: 1.0
ClusterLoss debug: loss_val=2.888437, ne_loss=0.107991, total=2.996429
Sim min: 0.002331140451133251 Sim max: 1.0
ClusterLoss debug: loss_val=2.875388, ne_loss=0.125996, total=3.001384
Sim min: 0.002905532717704773 Sim max: 1.0
ClusterLoss debug: loss_val=2.889776, ne_loss=0.177703, total=3.067479
Sim min: 0.0027742062229663134 Sim max: 1.0
ClusterLoss debug: loss_val=2.866374, ne_loss=0.034550, total=2.900924
Train Epoch: 4 [30848/18637 (83%)]	Instance Loss: 2.192779	Cluster Loss: 2.900924 	sec/iter: 0.8156
Sim min: 0.0012667460832744837 Sim max: 1.0
ClusterLoss debug: loss_val=2.863670, ne_loss=0.113297, total=2.976967
Sim min: 0.0005925822188146412 Sim max: 1.0
ClusterLoss debug: loss_val=2.892124, ne_loss=0.208118, total=3.100242
Sim min: 0.0013226389419287443 Sim max: 1.0
ClusterLoss debug: loss_val=2.861543, ne_loss=0.082310, total=2.943853
Sim min: 0.002083649393171072 Sim max: 1.0
ClusterLoss debug: loss_val=2.877055, ne_loss=0.137621, total=3.014676
Sim min: 0.0009941316675394773 Sim max: 1.0
ClusterLoss debug: loss_val=2.868331, ne_loss=0.059965, total=2.928296
Sim min: 0.0032799167092889547 Sim max: 1.0
ClusterLoss debug: loss_val=2.883253, ne_loss=0.125180, total=3.008434
Sim min: 0.002093997783958912 Sim max: 1.0
ClusterLoss debug: loss_val=2.887436, ne_loss=0.085522, total=2.972958
Sim min: 0.0026965830475091934 Sim max: 1.0
ClusterLoss debug: loss_val=2.893741, ne_loss=0.141278, total=3.035020
Sim min: 0.0010382425971329212 Sim max: 1.0
ClusterLoss debug: loss_val=2.864795, ne_loss=0.077091, total=2.941885
Sim min: 0.0030378540977835655 Sim max: 1.0
ClusterLoss debug: loss_val=2.882560, ne_loss=0.056000, total=2.938560
Train Epoch: 4 [32128/18637 (86%)]	Instance Loss: 2.295176	Cluster Loss: 2.938560 	sec/iter: 0.8161
Sim min: 0.0022797007113695145 Sim max: 1.0
ClusterLoss debug: loss_val=2.904977, ne_loss=0.057161, total=2.962138
Sim min: 0.0017979239346459508 Sim max: 1.0
ClusterLoss debug: loss_val=2.867419, ne_loss=0.114347, total=2.981766
Sim min: 0.0013664858415722847 Sim max: 1.0
ClusterLoss debug: loss_val=2.862324, ne_loss=0.117263, total=2.979587
Sim min: 0.001746692112646997 Sim max: 1.0
ClusterLoss debug: loss_val=2.878732, ne_loss=0.061472, total=2.940204
Sim min: 0.0016456118319183588 Sim max: 1.0
ClusterLoss debug: loss_val=2.878312, ne_loss=0.082723, total=2.961035
Sim min: 0.0014307257952168584 Sim max: 1.0
ClusterLoss debug: loss_val=2.863326, ne_loss=0.111329, total=2.974655
Sim min: 0.0011610729852691293 Sim max: 1.0
ClusterLoss debug: loss_val=2.882535, ne_loss=0.113926, total=2.996461
Sim min: 0.0020396194886416197 Sim max: 1.0
ClusterLoss debug: loss_val=2.900495, ne_loss=0.112230, total=3.012725
Sim min: 0.0009896233677864075 Sim max: 1.0
ClusterLoss debug: loss_val=2.857202, ne_loss=0.074337, total=2.931539
Sim min: 0.0015837883111089468 Sim max: 1.0
ClusterLoss debug: loss_val=2.873218, ne_loss=0.103757, total=2.976975
Train Epoch: 4 [33408/18637 (89%)]	Instance Loss: 2.295852	Cluster Loss: 2.976975 	sec/iter: 0.8144
Sim min: 0.0012924609472975135 Sim max: 1.0
ClusterLoss debug: loss_val=2.858927, ne_loss=0.146867, total=3.005794
Sim min: 0.0016719833947718143 Sim max: 1.0
ClusterLoss debug: loss_val=2.866529, ne_loss=0.137475, total=3.004004
Sim min: 0.0013824200723320246 Sim max: 1.0
ClusterLoss debug: loss_val=2.868026, ne_loss=0.202875, total=3.070901
Sim min: 0.0009694657637737691 Sim max: 1.0
ClusterLoss debug: loss_val=2.878595, ne_loss=0.124790, total=3.003385
Sim min: 0.0013539873762056231 Sim max: 1.0
ClusterLoss debug: loss_val=2.894296, ne_loss=0.121447, total=3.015744
Sim min: 0.0006778996903449297 Sim max: 1.0
ClusterLoss debug: loss_val=2.880841, ne_loss=0.113862, total=2.994703
Sim min: 0.002637333469465375 Sim max: 1.0
ClusterLoss debug: loss_val=2.898535, ne_loss=0.158632, total=3.057167
Sim min: 0.002969317836686969 Sim max: 1.0
ClusterLoss debug: loss_val=2.878857, ne_loss=0.073698, total=2.952555
Sim min: 0.0009595360024832189 Sim max: 1.0
ClusterLoss debug: loss_val=2.895271, ne_loss=0.186097, total=3.081368
Sim min: 0.0007148191798478365 Sim max: 1.0
ClusterLoss debug: loss_val=2.869031, ne_loss=0.092140, total=2.961171
Train Epoch: 4 [34688/18637 (93%)]	Instance Loss: 2.403421	Cluster Loss: 2.961171 	sec/iter: 0.8141
Sim min: 0.0019430659012869 Sim max: 1.0
ClusterLoss debug: loss_val=2.873377, ne_loss=0.174263, total=3.047640
Sim min: 0.0019251172197982669 Sim max: 1.0
ClusterLoss debug: loss_val=2.875481, ne_loss=0.088786, total=2.964267
Sim min: 0.0013453832361847162 Sim max: 1.0
ClusterLoss debug: loss_val=2.872952, ne_loss=0.087086, total=2.960037
Sim min: 0.0011201961897313595 Sim max: 1.0
ClusterLoss debug: loss_val=2.867573, ne_loss=0.089490, total=2.957064
Sim min: 0.0022405802737921476 Sim max: 1.0
ClusterLoss debug: loss_val=2.864520, ne_loss=0.137069, total=3.001589
Sim min: 0.0007730084471404552 Sim max: 1.0
ClusterLoss debug: loss_val=2.858404, ne_loss=0.103755, total=2.962159
Sim min: 0.0017211873782798648 Sim max: 1.0
ClusterLoss debug: loss_val=2.885045, ne_loss=0.157476, total=3.042521
Sim min: 0.001309804036282003 Sim max: 1.0
ClusterLoss debug: loss_val=2.875920, ne_loss=0.065212, total=2.941132
Sim min: 0.0018643359653651714 Sim max: 1.0
ClusterLoss debug: loss_val=2.866858, ne_loss=0.118320, total=2.985178
Sim min: 0.001147923176176846 Sim max: 1.0
ClusterLoss debug: loss_val=2.881602, ne_loss=0.089916, total=2.971517
Train Epoch: 4 [35968/18637 (96%)]	Instance Loss: 2.261371	Cluster Loss: 2.971517 	sec/iter: 0.8148
Sim min: 0.0024632324930280447 Sim max: 1.0
ClusterLoss debug: loss_val=2.873034, ne_loss=0.085741, total=2.958776
Sim min: 0.002038734732195735 Sim max: 1.0
ClusterLoss debug: loss_val=2.886831, ne_loss=0.073122, total=2.959953
Sim min: 0.0010237812530249357 Sim max: 1.0
ClusterLoss debug: loss_val=2.886790, ne_loss=0.219124, total=3.105914
Sim min: 0.001664065639488399 Sim max: 1.0
ClusterLoss debug: loss_val=2.858631, ne_loss=0.066671, total=2.925302
Sim min: 0.0021823220886290073 Sim max: 1.0
ClusterLoss debug: loss_val=2.871588, ne_loss=0.061513, total=2.933101
Sim min: 0.0019742061849683523 Sim max: 1.0
ClusterLoss debug: loss_val=2.872608, ne_loss=0.111474, total=2.984081
Sim min: 0.002128932159394026 Sim max: 1.0
ClusterLoss debug: loss_val=2.867981, ne_loss=0.101171, total=2.969152
Sim min: 0.0019082835642620921 Sim max: 1.0
ClusterLoss debug: loss_val=2.878489, ne_loss=0.087000, total=2.965489
Sim min: 0.0020738686434924603 Sim max: 1.0
ClusterLoss debug: loss_val=2.903636, ne_loss=0.081995, total=2.985631
Sim min: 0.001652766251936555 Sim max: 1.0
ClusterLoss debug: loss_val=2.898294, ne_loss=0.131338, total=3.029632
Train Epoch: 4 [37248/18637 (100%)]	Instance Loss: 2.185915	Cluster Loss: 3.029632 	sec/iter: 0.8148
Sim min: 0.0023850277066230774 Sim max: 1.0
ClusterLoss debug: loss_val=3.000904, ne_loss=0.286747, total=3.287651
Train Epoch: 4 [37274/18637 (100%)]	Instance Loss: 0.417202	Cluster Loss: 3.287651 	sec/iter: 0.8126
Model saved to model_epoch_4.pth
请输入数字1以继续运行: 
===== 分类评估 =====
Epoch 4 - Loss: 3.0969, Acc: 4.54%
Recall:     0.0800 0.0000 0.0000 0.0500 0.0000 0.0100 0.0000 0.0000 0.1400 0.2400 0.0000 0.0000 0.0000 0.0000 0.0100 0.0141 0.3900 0.0100 0.0000 0.0000 0.0000
Precision:  0.0369 0.0000 0.0000 0.0153 0.0000 0.0526 0.0000 0.0000 0.0586 0.2124 0.0000 0.0000 0.0000 0.0000 0.0227 0.0065 0.1121 0.0066 0.0000 0.0000 0.0000
F1-score:   0.0505 0.0000 0.0000 0.0234 0.0000 0.0168 0.0000 0.0000 0.0826 0.2254 0.0000 0.0000 0.0000 0.0000 0.0139 0.0089 0.1741 0.0080 0.0000 0.0000 0.0000

===== 聚类评估 =====
混淆矩阵:
 [[ 0  0  0  0  0  0  0 76  0  0  0  0  0 21  0  0  0  3  0  0  0]
 [21  0  0  0  0 76  0  1  0  0  0  0  0  1  0  0  0  1  0  0  0]
 [ 2  0  0  0  0  0  0 77  0  0  0  0  0  8  0  0  0 12  1  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  2  0  0 20  0  0  0  1  4 70  0]
 [ 0  0  0  0  0  0  0 67  0  0  0  0  0  5  0  0  0 28  0  0  0]
 [83  0  0  1  0  0  0  4  0  0  0  0  0 10  0  0  0  1  1  0  0]
 [ 0 12  0 21  4  0  1  3 30  0  0  0  7  0  0 16  0  0  0  1  5]
 [ 0  0  0  0  0  0  0  0  0 11  0  0  0 89  0  0  0  0  0  0  0]
 [ 0  0  0  4  0  1  0 86  0  0  0  0  0  8  0  0  0  1  0  0  0]
 [ 0  0  0  0  0  0  0 63  0  0  0  0  0  2  0  0  0 35  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  1  0 98  0  0  0  0  0  0  0]
 [ 0  0 25 64  0  0  0  5  0  0  0  0  0  2  0  0  2  2  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0 19 80  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  5  0  0  0 28  0 42  0  0  0  0  0 25  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  5  0 93  0  0  0  0  0  0  0]
 [ 0  0  0 11  0  0  0 58  0  0  0  0  0  0  0  0  0  2  0  0  0]
 [96  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  1  0  0  0]
 [10  0  0  0  0  0  0  1  0  0  0  0  0 88  0  0  0  0  0  1  0]
 [ 0  0  0  0  0  0  0 97  0  0  0  0  0  1  0  0  0  1  0  1  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0 99  0  0  0  0  0  0  0]
 [ 0  0  0  0  0 93  0  4  0  0  0  0  0  2  0  0  0  1  0  0  0]]
代价矩阵:
 [[212. 191. 210. 212. 212. 129. 212. 212. 212. 212. 212. 212. 212. 212.
  212. 212. 116. 202. 212. 212. 212.]
 [ 12.  12.  12.  12.  12.  12.   0.  12.  12.  12.  12.  12.  12.  12.
   12.  12.  12.  12.  12.  12.  12.]
 [ 25.  25.  25.  25.  25.  25.  25.  25.  25.  25.  25.   0.  25.  25.
   25.  25.  25.  25.  25.  25.  25.]
 [102. 102. 102. 102. 102. 101.  81. 102.  98. 102. 102.  38. 101. 102.
  102.  91. 102. 102. 102. 102. 102.]
 [  4.   4.   4.   4.   4.   4.   0.   4.   4.   4.   4.   4.   4.   4.
    4.   4.   4.   4.   4.   4.   4.]
 [170.  94. 170. 170. 170. 170. 170. 170. 169. 170. 170. 170. 170. 170.
  170. 170. 170. 170. 170. 170.  77.]
 [  1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.]
 [479. 554. 478. 552. 488. 551. 552. 555. 469. 492. 554. 550. 555. 550.
  553. 497. 554. 554. 458. 554. 551.]
 [ 30.  30.  30.  30.  30.  30.   0.  30.  30.  30.  30.  30.  30.  30.
   30.  30.  30.  30.  30.  30.  30.]
 [ 11.  11.  11.  11.  11.  11.  11.   0.  11.  11.  11.  11.  11.  11.
   11.  11.  11.  11.  11.  11.  11.]
 [  2.   2.   2.   0.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [ 34.  34.  34.  34.  34.  34.  34.  34.  34.  34.  33.  34.  34.   6.
   29.  34.  34.  34.  34.  34.  34.]
 [  7.   7.   7.   7.   7.   7.   0.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.]
 [589. 609. 602. 590. 605. 600. 610. 521. 602. 608. 512. 608. 591. 568.
  517. 610. 608. 522. 609. 511. 608.]
 [ 80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.   0.  80.
   80.  80.  80.  80.  80.  80.  80.]
 [ 16.  16.  16.  16.  16.  16.   0.  16.  16.  16.  16.  16.  16.  16.
   16.  16.  16.  16.  16.  16.  16.]
 [  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [ 86.  88.  77.  88.  61.  88.  89.  89.  88.  54.  89.  87.  89.  89.
   89.  87.  88.  89.  88.  89.  88.]
 [  6.   6.   5.   2.   6.   5.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.]
 [ 98.  98.  98.  28.  98.  98.  97.  98.  98.  98.  98.  98.  98.  73.
   98.  98.  98.  97.  97.  98.  98.]
 [  5.   5.   5.   5.   5.   5.   0.   5.   5.   5.   5.   5.   5.   5.
    5.   5.   5.   5.   5.   5.   5.]]
最佳映射关系: [16.  0. 17. 11. 10. 20.  8. 18.  6.  7.  1. 13. 14. 19. 12. 15.  2.  9.
  5.  3.  4.]
聚类分配的最大索引: 20

=== 实例级嵌入 ===
NMI: 0.5667  ARI: 0.2196
F-score: 0.3451  Silhouette: 0.2488
Adjusted Acc: 0.3399
混淆矩阵:
 [[32  0  0  1  0 23  0  4 30  0  0  0  0  0  0  5  0  0  0  5  0]
 [ 8  4  0  0  7  0  0  0  1  0  0 37 37  0  0  4  0  1  1  0  0]
 [22 14  1  0  0 10  1  0 24  0  4  0  0  0  0  4  0  6  2  1 11]
 [ 3  0 25  0  0  0 16  1  2  3  0  0  1  0  0  1  0 35 11  2  0]
 [ 5  5  2  0 11 15  0  0 14  0 33  0  0  0  0  3  0  0  1 11  0]
 [ 8  2  2  7  8 18  1 11  8  0  0  7  0  0  0 14  0  2  1  5  6]
 [ 0  0 33  0  0  0 23  0  0 38  0  0  0  0  4  0  0  2  0  0  0]
 [ 0  0 35  0  0  0  1  0  0  3  0  0  0  0  8  0  0  5 48  0  0]
 [ 1  0  1  9 12  3 18  0 21  0  2  0  2  0  0 14 12  0  1  1  3]
 [ 0  0  9 35  0  0  0  0 23  0  0  0  0  0  0  0 31  1  0  1  0]
 [ 7  1  0 67  0  1  1  1 10  0  0  0  0  0  0  3  8  1  0  0  0]
 [ 1  0 13  0  1  0 26  0  0  0  0  0  0 27  0  0  0 10 22  0  0]
 [ 0  0  3  0  6  0  0  6  0  0  0 18  4  0 45  8  0  8  1  1  0]
 [ 5 12  1  0  1  2  3 57  3  0  0  0  0  0  0  7  0  0  3  4  2]
 [ 1  2  5  0  0  7  0 60  4  0  0  0  0  0  0  9  0  2  1  1  8]
 [ 0  0  3  0  2  0 12  1 18  0  0  0  0  0  0  5  0  1 20  0  9]
 [ 8  4  0  0 40  0  0  1 15  0  5  0  0  0  0 16  0  2  0  9  0]
 [ 4  4  1 29  2  1  0  3 10  0  3  0  0  0  0 16  0  0  0  6 21]
 [ 2  0  0  0  0  1  0  1  3  0 57  0  0  0  0  0  0  1  0  0 35]
 [ 4  0  0  1  1 12  3  0  4  0  0  0  0  0  0 20  0  0  3 32 20]
 [ 0 42  0  0  0  0  0 19  0  0  1  0 31  0  0  5  0  1  1  0  0]]
代价矩阵:
 [[ 79. 103.  89. 108. 106. 103. 111. 111. 110. 111. 104. 110. 111. 106.
  110. 111. 103. 107. 109. 107. 111.]
 [ 90.  86.  76.  90.  85.  88.  90.  90.  90.  90.  89.  90.  90.  78.
   88.  90.  86.  86.  90.  90.  48.]
 [134. 134. 133. 109. 132. 132. 101.  99. 133. 125. 134. 121. 131. 133.
  129. 131. 134. 133. 134. 134. 134.]
 [148. 149. 149. 149. 149. 142. 149. 149. 140. 114.  82. 149. 149. 149.
  149. 149. 149. 120. 149. 148. 149.]
 [ 91.  84.  91.  91.  80.  83.  91.  91.  79.  91.  91.  90.  85.  90.
   91.  89.  51.  89.  91.  90.  91.]
 [ 70.  93.  83.  93.  78.  75.  93.  93.  90.  93.  92.  93.  93.  91.
   86.  93.  93.  92.  92.  81.  93.]
 [105. 105. 104.  89. 105. 104.  82. 104.  87. 105. 104.  79. 105. 102.
  105.  93. 105. 105. 105. 102. 105.]
 [161. 165. 165. 164. 165. 154. 165. 165. 165. 165. 164. 165. 159. 108.
  105. 164. 164. 162. 164. 165. 146.]
 [160. 189. 166. 188. 176. 182. 190. 190. 169. 167. 180. 190. 190. 187.
  186. 172. 175. 180. 187. 186. 190.]
 [ 44.  44.  44.  41.  44.  44.   6.  41.  44.  44.  44.  44.  44.  44.
   44.  44.  44.  44.  44.  44.  44.]
 [105. 105. 101. 105.  72. 105. 105. 105. 103. 105. 105. 105. 105. 105.
  105. 105. 100. 102.  48. 105. 104.]
 [ 62.  25.  62.  62.  62.  55.  62.  62.  62.  62.  62.  62.  44.  62.
   62.  62.  62.  62.  62.  62.  62.]
 [ 75.  38.  75.  74.  75.  75.  75.  75.  73.  75.  75.  75.  71.  75.
   75.  75.  75.  75.  75.  75.  44.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 57.  57.  57.  57.  57.  57.  53.  49.  57.  57.  57.  57.  12.  57.
   57.  57.  57.  57.  57.  57.  57.]
 [129. 130. 130. 133. 131. 120. 134. 134. 120. 134. 131. 134. 126. 127.
  125. 129. 118. 118. 134. 114. 129.]
 [ 51.  51.  51.  51.  51.  51.  51.  51.  39.  20.  43.  51.  51.  51.
   51.  51.  51.  51.  51.  51.  51.]
 [ 78.  77.  72.  43.  78.  76.  76.  73.  78.  77.  77.  68.  70.  78.
   76.  77.  76.  78.  77.  78.  77.]
 [116. 115. 114. 105. 115. 115. 116.  68. 115. 116. 116.  94. 115. 113.
  115.  96. 116. 116. 116. 113. 115.]
 [ 74.  79.  78.  77.  68.  74.  79.  79.  78.  78.  79.  79.  78.  75.
   78.  79.  70.  73.  79.  47.  79.]
 [115. 115. 104. 115. 115. 109. 115. 115. 112. 115. 115. 115. 115. 113.
  107. 106. 115.  94.  80.  95. 115.]]
最佳映射关系: [ 0. 13.  7. 10. 16.  4.  8. 14.  2.  6. 18.  1. 20. 11. 12.  5.  9.  3.
 15. 19. 17.]
聚类分配的最大索引: 20

=== 簇级嵌入 ===
NMI: 0.4355  ARI: 0.1974
F-score: 0.2389  Silhouette: 0.5573
Adjusted Acc: 0.3337

总耗时: 33.85s
分类准确率: 4.54%
簇级嵌入NMI: 0.4355
请输入数字2以继续运行: Sim min: 0.005257229320704937 Sim max: 1.0
ClusterLoss debug: loss_val=2.938235, ne_loss=0.121944, total=3.060179
Train Epoch: 5 [128/18637 (0%)]	Instance Loss: 2.345826	Cluster Loss: 3.060179 	sec/iter: 0.7087
Sim min: 0.0008752954308874905 Sim max: 1.0
ClusterLoss debug: loss_val=2.870000, ne_loss=0.174335, total=3.044335
Sim min: 0.0024166617076843977 Sim max: 1.0
ClusterLoss debug: loss_val=2.905764, ne_loss=0.090258, total=2.996022
Sim min: 0.002226745942607522 Sim max: 1.0
ClusterLoss debug: loss_val=2.892145, ne_loss=0.158026, total=3.050170
Sim min: 0.002808772725984454 Sim max: 1.0
ClusterLoss debug: loss_val=2.899889, ne_loss=0.080036, total=2.979925
Sim min: 0.001723888679407537 Sim max: 1.0
ClusterLoss debug: loss_val=2.889701, ne_loss=0.075239, total=2.964940
Sim min: 0.0014639373403042555 Sim max: 1.0
ClusterLoss debug: loss_val=2.881726, ne_loss=0.081888, total=2.963614
Sim min: 0.002414705464616418 Sim max: 1.0
ClusterLoss debug: loss_val=2.869302, ne_loss=0.113538, total=2.982840
Sim min: 0.0011167399352416396 Sim max: 1.0
ClusterLoss debug: loss_val=2.895175, ne_loss=0.087410, total=2.982585
Sim min: 0.0025741897989064455 Sim max: 1.0
ClusterLoss debug: loss_val=2.870297, ne_loss=0.150814, total=3.021111
Sim min: 0.0013988111168146133 Sim max: 1.0
ClusterLoss debug: loss_val=2.860494, ne_loss=0.076209, total=2.936702
Train Epoch: 5 [1408/18637 (4%)]	Instance Loss: 2.101114	Cluster Loss: 2.936702 	sec/iter: 0.7600
Sim min: 0.0011669291416183114 Sim max: 1.0
ClusterLoss debug: loss_val=2.864479, ne_loss=0.069126, total=2.933604
Sim min: 0.0020863718818873167 Sim max: 1.0
ClusterLoss debug: loss_val=2.858309, ne_loss=0.070671, total=2.928979
Sim min: 0.0015729882288724184 Sim max: 1.0
ClusterLoss debug: loss_val=2.870728, ne_loss=0.076253, total=2.946981
Sim min: 0.0012161084450781345 Sim max: 1.0
ClusterLoss debug: loss_val=2.864474, ne_loss=0.055463, total=2.919937
Sim min: 0.0015904257306829095 Sim max: 1.0
ClusterLoss debug: loss_val=2.875312, ne_loss=0.056327, total=2.931638
Sim min: 0.0014066159492358565 Sim max: 1.0
ClusterLoss debug: loss_val=2.894784, ne_loss=0.071826, total=2.966610
Sim min: 0.0017036409117281437 Sim max: 1.0
ClusterLoss debug: loss_val=2.916130, ne_loss=0.168009, total=3.084139
Sim min: 0.0014234851114451885 Sim max: 1.0
ClusterLoss debug: loss_val=2.873971, ne_loss=0.071882, total=2.945854
Sim min: 0.001188835478387773 Sim max: 1.0
ClusterLoss debug: loss_val=2.847111, ne_loss=0.124520, total=2.971631
Sim min: 0.0016708020120859146 Sim max: 1.0
ClusterLoss debug: loss_val=2.880443, ne_loss=0.123638, total=3.004081
Train Epoch: 5 [2688/18637 (7%)]	Instance Loss: 2.079267	Cluster Loss: 3.004081 	sec/iter: 0.8036
Sim min: 0.0026549082249403 Sim max: 1.0
ClusterLoss debug: loss_val=2.900155, ne_loss=0.040691, total=2.940846
Sim min: 0.001180664636194706 Sim max: 1.0
ClusterLoss debug: loss_val=2.863993, ne_loss=0.093468, total=2.957461
Sim min: 0.0010782567551359534 Sim max: 1.0
ClusterLoss debug: loss_val=2.878856, ne_loss=0.181772, total=3.060628
Sim min: 0.001972911646589637 Sim max: 1.0
ClusterLoss debug: loss_val=2.892868, ne_loss=0.061267, total=2.954135
Sim min: 0.0015684578102082014 Sim max: 1.0
ClusterLoss debug: loss_val=2.898942, ne_loss=0.164452, total=3.063394
Sim min: 0.002365992870181799 Sim max: 1.0
ClusterLoss debug: loss_val=2.884829, ne_loss=0.085954, total=2.970783
Sim min: 0.0011241271859034896 Sim max: 1.0
ClusterLoss debug: loss_val=2.877227, ne_loss=0.106946, total=2.984173
Sim min: 0.0009622995276004076 Sim max: 1.0
ClusterLoss debug: loss_val=2.881851, ne_loss=0.131854, total=3.013705
Sim min: 0.0009554601856507361 Sim max: 1.0
ClusterLoss debug: loss_val=2.867837, ne_loss=0.171004, total=3.038841
Sim min: 0.002643516520038247 Sim max: 1.0
ClusterLoss debug: loss_val=2.885420, ne_loss=0.138170, total=3.023591
Train Epoch: 5 [3968/18637 (11%)]	Instance Loss: 2.202328	Cluster Loss: 3.023591 	sec/iter: 0.7998
Sim min: 0.0014163849409669638 Sim max: 1.0
ClusterLoss debug: loss_val=2.872225, ne_loss=0.146212, total=3.018437
Sim min: 0.0022540960926562548 Sim max: 1.0
ClusterLoss debug: loss_val=2.886066, ne_loss=0.131780, total=3.017846
Sim min: 0.0008886698051355779 Sim max: 1.0
ClusterLoss debug: loss_val=2.875770, ne_loss=0.121975, total=2.997744
Sim min: 0.001901731244288385 Sim max: 1.0
ClusterLoss debug: loss_val=2.886849, ne_loss=0.080205, total=2.967054
Sim min: 0.0015196079621091485 Sim max: 1.0
ClusterLoss debug: loss_val=2.870981, ne_loss=0.129030, total=3.000011
Sim min: 0.003641159040853381 Sim max: 1.0
ClusterLoss debug: loss_val=2.875050, ne_loss=0.142651, total=3.017701
Sim min: 0.0030870605260133743 Sim max: 1.0
ClusterLoss debug: loss_val=2.894514, ne_loss=0.101695, total=2.996209
Sim min: 0.005297985393553972 Sim max: 1.0
ClusterLoss debug: loss_val=2.887756, ne_loss=0.185992, total=3.073748
Sim min: 0.0015269635478034616 Sim max: 1.0
ClusterLoss debug: loss_val=2.863300, ne_loss=0.102798, total=2.966097
Sim min: 0.0007356248097494245 Sim max: 1.0
ClusterLoss debug: loss_val=2.862782, ne_loss=0.177744, total=3.040526
Train Epoch: 5 [5248/18637 (14%)]	Instance Loss: 2.005432	Cluster Loss: 3.040526 	sec/iter: 0.8002
Sim min: 0.0025550215505063534 Sim max: 1.0
ClusterLoss debug: loss_val=2.881071, ne_loss=0.061498, total=2.942569
Sim min: 0.0011588014895096421 Sim max: 1.0
ClusterLoss debug: loss_val=2.901884, ne_loss=0.228824, total=3.130708
Sim min: 0.002709653927013278 Sim max: 1.0
ClusterLoss debug: loss_val=2.885440, ne_loss=0.083378, total=2.968818
Sim min: 0.001387759461067617 Sim max: 1.0
ClusterLoss debug: loss_val=2.861890, ne_loss=0.151183, total=3.013073
Sim min: 0.0019346908666193485 Sim max: 1.0
ClusterLoss debug: loss_val=2.894074, ne_loss=0.117158, total=3.011232
Sim min: 0.00182897609192878 Sim max: 1.0
ClusterLoss debug: loss_val=2.866226, ne_loss=0.084787, total=2.951012
Sim min: 0.002727002138271928 Sim max: 1.0
ClusterLoss debug: loss_val=2.917518, ne_loss=0.114369, total=3.031887
Sim min: 0.0009067290811799467 Sim max: 1.0
ClusterLoss debug: loss_val=2.878620, ne_loss=0.223192, total=3.101812
Sim min: 0.00401709508150816 Sim max: 1.0
ClusterLoss debug: loss_val=2.885243, ne_loss=0.081762, total=2.967005
Sim min: 0.0012363276910036802 Sim max: 1.0
ClusterLoss debug: loss_val=2.908235, ne_loss=0.190488, total=3.098722
Train Epoch: 5 [6528/18637 (17%)]	Instance Loss: 2.235375	Cluster Loss: 3.098722 	sec/iter: 0.8031
Sim min: 0.0018533265683799982 Sim max: 1.0
ClusterLoss debug: loss_val=2.865886, ne_loss=0.106725, total=2.972610
Sim min: 0.0012676549376919866 Sim max: 1.0
ClusterLoss debug: loss_val=2.877725, ne_loss=0.025790, total=2.903515
Sim min: 0.001891666674055159 Sim max: 1.0
ClusterLoss debug: loss_val=2.912937, ne_loss=0.124117, total=3.037054
Sim min: 0.0017510707257315516 Sim max: 1.0
ClusterLoss debug: loss_val=2.883269, ne_loss=0.064201, total=2.947470
Sim min: 0.0023550204932689667 Sim max: 1.0
ClusterLoss debug: loss_val=2.880441, ne_loss=0.210993, total=3.091434
Sim min: 0.002788335783407092 Sim max: 1.0
ClusterLoss debug: loss_val=2.884900, ne_loss=0.088670, total=2.973570
Sim min: 0.0011330898851156235 Sim max: 1.0
ClusterLoss debug: loss_val=2.872027, ne_loss=0.098473, total=2.970500
Sim min: 0.0005650182720273733 Sim max: 1.0
ClusterLoss debug: loss_val=2.852419, ne_loss=0.140188, total=2.992607
Sim min: 0.0007883026846684515 Sim max: 1.0
ClusterLoss debug: loss_val=2.859725, ne_loss=0.228588, total=3.088313
Sim min: 0.0017456338973715901 Sim max: 1.0
ClusterLoss debug: loss_val=2.865409, ne_loss=0.255842, total=3.121251
Train Epoch: 5 [7808/18637 (21%)]	Instance Loss: 2.122611	Cluster Loss: 3.121251 	sec/iter: 0.7963
Sim min: 0.001593955559656024 Sim max: 1.0
ClusterLoss debug: loss_val=2.874626, ne_loss=0.125062, total=2.999688
Sim min: 0.0016483908984810114 Sim max: 1.0
ClusterLoss debug: loss_val=2.883222, ne_loss=0.121645, total=3.004868
Sim min: 0.0006932348478585482 Sim max: 1.0
ClusterLoss debug: loss_val=2.889566, ne_loss=0.145012, total=3.034578
Sim min: 0.0013690181076526642 Sim max: 1.0
ClusterLoss debug: loss_val=2.882708, ne_loss=0.147656, total=3.030365
Sim min: 0.003951686434447765 Sim max: 1.0
ClusterLoss debug: loss_val=2.870357, ne_loss=0.117192, total=2.987550
Sim min: 0.0013044613879173994 Sim max: 1.0
ClusterLoss debug: loss_val=2.875354, ne_loss=0.134989, total=3.010343
Sim min: 0.0025300823617726564 Sim max: 1.0
ClusterLoss debug: loss_val=2.912729, ne_loss=0.170911, total=3.083640
Sim min: 0.0017972575733438134 Sim max: 1.0
ClusterLoss debug: loss_val=2.878433, ne_loss=0.089957, total=2.968390
Sim min: 0.0016812380636110902 Sim max: 1.0
ClusterLoss debug: loss_val=2.888088, ne_loss=0.065562, total=2.953650
Sim min: 0.0017341941129416227 Sim max: 1.0
ClusterLoss debug: loss_val=2.900640, ne_loss=0.152006, total=3.052646
Train Epoch: 5 [9088/18637 (24%)]	Instance Loss: 2.311120	Cluster Loss: 3.052646 	sec/iter: 0.7993
Sim min: 0.002344661857932806 Sim max: 1.0
ClusterLoss debug: loss_val=2.893699, ne_loss=0.051353, total=2.945051
Sim min: 0.0016351662343367934 Sim max: 1.0
ClusterLoss debug: loss_val=2.856159, ne_loss=0.093277, total=2.949436
Sim min: 0.0033109658397734165 Sim max: 1.0
ClusterLoss debug: loss_val=2.895384, ne_loss=0.095339, total=2.990723
Sim min: 0.0012761633843183517 Sim max: 1.0
ClusterLoss debug: loss_val=2.861916, ne_loss=0.080669, total=2.942585
Sim min: 0.0022987413685768843 Sim max: 1.0
ClusterLoss debug: loss_val=2.900200, ne_loss=0.080125, total=2.980325
Sim min: 0.002087014727294445 Sim max: 1.0
ClusterLoss debug: loss_val=2.880053, ne_loss=0.109008, total=2.989061
Sim min: 0.0015693972818553448 Sim max: 1.0
ClusterLoss debug: loss_val=2.864085, ne_loss=0.117381, total=2.981465
Sim min: 0.0023710676468908787 Sim max: 1.0
ClusterLoss debug: loss_val=2.945722, ne_loss=0.160246, total=3.105968
Sim min: 0.0027945477049797773 Sim max: 1.0
ClusterLoss debug: loss_val=2.874416, ne_loss=0.117751, total=2.992167
Sim min: 0.001108829048462212 Sim max: 1.0
ClusterLoss debug: loss_val=2.871364, ne_loss=0.101214, total=2.972577
Train Epoch: 5 [10368/18637 (28%)]	Instance Loss: 2.000098	Cluster Loss: 2.972577 	sec/iter: 0.8278
Sim min: 0.0020982783753424883 Sim max: 1.0
ClusterLoss debug: loss_val=2.863649, ne_loss=0.140078, total=3.003727
Sim min: 0.0011849557049572468 Sim max: 1.0
ClusterLoss debug: loss_val=2.869609, ne_loss=0.106823, total=2.976432
Sim min: 0.002226585056632757 Sim max: 1.0
ClusterLoss debug: loss_val=2.888618, ne_loss=0.072123, total=2.960741
Sim min: 0.00215996359474957 Sim max: 1.0
ClusterLoss debug: loss_val=2.868039, ne_loss=0.062870, total=2.930909
Sim min: 0.0016013289568945765 Sim max: 1.0
ClusterLoss debug: loss_val=2.884157, ne_loss=0.125367, total=3.009525
Sim min: 0.002676574978977442 Sim max: 1.0
ClusterLoss debug: loss_val=2.878302, ne_loss=0.100915, total=2.979217
Sim min: 0.003276409348472953 Sim max: 1.0
ClusterLoss debug: loss_val=2.902194, ne_loss=0.059855, total=2.962049
Sim min: 0.0020293002016842365 Sim max: 1.0
ClusterLoss debug: loss_val=2.891057, ne_loss=0.151593, total=3.042650
Sim min: 0.0026957958471029997 Sim max: 1.0
ClusterLoss debug: loss_val=2.883183, ne_loss=0.103619, total=2.986801
Sim min: 0.0019710715860128403 Sim max: 1.0
ClusterLoss debug: loss_val=2.871015, ne_loss=0.111589, total=2.982604
Train Epoch: 5 [11648/18637 (31%)]	Instance Loss: 2.372498	Cluster Loss: 2.982604 	sec/iter: 0.8506
Sim min: 0.00193418946582824 Sim max: 1.0
ClusterLoss debug: loss_val=2.869511, ne_loss=0.130152, total=2.999663
Sim min: 0.0034732671920210123 Sim max: 1.0
ClusterLoss debug: loss_val=2.908139, ne_loss=0.046361, total=2.954500
Sim min: 0.002589509356766939 Sim max: 1.0
ClusterLoss debug: loss_val=2.900875, ne_loss=0.128352, total=3.029227
Sim min: 0.0009459496941417456 Sim max: 1.0
ClusterLoss debug: loss_val=2.853882, ne_loss=0.073961, total=2.927843
Sim min: 0.0019929087720811367 Sim max: 1.0
ClusterLoss debug: loss_val=2.892255, ne_loss=0.051632, total=2.943888
Sim min: 0.001947936718352139 Sim max: 1.0
ClusterLoss debug: loss_val=2.877420, ne_loss=0.083142, total=2.960562
Sim min: 0.0021293838508427143 Sim max: 1.0
ClusterLoss debug: loss_val=2.866556, ne_loss=0.074830, total=2.941386
Sim min: 0.0014294690918177366 Sim max: 1.0
ClusterLoss debug: loss_val=2.879043, ne_loss=0.066666, total=2.945709
Sim min: 0.0019358386052772403 Sim max: 1.0
ClusterLoss debug: loss_val=2.887089, ne_loss=0.120533, total=3.007622
Sim min: 0.0026051735039800406 Sim max: 1.0
ClusterLoss debug: loss_val=2.894182, ne_loss=0.143192, total=3.037375
Train Epoch: 5 [12928/18637 (35%)]	Instance Loss: 2.252103	Cluster Loss: 3.037375 	sec/iter: 0.8607
Sim min: 0.001959671266376972 Sim max: 1.0
ClusterLoss debug: loss_val=2.864584, ne_loss=0.075198, total=2.939782
Sim min: 0.000620063510723412 Sim max: 1.0
ClusterLoss debug: loss_val=2.871470, ne_loss=0.095614, total=2.967084
Sim min: 0.0023202167358249426 Sim max: 1.0
ClusterLoss debug: loss_val=2.915872, ne_loss=0.158790, total=3.074662
Sim min: 0.0011601964943110943 Sim max: 1.0
ClusterLoss debug: loss_val=2.864022, ne_loss=0.108239, total=2.972261
Sim min: 0.0008713059360161424 Sim max: 1.0
ClusterLoss debug: loss_val=2.852067, ne_loss=0.103963, total=2.956031
Sim min: 0.002855741186067462 Sim max: 1.0
ClusterLoss debug: loss_val=2.877336, ne_loss=0.102725, total=2.980061
Sim min: 0.00265514780767262 Sim max: 1.0
ClusterLoss debug: loss_val=2.885104, ne_loss=0.100293, total=2.985397
Sim min: 0.004592637997120619 Sim max: 1.0
ClusterLoss debug: loss_val=2.874300, ne_loss=0.120273, total=2.994573
Sim min: 0.0005881218821741641 Sim max: 1.0
ClusterLoss debug: loss_val=2.861936, ne_loss=0.128992, total=2.990928
Sim min: 0.003584998892620206 Sim max: 1.0
ClusterLoss debug: loss_val=2.888940, ne_loss=0.066814, total=2.955753
Train Epoch: 5 [14208/18637 (38%)]	Instance Loss: 2.184718	Cluster Loss: 2.955753 	sec/iter: 0.8571
Sim min: 0.0023920959793031216 Sim max: 1.0
ClusterLoss debug: loss_val=2.871110, ne_loss=0.142895, total=3.014005
Sim min: 0.0025553700979799032 Sim max: 1.0
ClusterLoss debug: loss_val=2.867560, ne_loss=0.066713, total=2.934273
Sim min: 0.002484692493453622 Sim max: 1.0
ClusterLoss debug: loss_val=2.870025, ne_loss=0.055264, total=2.925290
Sim min: 0.0016213970957323909 Sim max: 1.0
ClusterLoss debug: loss_val=2.870882, ne_loss=0.073336, total=2.944218
Sim min: 0.0022127917036414146 Sim max: 1.0
ClusterLoss debug: loss_val=2.867606, ne_loss=0.071512, total=2.939118
Sim min: 0.002097727730870247 Sim max: 1.0
ClusterLoss debug: loss_val=2.870600, ne_loss=0.199089, total=3.069689
Sim min: 0.0041796136647462845 Sim max: 1.0
ClusterLoss debug: loss_val=2.886212, ne_loss=0.128855, total=3.015067
Sim min: 0.0014868245925754309 Sim max: 1.0
ClusterLoss debug: loss_val=2.877075, ne_loss=0.078851, total=2.955926
Sim min: 0.002835031831637025 Sim max: 1.0
ClusterLoss debug: loss_val=2.864225, ne_loss=0.083648, total=2.947873
Sim min: 0.001864883815869689 Sim max: 1.0
ClusterLoss debug: loss_val=2.862705, ne_loss=0.098589, total=2.961294
Train Epoch: 5 [15488/18637 (41%)]	Instance Loss: 2.336353	Cluster Loss: 2.961294 	sec/iter: 0.8529
Sim min: 0.0018304683035239577 Sim max: 1.0
ClusterLoss debug: loss_val=2.855704, ne_loss=0.084674, total=2.940379
Sim min: 0.0037703942507505417 Sim max: 1.0
ClusterLoss debug: loss_val=2.878380, ne_loss=0.133166, total=3.011546
Sim min: 0.0021836115047335625 Sim max: 1.0
ClusterLoss debug: loss_val=2.877189, ne_loss=0.138115, total=3.015304
Sim min: 0.000995048088952899 Sim max: 1.0
ClusterLoss debug: loss_val=2.886659, ne_loss=0.099009, total=2.985668
Sim min: 0.0016573971370235085 Sim max: 1.0
ClusterLoss debug: loss_val=2.857712, ne_loss=0.189631, total=3.047343
Sim min: 0.001524828840047121 Sim max: 1.0
ClusterLoss debug: loss_val=2.866767, ne_loss=0.138976, total=3.005743
Sim min: 0.0012275876943022013 Sim max: 1.0
ClusterLoss debug: loss_val=2.892553, ne_loss=0.134987, total=3.027539
Sim min: 0.001334171975031495 Sim max: 1.0
ClusterLoss debug: loss_val=2.857375, ne_loss=0.052300, total=2.909675
Sim min: 0.0016695467056706548 Sim max: 1.0
ClusterLoss debug: loss_val=2.873048, ne_loss=0.080932, total=2.953980
Sim min: 0.0016200350364670157 Sim max: 1.0
ClusterLoss debug: loss_val=2.874165, ne_loss=0.074485, total=2.948650
Train Epoch: 5 [16768/18637 (45%)]	Instance Loss: 2.070186	Cluster Loss: 2.948650 	sec/iter: 0.8443
Sim min: 0.0009352948982268572 Sim max: 1.0
ClusterLoss debug: loss_val=2.861166, ne_loss=0.170008, total=3.031173
Sim min: 0.0021945666521787643 Sim max: 1.0
ClusterLoss debug: loss_val=2.895514, ne_loss=0.129287, total=3.024801
Sim min: 0.00211775628849864 Sim max: 1.0
ClusterLoss debug: loss_val=2.869116, ne_loss=0.068848, total=2.937964
Sim min: 0.00057255505817011 Sim max: 1.0
ClusterLoss debug: loss_val=2.874059, ne_loss=0.127870, total=3.001929
Sim min: 0.0025895677972584963 Sim max: 1.0
ClusterLoss debug: loss_val=2.869422, ne_loss=0.099563, total=2.968985
Sim min: 0.0011190881486982107 Sim max: 1.0
ClusterLoss debug: loss_val=2.868251, ne_loss=0.149306, total=3.017556
Sim min: 0.0013193788472563028 Sim max: 1.0
ClusterLoss debug: loss_val=2.866626, ne_loss=0.100810, total=2.967437
Sim min: 0.002522452035918832 Sim max: 1.0
ClusterLoss debug: loss_val=2.895146, ne_loss=0.035849, total=2.930995
Sim min: 0.001649620826356113 Sim max: 1.0
ClusterLoss debug: loss_val=2.869466, ne_loss=0.180000, total=3.049466
Sim min: 0.001634284621104598 Sim max: 1.0
ClusterLoss debug: loss_val=2.869601, ne_loss=0.071152, total=2.940753
Train Epoch: 5 [18048/18637 (48%)]	Instance Loss: 2.337742	Cluster Loss: 2.940753 	sec/iter: 0.8392
Sim min: 0.001200434286147356 Sim max: 1.0
ClusterLoss debug: loss_val=2.870950, ne_loss=0.087623, total=2.958572
Sim min: 0.001081488560885191 Sim max: 1.0
ClusterLoss debug: loss_val=2.863150, ne_loss=0.122153, total=2.985303
Sim min: 0.0018313557375222445 Sim max: 1.0
ClusterLoss debug: loss_val=2.881128, ne_loss=0.245052, total=3.126180
Sim min: 0.0018124087946489453 Sim max: 1.0
ClusterLoss debug: loss_val=2.863219, ne_loss=0.113841, total=2.977060
Sim min: 0.0008648820803500712 Sim max: 1.0
ClusterLoss debug: loss_val=2.857152, ne_loss=0.064101, total=2.921252
Sim min: 0.0016867544036358595 Sim max: 1.0
ClusterLoss debug: loss_val=2.865912, ne_loss=0.113479, total=2.979391
Sim min: 0.001534786424599588 Sim max: 1.0
ClusterLoss debug: loss_val=2.905966, ne_loss=0.111618, total=3.017584
Sim min: 0.0016046782257035375 Sim max: 1.0
ClusterLoss debug: loss_val=2.858564, ne_loss=0.113022, total=2.971586
Sim min: 0.0021342658437788486 Sim max: 1.0
ClusterLoss debug: loss_val=2.884413, ne_loss=0.122132, total=3.006546
Sim min: 0.0024084600154310465 Sim max: 1.0
ClusterLoss debug: loss_val=2.898879, ne_loss=0.108877, total=3.007756
Train Epoch: 5 [19328/18637 (52%)]	Instance Loss: 2.116148	Cluster Loss: 3.007756 	sec/iter: 0.8356
Sim min: 0.0012932362733408809 Sim max: 1.0
ClusterLoss debug: loss_val=2.877745, ne_loss=0.072916, total=2.950661
Sim min: 0.0014932304620742798 Sim max: 1.0
ClusterLoss debug: loss_val=2.866645, ne_loss=0.032515, total=2.899160
Sim min: 0.002718924544751644 Sim max: 1.0
ClusterLoss debug: loss_val=2.900543, ne_loss=0.130722, total=3.031265
Sim min: 0.0014452328905463219 Sim max: 1.0
ClusterLoss debug: loss_val=2.862762, ne_loss=0.046211, total=2.908973
Sim min: 0.0006282102549448609 Sim max: 1.0
ClusterLoss debug: loss_val=2.863113, ne_loss=0.171065, total=3.034178
Sim min: 0.0018610021797940135 Sim max: 1.0
ClusterLoss debug: loss_val=2.884661, ne_loss=0.054039, total=2.938701
Sim min: 0.0025133967865258455 Sim max: 1.0
ClusterLoss debug: loss_val=2.877372, ne_loss=0.067228, total=2.944600
Sim min: 0.0025094456505030394 Sim max: 1.0
ClusterLoss debug: loss_val=2.868757, ne_loss=0.085315, total=2.954072
Sim min: 0.0017419764772057533 Sim max: 1.0
ClusterLoss debug: loss_val=2.876222, ne_loss=0.088088, total=2.964310
Sim min: 0.002272862009704113 Sim max: 1.0
ClusterLoss debug: loss_val=2.877721, ne_loss=0.158181, total=3.035902
Train Epoch: 5 [20608/18637 (55%)]	Instance Loss: 2.296187	Cluster Loss: 3.035902 	sec/iter: 0.8356
Sim min: 0.00285361148416996 Sim max: 1.0
ClusterLoss debug: loss_val=2.877949, ne_loss=0.102434, total=2.980383
Sim min: 0.001417216146364808 Sim max: 1.0
ClusterLoss debug: loss_val=2.859735, ne_loss=0.180785, total=3.040520
Sim min: 0.003168438095599413 Sim max: 1.0
ClusterLoss debug: loss_val=2.880947, ne_loss=0.148394, total=3.029341
Sim min: 0.000910723814740777 Sim max: 1.0
ClusterLoss debug: loss_val=2.860559, ne_loss=0.184736, total=3.045296
Sim min: 0.0022286351304501295 Sim max: 1.0
ClusterLoss debug: loss_val=2.890303, ne_loss=0.143887, total=3.034189
Sim min: 0.0023281613830477 Sim max: 1.0
ClusterLoss debug: loss_val=2.879663, ne_loss=0.108790, total=2.988452
Sim min: 0.004395183641463518 Sim max: 1.0
ClusterLoss debug: loss_val=2.895263, ne_loss=0.090256, total=2.985519
Sim min: 0.002140324329957366 Sim max: 1.0
ClusterLoss debug: loss_val=2.862735, ne_loss=0.111075, total=2.973809
Sim min: 0.002108209300786257 Sim max: 1.0
ClusterLoss debug: loss_val=2.871477, ne_loss=0.122550, total=2.994027
Sim min: 0.0010417664889246225 Sim max: 1.0
ClusterLoss debug: loss_val=2.869553, ne_loss=0.123261, total=2.992813
Train Epoch: 5 [21888/18637 (59%)]	Instance Loss: 2.320668	Cluster Loss: 2.992813 	sec/iter: 0.8345
Sim min: 0.0018454864621162415 Sim max: 1.0
ClusterLoss debug: loss_val=2.852174, ne_loss=0.101891, total=2.954065
Sim min: 0.0012307569850236177 Sim max: 1.0
ClusterLoss debug: loss_val=2.866214, ne_loss=0.081062, total=2.947276
Sim min: 0.001158199505880475 Sim max: 1.0
ClusterLoss debug: loss_val=2.866301, ne_loss=0.067791, total=2.934091
Sim min: 0.004131626337766647 Sim max: 1.0
ClusterLoss debug: loss_val=2.905717, ne_loss=0.120558, total=3.026274
Sim min: 0.0017680908786132932 Sim max: 1.0
ClusterLoss debug: loss_val=2.872750, ne_loss=0.108361, total=2.981111
Sim min: 0.0012975056888535619 Sim max: 1.0
ClusterLoss debug: loss_val=2.856034, ne_loss=0.116380, total=2.972414
Sim min: 0.002661211183294654 Sim max: 1.0
ClusterLoss debug: loss_val=2.865446, ne_loss=0.097793, total=2.963239
Sim min: 0.0017209964571520686 Sim max: 1.0
ClusterLoss debug: loss_val=2.884183, ne_loss=0.067065, total=2.951248
Sim min: 0.0017792748985812068 Sim max: 1.0
ClusterLoss debug: loss_val=2.880901, ne_loss=0.123683, total=3.004584
Sim min: 0.0018351386534050107 Sim max: 1.0
ClusterLoss debug: loss_val=2.877516, ne_loss=0.098346, total=2.975863
Train Epoch: 5 [23168/18637 (62%)]	Instance Loss: 2.119629	Cluster Loss: 2.975863 	sec/iter: 0.8310
Sim min: 0.0019014040008187294 Sim max: 1.0
ClusterLoss debug: loss_val=2.872033, ne_loss=0.177501, total=3.049535
Sim min: 0.0036467730533331633 Sim max: 1.0
ClusterLoss debug: loss_val=2.884012, ne_loss=0.186696, total=3.070708
Sim min: 0.002555808750912547 Sim max: 1.0
ClusterLoss debug: loss_val=2.872549, ne_loss=0.134835, total=3.007384
Sim min: 0.0014358463231474161 Sim max: 1.0
ClusterLoss debug: loss_val=2.869725, ne_loss=0.069830, total=2.939555
Sim min: 0.0018738718936219811 Sim max: 1.0
ClusterLoss debug: loss_val=2.871985, ne_loss=0.107666, total=2.979651
Sim min: 0.0042723254300653934 Sim max: 1.0
ClusterLoss debug: loss_val=2.876537, ne_loss=0.069787, total=2.946324
Sim min: 0.002363490406423807 Sim max: 1.0
ClusterLoss debug: loss_val=2.875635, ne_loss=0.041818, total=2.917454
Sim min: 0.001825100858695805 Sim max: 1.0
ClusterLoss debug: loss_val=2.879621, ne_loss=0.097470, total=2.977090
Sim min: 0.002046638634055853 Sim max: 1.0
ClusterLoss debug: loss_val=2.881779, ne_loss=0.239174, total=3.120953
Sim min: 0.0016556339105591178 Sim max: 1.0
ClusterLoss debug: loss_val=2.870203, ne_loss=0.108022, total=2.978225
Train Epoch: 5 [24448/18637 (65%)]	Instance Loss: 2.121215	Cluster Loss: 2.978225 	sec/iter: 0.8303
Sim min: 0.001267144689336419 Sim max: 1.0
ClusterLoss debug: loss_val=2.865942, ne_loss=0.098739, total=2.964680
Sim min: 0.0018740815576165915 Sim max: 1.0
ClusterLoss debug: loss_val=2.892903, ne_loss=0.168244, total=3.061147
Sim min: 0.0015350450994446874 Sim max: 1.0
ClusterLoss debug: loss_val=2.867300, ne_loss=0.143898, total=3.011198
Sim min: 0.0019956601317971945 Sim max: 1.0
ClusterLoss debug: loss_val=2.863219, ne_loss=0.145654, total=3.008873
Sim min: 0.0018909627106040716 Sim max: 1.0
ClusterLoss debug: loss_val=2.879419, ne_loss=0.098712, total=2.978132
Sim min: 0.0006377070676535368 Sim max: 1.0
ClusterLoss debug: loss_val=2.871167, ne_loss=0.071211, total=2.942378
Sim min: 0.0016882496420294046 Sim max: 1.0
ClusterLoss debug: loss_val=2.865153, ne_loss=0.100800, total=2.965953
Sim min: 0.0017394660972058773 Sim max: 1.0
ClusterLoss debug: loss_val=2.858501, ne_loss=0.155432, total=3.013933
Sim min: 0.0013871333794668317 Sim max: 1.0
ClusterLoss debug: loss_val=2.902325, ne_loss=0.147392, total=3.049716
Sim min: 0.0027534975670278072 Sim max: 1.0
ClusterLoss debug: loss_val=2.891095, ne_loss=0.062247, total=2.953342
Train Epoch: 5 [25728/18637 (69%)]	Instance Loss: 2.130046	Cluster Loss: 2.953342 	sec/iter: 0.8308
Sim min: 0.0010458908509463072 Sim max: 1.0
ClusterLoss debug: loss_val=2.903869, ne_loss=0.251509, total=3.155378
Sim min: 0.0012113332049921155 Sim max: 1.0
ClusterLoss debug: loss_val=2.875582, ne_loss=0.091974, total=2.967555
Sim min: 0.002215482760220766 Sim max: 1.0
ClusterLoss debug: loss_val=2.895328, ne_loss=0.125031, total=3.020359
Sim min: 0.00124035042244941 Sim max: 1.0
ClusterLoss debug: loss_val=2.878479, ne_loss=0.112637, total=2.991116
Sim min: 0.002421722048893571 Sim max: 1.0
ClusterLoss debug: loss_val=2.860101, ne_loss=0.142356, total=3.002457
Sim min: 0.0027964350301772356 Sim max: 1.0
ClusterLoss debug: loss_val=2.864345, ne_loss=0.206466, total=3.070811
Sim min: 0.0017339336918666959 Sim max: 1.0
ClusterLoss debug: loss_val=2.866523, ne_loss=0.076382, total=2.942905
Sim min: 0.0022150923032313585 Sim max: 1.0
ClusterLoss debug: loss_val=2.866337, ne_loss=0.134448, total=3.000785
Sim min: 0.0020651842933148146 Sim max: 1.0
ClusterLoss debug: loss_val=2.879507, ne_loss=0.150678, total=3.030185
Sim min: 0.0021428694017231464 Sim max: 1.0
ClusterLoss debug: loss_val=2.867454, ne_loss=0.071761, total=2.939214
Train Epoch: 5 [27008/18637 (72%)]	Instance Loss: 2.329631	Cluster Loss: 2.939214 	sec/iter: 0.8312
Sim min: 0.0013698060065507889 Sim max: 1.0
ClusterLoss debug: loss_val=2.905205, ne_loss=0.170817, total=3.076022
Sim min: 0.002901898231357336 Sim max: 1.0
ClusterLoss debug: loss_val=2.876828, ne_loss=0.062055, total=2.938883
Sim min: 0.0012514835689216852 Sim max: 1.0
ClusterLoss debug: loss_val=2.877137, ne_loss=0.100981, total=2.978118
Sim min: 0.0025145630352199078 Sim max: 1.0
ClusterLoss debug: loss_val=2.870427, ne_loss=0.218042, total=3.088469
Sim min: 0.0016956392209976912 Sim max: 1.0
ClusterLoss debug: loss_val=2.864885, ne_loss=0.035638, total=2.900523
Sim min: 0.0016049487749114633 Sim max: 1.0
ClusterLoss debug: loss_val=2.872723, ne_loss=0.105699, total=2.978421
Sim min: 0.0027617437299340963 Sim max: 1.0
ClusterLoss debug: loss_val=2.879617, ne_loss=0.067823, total=2.947440
Sim min: 0.0014211263041943312 Sim max: 1.0
ClusterLoss debug: loss_val=2.881838, ne_loss=0.140496, total=3.022334
Sim min: 0.0013410017127171159 Sim max: 1.0
ClusterLoss debug: loss_val=2.882407, ne_loss=0.143444, total=3.025850
Sim min: 0.0028130183927714825 Sim max: 1.0
ClusterLoss debug: loss_val=2.880716, ne_loss=0.120686, total=3.001402
Train Epoch: 5 [28288/18637 (76%)]	Instance Loss: 2.230420	Cluster Loss: 3.001402 	sec/iter: 0.8310
Sim min: 0.002645366359502077 Sim max: 1.0
ClusterLoss debug: loss_val=2.875314, ne_loss=0.069488, total=2.944802
Sim min: 0.0033359061926603317 Sim max: 1.0
ClusterLoss debug: loss_val=2.911700, ne_loss=0.049715, total=2.961415
Sim min: 0.0018352310871705413 Sim max: 1.0
ClusterLoss debug: loss_val=2.878546, ne_loss=0.127408, total=3.005955
Sim min: 0.0006635984173044562 Sim max: 1.0
ClusterLoss debug: loss_val=2.886292, ne_loss=0.216562, total=3.102854
Sim min: 0.0026550577022135258 Sim max: 1.0
ClusterLoss debug: loss_val=2.884813, ne_loss=0.192241, total=3.077054
Sim min: 0.0017791844438761473 Sim max: 1.0
ClusterLoss debug: loss_val=2.858770, ne_loss=0.083807, total=2.942577
Sim min: 0.0012610411504283547 Sim max: 1.0
ClusterLoss debug: loss_val=2.889111, ne_loss=0.126925, total=3.016036
Sim min: 0.0033864995930343866 Sim max: 1.0
ClusterLoss debug: loss_val=2.899746, ne_loss=0.096343, total=2.996089
Sim min: 0.004731332417577505 Sim max: 1.0
ClusterLoss debug: loss_val=2.912216, ne_loss=0.141502, total=3.053718
Sim min: 0.001933622406795621 Sim max: 1.0
ClusterLoss debug: loss_val=2.882578, ne_loss=0.112188, total=2.994766
Train Epoch: 5 [29568/18637 (79%)]	Instance Loss: 2.125301	Cluster Loss: 2.994766 	sec/iter: 0.8285
Sim min: 0.0006607143441215158 Sim max: 1.0
ClusterLoss debug: loss_val=2.857625, ne_loss=0.114847, total=2.972472
Sim min: 0.0007432873244397342 Sim max: 1.0
ClusterLoss debug: loss_val=2.870675, ne_loss=0.116056, total=2.986730
Sim min: 0.001263457932509482 Sim max: 1.0
ClusterLoss debug: loss_val=2.870337, ne_loss=0.089146, total=2.959483
Sim min: 0.0022414731793105602 Sim max: 1.0
ClusterLoss debug: loss_val=2.864025, ne_loss=0.167155, total=3.031180
Sim min: 0.001752321608364582 Sim max: 1.0
ClusterLoss debug: loss_val=2.882733, ne_loss=0.088141, total=2.970874
Sim min: 0.002056910190731287 Sim max: 1.0
ClusterLoss debug: loss_val=2.889442, ne_loss=0.104058, total=2.993501
Sim min: 0.0014639627188444138 Sim max: 1.0
ClusterLoss debug: loss_val=2.893524, ne_loss=0.088428, total=2.981952
Sim min: 0.0016320875147357583 Sim max: 1.0
ClusterLoss debug: loss_val=2.867675, ne_loss=0.159796, total=3.027471
Sim min: 0.001172359799966216 Sim max: 1.0
ClusterLoss debug: loss_val=2.874487, ne_loss=0.049187, total=2.923674
Sim min: 0.002644752385094762 Sim max: 1.0
ClusterLoss debug: loss_val=2.878360, ne_loss=0.078517, total=2.956877
Train Epoch: 5 [30848/18637 (83%)]	Instance Loss: 2.193203	Cluster Loss: 2.956877 	sec/iter: 0.8273
Sim min: 0.0012848180485889316 Sim max: 1.0
ClusterLoss debug: loss_val=2.866248, ne_loss=0.118833, total=2.985081
Sim min: 0.0015734918415546417 Sim max: 1.0
ClusterLoss debug: loss_val=2.871409, ne_loss=0.084435, total=2.955844
Sim min: 0.002590286312624812 Sim max: 1.0
ClusterLoss debug: loss_val=2.893818, ne_loss=0.042544, total=2.936362
Sim min: 0.0012317881919443607 Sim max: 1.0
ClusterLoss debug: loss_val=2.864104, ne_loss=0.076650, total=2.940753
Sim min: 0.0016857648734003305 Sim max: 1.0
ClusterLoss debug: loss_val=2.862721, ne_loss=0.125628, total=2.988350
Sim min: 0.0018549367086961865 Sim max: 1.0
ClusterLoss debug: loss_val=2.872871, ne_loss=0.088943, total=2.961815
Sim min: 0.0007556029595434666 Sim max: 1.0
ClusterLoss debug: loss_val=2.855705, ne_loss=0.093116, total=2.948821
Sim min: 0.00097769929561764 Sim max: 1.0
ClusterLoss debug: loss_val=2.873104, ne_loss=0.129404, total=3.002509
Sim min: 0.0030145221389830112 Sim max: 1.0
ClusterLoss debug: loss_val=2.885533, ne_loss=0.044927, total=2.930460
Sim min: 0.0018493442330509424 Sim max: 1.0
ClusterLoss debug: loss_val=2.877214, ne_loss=0.095791, total=2.973005
Train Epoch: 5 [32128/18637 (86%)]	Instance Loss: 2.341455	Cluster Loss: 2.973005 	sec/iter: 0.8332
Sim min: 0.0010154051706194878 Sim max: 1.0
ClusterLoss debug: loss_val=2.861929, ne_loss=0.083619, total=2.945548
Sim min: 0.002183047356083989 Sim max: 1.0
ClusterLoss debug: loss_val=2.873046, ne_loss=0.071115, total=2.944161
Sim min: 0.003926333971321583 Sim max: 1.0
ClusterLoss debug: loss_val=2.875197, ne_loss=0.165974, total=3.041171
Sim min: 0.003468543291091919 Sim max: 1.0
ClusterLoss debug: loss_val=2.872005, ne_loss=0.043559, total=2.915565
Sim min: 0.0023502432741224766 Sim max: 1.0
ClusterLoss debug: loss_val=2.881444, ne_loss=0.118658, total=3.000102
Sim min: 0.0021480508148670197 Sim max: 1.0
ClusterLoss debug: loss_val=2.867166, ne_loss=0.037245, total=2.904411
Sim min: 0.0021337284706532955 Sim max: 1.0
ClusterLoss debug: loss_val=2.886765, ne_loss=0.083190, total=2.969956
Sim min: 0.0030583911575376987 Sim max: 1.0
ClusterLoss debug: loss_val=2.877474, ne_loss=0.090248, total=2.967722
Sim min: 0.002325606532394886 Sim max: 1.0
ClusterLoss debug: loss_val=2.892749, ne_loss=0.091344, total=2.984094
Sim min: 0.0012659080093726516 Sim max: 1.0
ClusterLoss debug: loss_val=2.853740, ne_loss=0.089035, total=2.942775
Train Epoch: 5 [33408/18637 (89%)]	Instance Loss: 2.255294	Cluster Loss: 2.942775 	sec/iter: 0.8336
Sim min: 0.0012376791564747691 Sim max: 1.0
ClusterLoss debug: loss_val=2.875559, ne_loss=0.191018, total=3.066576
Sim min: 0.0013609350426122546 Sim max: 1.0
ClusterLoss debug: loss_val=2.868397, ne_loss=0.111856, total=2.980254
Sim min: 0.0020310203544795513 Sim max: 1.0
ClusterLoss debug: loss_val=2.897653, ne_loss=0.106508, total=3.004161
Sim min: 0.0008371009607799351 Sim max: 1.0
ClusterLoss debug: loss_val=2.868841, ne_loss=0.110767, total=2.979609
Sim min: 0.003189867129549384 Sim max: 1.0
ClusterLoss debug: loss_val=2.868436, ne_loss=0.087158, total=2.955594
Sim min: 0.0029153102077543736 Sim max: 1.0
ClusterLoss debug: loss_val=2.874289, ne_loss=0.049672, total=2.923961
Sim min: 0.0012700114166364074 Sim max: 1.0
ClusterLoss debug: loss_val=2.867869, ne_loss=0.074358, total=2.942226
Sim min: 0.0009646197431720793 Sim max: 1.0
ClusterLoss debug: loss_val=2.849359, ne_loss=0.113163, total=2.962522
Sim min: 0.0013532163575291634 Sim max: 1.0
ClusterLoss debug: loss_val=2.869977, ne_loss=0.109689, total=2.979666
Sim min: 0.0008046121220104396 Sim max: 1.0
ClusterLoss debug: loss_val=2.846452, ne_loss=0.140388, total=2.986840
Train Epoch: 5 [34688/18637 (93%)]	Instance Loss: 2.070415	Cluster Loss: 2.986840 	sec/iter: 0.8323
Sim min: 0.0009816677775233984 Sim max: 1.0
ClusterLoss debug: loss_val=2.867277, ne_loss=0.115436, total=2.982713
Sim min: 0.001376744476146996 Sim max: 1.0
ClusterLoss debug: loss_val=2.856115, ne_loss=0.066436, total=2.922552
Sim min: 0.0016892392886802554 Sim max: 1.0
ClusterLoss debug: loss_val=2.889230, ne_loss=0.171014, total=3.060244
Sim min: 0.0015790717443451285 Sim max: 1.0
ClusterLoss debug: loss_val=2.850455, ne_loss=0.036383, total=2.886838
Sim min: 0.0031314720399677753 Sim max: 1.0
ClusterLoss debug: loss_val=2.892614, ne_loss=0.087061, total=2.979675
Sim min: 0.0016962493536993861 Sim max: 1.0
ClusterLoss debug: loss_val=2.868202, ne_loss=0.081695, total=2.949897
Sim min: 0.0007984495023265481 Sim max: 1.0
ClusterLoss debug: loss_val=2.863514, ne_loss=0.155154, total=3.018668
Sim min: 0.006078310776501894 Sim max: 1.0
ClusterLoss debug: loss_val=2.888361, ne_loss=0.223574, total=3.111935
Sim min: 0.003440732602030039 Sim max: 1.0
ClusterLoss debug: loss_val=2.880489, ne_loss=0.070303, total=2.950793
Sim min: 0.0011263072956353426 Sim max: 1.0
ClusterLoss debug: loss_val=2.857365, ne_loss=0.103978, total=2.961343
Train Epoch: 5 [35968/18637 (96%)]	Instance Loss: 2.203765	Cluster Loss: 2.961343 	sec/iter: 0.8302
Sim min: 0.002909216796979308 Sim max: 1.0
ClusterLoss debug: loss_val=2.868675, ne_loss=0.156684, total=3.025359
Sim min: 0.0035251080989837646 Sim max: 1.0
ClusterLoss debug: loss_val=2.887516, ne_loss=0.099935, total=2.987451
Sim min: 0.0018538401927798986 Sim max: 1.0
ClusterLoss debug: loss_val=2.881990, ne_loss=0.106856, total=2.988847
Sim min: 0.001710390322841704 Sim max: 1.0
ClusterLoss debug: loss_val=2.865405, ne_loss=0.127012, total=2.992417
Sim min: 0.0029836827889084816 Sim max: 1.0
ClusterLoss debug: loss_val=2.871354, ne_loss=0.131339, total=3.002693
Sim min: 0.002980039222165942 Sim max: 1.0
ClusterLoss debug: loss_val=2.863297, ne_loss=0.151360, total=3.014657
Sim min: 0.0023840623907744884 Sim max: 1.0
ClusterLoss debug: loss_val=2.883342, ne_loss=0.069002, total=2.952344
Sim min: 0.002393093192949891 Sim max: 1.0
ClusterLoss debug: loss_val=2.888853, ne_loss=0.175363, total=3.064216
Sim min: 0.002208082703873515 Sim max: 1.0
ClusterLoss debug: loss_val=2.900995, ne_loss=0.300411, total=3.201406
Sim min: 0.0016001688782125711 Sim max: 1.0
ClusterLoss debug: loss_val=2.863319, ne_loss=0.099185, total=2.962504
Train Epoch: 5 [37248/18637 (100%)]	Instance Loss: 2.100862	Cluster Loss: 2.962504 	sec/iter: 0.8248
Sim min: 0.0005494988872669637 Sim max: 1.0
ClusterLoss debug: loss_val=3.010545, ne_loss=0.479548, total=3.490093
Train Epoch: 5 [37274/18637 (100%)]	Instance Loss: 0.936452	Cluster Loss: 3.490093 	sec/iter: 0.8226
Model saved to model_epoch_5.pth
请输入数字1以继续运行: 
===== 分类评估 =====
Epoch 5 - Loss: 3.0931, Acc: 4.54%
Recall:     0.0800 0.0000 0.0000 0.0500 0.0000 0.0100 0.0000 0.0000 0.1400 0.2400 0.0000 0.0000 0.0000 0.0000 0.0100 0.0141 0.3900 0.0100 0.0000 0.0000 0.0000
Precision:  0.0367 0.0000 0.0000 0.0153 0.0000 0.0526 0.0000 0.0000 0.0588 0.2124 0.0000 0.0000 0.0000 0.0000 0.0227 0.0069 0.1105 0.0072 0.0000 0.0000 0.0000
F1-score:   0.0503 0.0000 0.0000 0.0235 0.0000 0.0168 0.0000 0.0000 0.0828 0.2254 0.0000 0.0000 0.0000 0.0000 0.0139 0.0093 0.1722 0.0084 0.0000 0.0000 0.0000

===== 聚类评估 =====
混淆矩阵:
 [[92  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  7]
 [ 0  0  0  0  1 73  0  0  0  7  0  0 18  0  0  0  0  0  0  1  0]
 [66  0  0  0  0  0  0  0  0  0  0  0  5  0  0  1  1  0  1  2 24]
 [ 0  0  0  0  1  0  0  0  0  0  0  0 16  4  0  0 70  0  0  1  8]
 [13  0  0  0 50  0  0  0  0  0  0  0  1  0  0  0  0 30  0  0  6]
 [ 4  0  0  0  2  1  1  0  0  0  0  0 87  0  0  0  0  0  0  1  4]
 [ 0 20  0  0  1  0  0 10  0  0  8  0  1  0 34  0  1  0  0 25  0]
 [ 0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 89]
 [ 5  0  0  0 80  0  0  0  0  0  0  0  0  0  0  0  1  0  0  4 10]
 [ 8  0  0  0 43  0  0  0  0  0  0  0 12  0  0 33  0  1  0  1  2]
 [ 2  0  0  0  0  0  1  0  0  0  0  0 79  0  0  0  0  0  0  0 18]
 [ 0  0 25  0  2  0  0  0  0  0  0  2  1  0  0  0  0  0  0 67  3]
 [ 0  0  0  0  0  0  0  0 61  0  0  0  0  0  0  0  0  0  0  1 38]
 [40  0  0  0  2  0 36  0  0  0  0  0  0  0  0  0  0  0  0  0 22]
 [92  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  4]
 [17  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0 50  0]
 [ 1  0  0  0  0  0  0  0  0  0  0  0 97  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  2  0  0  0  0  2  0  0  2  0  0  0  0  0  0  0 94]
 [ 1  0  0  0 60  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2 37]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0 99]
 [ 0  0  0  0  1  0  0  0  0 92  0  0  3  0  0  0  0  0  0  1  3]]
代价矩阵:
 [[249. 341. 275. 341. 328. 337. 341. 341. 336. 333. 339. 341. 341. 301.
  249. 324. 340. 341. 340. 341. 341.]
 [ 20.  20.  20.  20.  20.  20.   0.  20.  20.  20.  20.  20.  20.  20.
   20.  20.  20.  20.  20.  20.  20.]
 [ 25.  25.  25.  25.  25.  25.  25.  25.  25.  25.  25.   0.  25.  25.
   25.  25.  25.  25.  25.  25.  25.]
 [ 11.  11.  11.  11.  11.  11.  11.   0.  11.  11.  11.  11.  11.  11.
   11.  11.  11.  11.  11.  11.  11.]
 [249. 248. 249. 248. 199. 247. 248. 249. 169. 206. 249. 247. 249. 247.
  249. 245. 249. 247. 189. 249. 248.]
 [ 74.   1.  74.  74.  74.  73.  74.  74.  74.  74.  74.  74.  74.  74.
   74.  74.  74.  74.  74.  74.  74.]
 [ 42.  42.  42.  42.  42.  41.  42.  42.  42.  42.  41.  42.  42.   6.
   38.  42.  42.  42.  42.  42.  42.]
 [ 10.  10.  10.  10.  10.  10.   0.  10.  10.  10.  10.  10.  10.  10.
   10.  10.  10.  10.  10.  10.  10.]
 [ 61.  61.  61.  61.  61.  61.  61.  61.  61.  61.  61.  61.   0.  61.
   61.  61.  61.  61.  61.  61.  61.]
 [101.  94. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.
  101. 101. 101.  99. 101. 101.   9.]
 [  8.   8.   8.   8.   8.   8.   0.   8.   8.   8.   8.   8.   8.   8.
    8.   8.   8.   8.   8.   8.   8.]
 [  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   0.   2.   2.
    2.   2.   2.   2.   2.   2.   2.]
 [323. 305. 318. 307. 322. 236. 322. 323. 323. 311. 244. 322. 323. 323.
  323. 323. 226. 321. 323. 322. 320.]
 [  4.   4.   4.   0.   4.   4.   4.   4.   4.   4.   4.   4.   4.   4.
    4.   4.   4.   4.   4.   4.   4.]
 [ 34.  34.  34.  34.  34.  34.   0.  34.  34.  34.  34.  34.  34.  34.
   34.  34.  34.  34.  34.  34.  34.]
 [ 34.  34.  33.  34.  34.  34.  34.  34.  34.   1.  34.  34.  34.  34.
   34.  34.  34.  34.  34.  34.  34.]
 [ 73.  73.  72.   3.  73.  73.  72.  73.  72.  73.  73.  73.  73.  73.
   73.  73.  73.  73.  73.  73.  73.]
 [ 31.  31.  31.  31.   1.  31.  31.  31.  31.  30.  31.  31.  31.  31.
   31.  31.  31.  31.  31.  31.  31.]
 [  1.   1.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.]
 [158. 158. 157. 158. 159. 158. 134. 159. 155. 158. 159.  92. 158. 159.
  159. 109. 157. 159. 157. 159. 158.]
 [461. 468. 444. 460. 462. 464. 468. 379. 458. 466. 450. 465. 430. 446.
  464. 468. 468. 374. 431. 369. 465.]]
最佳映射关系: [ 0. 18. 11.  7.  8.  1. 13.  5. 12. 20. 10. 17. 16. 14.  6.  9.  3.  4.
  2. 15. 19.]
聚类分配的最大索引: 20

=== 实例级嵌入 ===
NMI: 0.6019  ARI: 0.2843
F-score: 0.3772  Silhouette: 0.2718
Adjusted Acc: 0.4268
混淆矩阵:
 [[33  0  2  0  0 23  5  0  0 29  1  0  0  0  0  0  0  4  0  0  3]
 [ 8  0  0  0  0  0  0  0 38  1  0 38  1  4  0  0  1  9  0  0  0]
 [25  3  0 13  0 11  0  1  0 22  0  0  1  5 11  0  5  3  0  0  0]
 [ 3 22  1  0  0  0  1 19  1  2  0  0 23  0  0  0 25  3  0  0  0]
 [ 9  2  1 29  0 11 11  0  0 14  0  0  1  8  0  0  0 14  0  0  0]
 [ 9  2  2  0  3 19  3  0  0  7  4  7  2  2  8  0  2 21  0  0  9]
 [ 0 37  0  0  0  0  0 23  0  0  0  0  0  0  0  0  2  0 35  3  0]
 [ 0 34  0  0  0  0  0  0  0  0  0  0 49  0  0  0  6  0 11  0  0]
 [ 4  1  1  2 12  2  0 18  2 19  9  0  1  1  4  0  0 24  0  0  0]
 [ 1  9  0  0 31  0  1  0  0 22 35  0  0  0  0  0  1  0  0  0  0]
 [ 7  0  0  0  7  1  0  1  0 10 68  0  0  2  0  0  1  2  0  0  1]
 [ 1 14  0  0  0  0  0 26  0  0  0  0 24  0  0 27  7  1  0  0  0]
 [ 0  4  6  0  0  0  1  0  4  0  0 18  1  0  0  0  7 14  0 45  0]
 [ 5  1 22  0  0  3  4  3  0  3  0  0  3 13  2  0  0  7  0  0 34]
 [ 1  5 18  0  0  8  1  0  0  4  0  0  1  4  7  0  2  5  0  0 44]
 [ 0  3  1  0  0  0  0 12  0 18  0  0 20  0 12  0  1  4  0  0  0]
 [ 8  0  0  7  0  0  6  0  0 16  0  0  0  2  0  0  2 58  0  0  1]
 [ 7  1  4  2  1  2  6  0  0  9 28  0  0  4 21  0  0 15  0  0  0]
 [ 2  0  1 57  0  1  0  0  0  3  0  0  0  0 35  0  1  0  0  0  0]
 [ 5  0  1  0  0 15 31  3  0  3  1  0  3  0 19  0  0 19  0  0  0]
 [ 0  0 20  1  0  0  0  0 31  0  0  0  1 41  0  0  1  5  0  0  0]]
代价矩阵:
 [[ 95. 120. 103. 125. 119. 119. 128. 128. 124. 127. 121. 127. 128. 123.
  127. 128. 120. 121. 126. 123. 128.]
 [138. 138. 135. 116. 136. 136. 101. 104. 137. 129. 138. 124. 134. 137.
  133. 135. 138. 137. 138. 138. 138.]
 [ 78.  80.  80.  79.  79.  78.  80.  80.  79.  80.  80.  80.  74.  58.
   62.  79.  80.  76.  79.  79.  60.]
 [111. 111.  98. 111.  82. 111. 111. 111. 109. 111. 111. 111. 111. 111.
  111. 111. 104. 109.  54. 111. 110.]
 [ 54.  54.  54.  54.  54.  51.  54.  54.  42.  23.  47.  54.  54.  54.
   54.  54.  54.  53.  54.  54.  54.]
 [ 73.  96.  85.  96.  85.  77.  96.  96.  94.  96.  95.  96.  96.  93.
   88.  96.  96.  94.  95.  81.  96.]
 [ 65.  70.  70.  69.  59.  67.  70.  70.  70.  69.  70.  70.  69.  66.
   69.  70.  64.  64.  70.  39.  70.]
 [106. 106. 105.  87. 106. 106.  83. 106.  88. 106. 105.  80. 106. 103.
  106.  94. 106. 106. 106. 103. 106.]
 [ 76.  38.  76.  75.  76.  76.  76.  76.  74.  76.  76.  76.  72.  76.
   76.  76.  76.  76.  76.  76.  45.]
 [153. 181. 160. 180. 168. 175. 182. 182. 163. 160. 172. 182. 182. 179.
  178. 164. 166. 173. 179. 179. 182.]
 [145. 146. 146. 146. 146. 142. 146. 146. 137. 111.  78. 146. 146. 146.
  146. 146. 146. 118. 146. 145. 146.]
 [ 63.  25.  63.  63.  63.  56.  63.  63.  63.  63.  63.  63.  45.  63.
   63.  63.  63.  63.  63.  63.  63.]
 [131. 130. 130. 108. 130. 129. 131.  82. 130. 131. 131. 107. 130. 128.
  130. 111. 131. 131. 131. 128. 130.]
 [ 86.  82.  81.  86.  78.  84.  86.  86.  85.  86.  84.  86.  86.  73.
   82.  86.  84.  82.  86.  86.  45.]
 [119. 119. 108. 119. 119. 111. 119. 119. 115. 119. 119. 119. 119. 117.
  112. 107. 119.  98.  84. 100. 119.]
 [ 27.  27.  27.  27.  27.  27.  27.  27.  27.  27.  27.   0.  27.  27.
   27.  27.  27.  27.  27.  27.  27.]
 [ 64.  63.  59.  39.  64.  62.  62.  58.  64.  63.  63.  57.  57.  64.
   62.  63.  62.  64.  63.  64.  63.]
 [204. 199. 205. 205. 194. 187. 208. 208. 184. 208. 206. 207. 194. 201.
  203. 204. 150. 193. 208. 189. 203.]
 [ 46.  46.  46.  46.  46.  46.  11.  35.  46.  46.  46.  46.  46.  46.
   46.  46.  46.  46.  46.  46.  46.]
 [ 48.  48.  48.  48.  48.  48.  45.  48.  48.  48.  48.  48.   3.  48.
   48.  48.  48.  48.  48.  48.  48.]
 [ 89.  92.  92.  92.  92.  83.  92.  92.  92.  92.  91.  92.  92.  58.
   48.  92.  91.  92.  92.  92.  92.]]
最佳映射关系: [ 0.  7. 13. 18.  9.  5. 19.  8.  1.  2. 10.  4. 15. 20. 17. 11.  3. 16.
  6. 12. 14.]
聚类分配的最大索引: 20

=== 簇级嵌入 ===
NMI: 0.4325  ARI: 0.1868
F-score: 0.2299  Silhouette: 0.5554
Adjusted Acc: 0.3327

总耗时: 25.49s
分类准确率: 4.54%
簇级嵌入NMI: 0.4325
请输入数字2以继续运行: 