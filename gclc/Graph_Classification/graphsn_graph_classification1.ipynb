{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/.conda/envs/tgcc/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import argparse\n",
    "\n",
    "import heapq as hp\n",
    "\n",
    "from graph_data import GraphData\n",
    "from data_reader import DataReader\n",
    "from models import GNN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn import preprocessing\n",
    "#from IPython.core.debugger import Tracer\n",
    "from torch_geometric.utils import precision, recall, f1_score,true_positive, true_negative, false_positive, false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=117, type=<class 'int'>, choices=None, help='Random seed.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "'''\n",
    "----------------------------\n",
    "Dataset  |   batchnorm_dim\n",
    "----------------------------\n",
    "MUTAG    |     28\n",
    "PTC_MR   |     64\n",
    "BZR      |     57\n",
    "COX2     |     56\n",
    "COX2_MD  |     36\n",
    "BZR-MD   |     33\n",
    "PROTEINS |    620\n",
    "D&D      |   5748\n",
    "'''\n",
    "## 创建 ArgumentParser 对象\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#parser.add_argument 是 Python 中 argparse 模块的一部分，用于向参数解析器添加命令行参数#的定义。通过调用这个方法，你可以指定脚本在运行时可以接受哪些参数、这些参数的类型、默认值以及#帮助信息。\n",
    "## 添加命令行参数\n",
    "parser.add_argument('--device', default='cuda', help='Select CPU/CUDA for training.')\n",
    "parser.add_argument('--dataset', default='Applications', help='Dataset name.')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.0025, help='Initial learning rate.')\n",
    "parser.add_argument('--wdecay', type=float, default=2e-3, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Batch size.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=64, help='Number of hidden units.')\n",
    "parser.add_argument('--n_layers', type=int, default=2, help='Number of MLP layers for GraphSN.')\n",
    "parser.add_argument('--batchnorm_dim', type=int, default=30, help='Batchnormalization dimension for GraphSN layer.')\n",
    "parser.add_argument('--dropout_1', type=float, default=0.25, help='Dropout rate for concatenation the outputs.')\n",
    "parser.add_argument('--dropout_2', type=float, default=0.25, help='Dropout rate for MLP layers in GraphSN.')\n",
    "parser.add_argument('--n_folds', type=int, default=1, help='Number of folds in cross validation.')\n",
    "parser.add_argument('--threads', type=int, default=0, help='Number of threads.')\n",
    "parser.add_argument('--log_interval', type=int, default=10 , help='Log interval for visualizing outputs.')\n",
    "parser.add_argument('--seed', type=int, default=117, help='Random seed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析命令行参数\n",
    "args = parser.parse_args(\"\")\n",
    "# 使用解析后的参数\n",
    "#print(\"Device:\", args.device)\n",
    "#print(\"Dataset:\", args.dataset)\n",
    "#print(\"Epochs:\", args.epochs)\n",
    "#print(\"Learning Rate:\", args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data')\n",
    "\n",
    "datareader = DataReader(data_dir='./data/%s/' % args.dataset.upper(),\n",
    "                        fold_dir=None,\n",
    "                        rnd_state=np.random.RandomState(args.seed),\n",
    "                        folds=args.n_folds,\n",
    "                        use_cont_node_attr=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图的数量\n",
    "dataset_length = len(datareader.data['adj_list'])\n",
    "for itr in np.arange(dataset_length):\n",
    "    # 每个图的矩阵\n",
    "    A_array = datareader.data['adj_list'][itr]\n",
    "    G = nx.from_numpy_matrix(A_array)\n",
    "\n",
    "    sub_graphs = []\n",
    "    subgraph_nodes_list = []\n",
    "    sub_graphs_adj = []\n",
    "    sub_graph_edges = []\n",
    "    new_adj = torch.zeros(A_array.shape[0], A_array.shape[0])\n",
    "    \n",
    "    # 每个图的子图\n",
    "    for i in np.arange(len(A_array)):\n",
    "        s_indexes = []\n",
    "        for j in np.arange(len(A_array)):\n",
    "            s_indexes.append(i)\n",
    "            #if(A_array[i][j]==1):\n",
    "            if(A_array[i][j]!=0):\n",
    "                s_indexes.append(j)\n",
    "        sub_graphs.append(G.subgraph(s_indexes))\n",
    "\n",
    " \n",
    "    # 每个图的每个子图的节点\n",
    "    for i in np.arange(len(sub_graphs)):\n",
    "        subgraph_nodes_list.append(list(sub_graphs[i].nodes))\n",
    "\n",
    "    # 每个图的每个子图矩阵\n",
    "    for index in np.arange(len(sub_graphs)):\n",
    "        sub_graphs_adj.append(nx.adjacency_matrix(sub_graphs[index]).toarray())\n",
    "    #print(\"sub_graphs_adj:\", sub_graphs_adj)\n",
    "\n",
    "\n",
    "    # 每个图的每个子图的边的数量\n",
    "    for index in np.arange(len(sub_graphs)):\n",
    "        sub_graph_edges.append(sub_graphs[index].number_of_edges())\n",
    "\n",
    "    # 每个图(包含每个图的子图)的新的矩阵\n",
    "    for node in np.arange(len(subgraph_nodes_list)):\n",
    "        sub_adj = sub_graphs_adj[node]\n",
    "        for neighbors in np.arange(len(subgraph_nodes_list[node])):\n",
    "            index = subgraph_nodes_list[node][neighbors]\n",
    "            count = torch.tensor(0).float()\n",
    "            if(index==node):\n",
    "                continue\n",
    "            else:\n",
    "                c_neighbors = set(subgraph_nodes_list[node]).intersection(subgraph_nodes_list[index])\n",
    "                if index in c_neighbors:\n",
    "                    nodes_list = subgraph_nodes_list[node]\n",
    "                    sub_graph_index = nodes_list.index(index)\n",
    "                    c_neighbors_list = list(c_neighbors)\n",
    "                    #print(len(c_neighbors))\n",
    "                    for i, item1 in enumerate(nodes_list):\n",
    "                        if(item1 in c_neighbors):\n",
    "                            for item2 in c_neighbors_list:\n",
    "                                j = nodes_list.index(item2)\n",
    "                                count += sub_adj[i][j]\n",
    "\n",
    "                new_adj[node][index] = count / 2\n",
    "                new_adj[node][index] = new_adj[node][index]/(len(c_neighbors)*(len(c_neighbors)-1))\n",
    "                new_adj[node][index] = new_adj[node][index] * (len(c_neighbors) ** 2)\n",
    "\n",
    "    weight = torch.FloatTensor(new_adj)\n",
    "    weight = weight / weight.sum(1, keepdim=True)\n",
    "\n",
    "    weight = weight + torch.FloatTensor(A_array)\n",
    "\n",
    "    coeff = weight.sum(1, keepdim=True)\n",
    "    coeff = torch.diag((coeff.T)[0])\n",
    "\n",
    "    weight = weight + coeff\n",
    "\n",
    "    weight = weight.detach().numpy()\n",
    "    #weight = np.nan_to_num(weight, nan=0)\n",
    "    weight = np.nan_to_num(weight)\n",
    "\n",
    "    datareader.data['adj_list'][itr] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc_folds = []\n",
    "#accuracy_arr = np.zeros((10, args.epochs), dtype=float)\n",
    "accuracy_arr = np.zeros((1, args.epochs), dtype=float)\n",
    "for fold_id in range(args.n_folds):\n",
    "    print('\\nFOLD', fold_id)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        gdata = GraphData(fold_id=fold_id,\n",
    "                             datareader=datareader,\n",
    "                             split=split)\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=split.find('train') >= 0,\n",
    "                                             num_workers=args.threads)\n",
    "        loaders.append(loader)\n",
    "    #print(loaders)\n",
    "    \n",
    "    model = GNN(input_dim=loaders[0].dataset.features_dim,\n",
    "                hidden_dim=args.hidden_dim,\n",
    "                output_dim=loaders[0].dataset.n_classes,\n",
    "                n_layers=args.n_layers,\n",
    "                batchnorm_dim=args.batchnorm_dim, \n",
    "                dropout_1=args.dropout_1, \n",
    "                dropout_2=args.dropout_2).to(args.device)\n",
    "\n",
    "    print('\\nInitialize model')\n",
    "    print(model)\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N trainable parameters:', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=args.lr,\n",
    "                weight_decay=args.wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    \n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.5)\n",
    "\n",
    "    def train(train_loader):\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, n_samples = 0, 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            scheduler.step()\n",
    "            if batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1) ))\n",
    "            #scheduler.step()\n",
    "\n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        preds_list = []\n",
    "        data_list = []\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(args.device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            #pred = output.detach().cuda().max(1, keepdim=True)[1]\n",
    "            ##################################\n",
    "            data_list += data[4].tolist()\n",
    "            preds_list += pred.tolist()\n",
    "            #################################\n",
    "\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "            #correct += pred.eq(data[4].detach().cuda().view_as(pred)).sum().item()\n",
    "        labels = torch.Tensor(data_list)\n",
    "        preds = torch.Tensor(preds_list)\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        test_loss /= n_samples\n",
    "\n",
    "        acc = 100. * correct / n_samples\n",
    "\n",
    "        #############################################################################\n",
    "        classnums = 21\n",
    "        r = recall(preds, labels.view_as(preds), classnums)\n",
    "        p = precision(preds, labels.view_as(preds), classnums)\n",
    "        f1 = f1_score(preds, labels.view_as(preds), classnums)\n",
    "        fp = false_positive(preds, labels.view_as(preds), classnums)\n",
    "        fn = false_negative(preds, labels.view_as(preds), classnums)\n",
    "        tp = true_positive(preds, labels.view_as(preds), classnums)\n",
    "        tn = true_negative(preds, labels.view_as(preds), classnums)\n",
    "\n",
    "        r = (r.numpy()).round(7)\n",
    "        p = (p.numpy()).round(7)\n",
    "        f1 = (f1.numpy()).round(7)\n",
    "        fp = fp.numpy()\n",
    "        fn = fn.numpy()\n",
    "        tp = tp.numpy()\n",
    "        tn = tn.numpy()\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        \"\"\"for i in range(classnums):\n",
    "            tpr.append( tp[i] / (tp[i] + fn[i]))\n",
    "            fpr.append(fp[i] / (fp[i] + tn[i]))\n",
    "        print('test_test_tpr', \" \".join('%s' % id for id in tpr))\n",
    "        print('test_test_fpr', \" \".join('%s' % id for id in fpr))\"\"\"\n",
    "        print('test_test_recall', \" \".join('%s' % id for id in r))\n",
    "        print('test_test_precision', \" \".join('%s' % id for id in p))\n",
    "        print('test_test_F1', \" \".join('%s' % id for id in f1))\n",
    "        ######################################################################\n",
    "        conf_matrix = get_confusion_matrix(labels.view_as(preds), preds)\n",
    "        plt.figure(figsize=(26, 26), dpi=60)\n",
    "        plot_confusion_matrix(conf_matrix, classnums, epoch)\n",
    "        ######################################################################\n",
    "\n",
    "\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return acc\n",
    "    ###################################################################\n",
    "    def plot_confusion_matrix(conf_matrix, num_classes, epoch):\n",
    "        plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "        indices = range(len(conf_matrix))\n",
    "        if num_classes == 21:\n",
    "            classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "        elif num_classes == 18:\n",
    "            classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "        elif num_classes == 15:\n",
    "            classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "        elif num_classes == 27:\n",
    "            classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
    "        elif num_classes == 33:\n",
    "            classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
    "        plt.xticks(indices, classes)\n",
    "        plt.yticks(indices, classes)\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('y_pred')\n",
    "        plt.ylabel('y_true')\n",
    "        for first_index in range(len(conf_matrix)):\n",
    "            for second_index in range(len(conf_matrix[first_index])):\n",
    "                plt.text(first_index, second_index, conf_matrix[second_index, first_index])\n",
    "        if epoch == 0:\n",
    "            plt.savefig('./fig0.png', format='png')\n",
    "        if epoch == 1:\n",
    "            plt.savefig('./fig1.png', format='png')\n",
    "        if epoch == 2:\n",
    "            plt.savefig('./fig2.png', format='png')\n",
    "        if epoch == 3:\n",
    "            plt.savefig('./fig3.png', format='png')\n",
    "        if epoch == 4:\n",
    "            plt.savefig('./fig4.png', format='png')\n",
    "        if epoch == 5:\n",
    "            plt.savefig('./fig5.png', format='png')\n",
    "        if epoch == 6:\n",
    "            plt.savefig('./fig6.png', format='png')\n",
    "        if epoch == 7:\n",
    "            plt.savefig('./fig7.png', format='png')\n",
    "        if epoch == 8:\n",
    "            plt.savefig('./fig8.png', format='png')\n",
    "        if epoch == 9:\n",
    "            plt.savefig('./fig9.png', format='png')\n",
    "        plt.show()\n",
    "\n",
    "    def get_confusion_matrix(label, pred):\n",
    "        conf_matrix = confusion_matrix(label, pred)\n",
    "        return conf_matrix\n",
    "    ###############################################################################################\n",
    "    loss_fn = F.cross_entropy\n",
    "    max_acc = 0.0\n",
    "    t_start = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        train(loaders[0])\n",
    "        acc = test(loaders[1])\n",
    "        accuracy_arr[fold_id][epoch] = acc\n",
    "        max_acc = max(max_acc, acc)\n",
    "    print(\"time: {:.4f}s\".format(time.time() - t_start))\n",
    "    acc_folds.append(max_acc)\n",
    "\n",
    "print(acc_folds)\n",
    "#print('{}-fold cross validation avg acc (+- std): {} ({})'.format(args.n_folds, np.mean(acc_folds), np.std(acc_folds)))\n",
    "\n",
    "# mean_validation = accuracy_arr.mean(axis=0)\n",
    "# maximum_epoch = np.argmax(mean_validation)\n",
    "# average = np.mean(accuracy_arr[:, maximum_epoch])\n",
    "# standard_dev = np.std(accuracy_arr[:, maximum_epoch])\n",
    "# print('{}-fold cross validation avg acc (+- std): {} ({})'.format(args.n_folds, average, standard_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
