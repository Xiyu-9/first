#!/usr/bin/env python
# -*- coding: utf-8 -*-
import numpy as np
import os
import glob
import re
import argparse
from data_reader import DataReader
from dataprocessor import DataProcessor
from aug import DataAugmentation
import numpy as np
import time
import networkx as nx
import torch
import torch.utils
import torch.utils.data
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import argparse
import augmentors as A
import heapq as hp
import pickle
import time

from evu import AverageMeter, ProgressMeter
from cluster_analysis import *
from sklearn import metrics
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
import seaborn as sns
from sklearn.metrics.pairwise import euclidean_distances

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import metrics
from munkres import Munkres
from graph_data import GraphData
from data_reader import DataReader
from models import GNN
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from itertools import chain
from models import Encoder
from losses import SupConLoss
from losses import ClusterLoss
from losses import ImprovedClusterLoss
from sklearn import preprocessing
#from IPython.core.debugger import Tracer
from torch_geometric.utils import precision, recall, f1_score,true_positive, true_negative, false_positive, false_negative
from tqdm import tqdm
from graphaug import GraphAugmentor
import logging
import os
from datetime import datetime
# =============================================================================
# Experiment parameters
# =============================================================================
'''
----------------------------
Dataset  |   batchnorm_dim
----------------------------
MUTAG    |     28
PTC_MR   |     64
BZR      |     57
COX2     |     56
COX2_MD  |     36
BZR-MD   |     33
PROTEINS |    620
D&D      |   5748
'''

parser = argparse.ArgumentParser()
parser.add_argument('--device', default='cuda', help='Select CPU/CUDA for training.')
parser.add_argument('--dataset', default='Applications', help='Dataset name.')
parser.add_argument('--epochs', type=int, default=40, help='Number of epochs to train.')
parser.add_argument('--lr', type=float, default=0.0025, help='Initial learning rate.')
parser.add_argument('--wdecay', type=float, default=2e-3, help='Weight decay (L2 loss on parameters).')
parser.add_argument('--batch_size', type=int, default=64, help='Batch size.')
parser.add_argument('--hidden_dim', type=int, default=64, help='Number of hidden units.')
parser.add_argument('--n_layers', type=int, default=2, help='Number of MLP layers for GraphSN.')
parser.add_argument('--batchnorm_dim', type=int, default=30, help='Batchnormalization dimension for GraphSN layer.')
parser.add_argument('--dropout_1', type=float, default=0.25, help='Dropout rate for concatenation the outputs.')
parser.add_argument('--dropout_2', type=float, default=0.25, help='Dropout rate for MLP layers in GraphSN.')
parser.add_argument('--n_folds', type=int, default=1, help='Number of folds in cross validation.')
parser.add_argument('--threads', type=int, default=0, help='Number of threads.')
parser.add_argument('--log_interval', type=int, default=10 , help='Log interval for visualizing outputs.')
parser.add_argument('--seed', type=int, default=117, help='Random seed.')
parser.add_argument('--used_loss', type=str, default="default", help='Type of loss function used.')

parser.add_argument('--entropy_weight', type=float, default=0.5, help='Weight for entropy term in improved cluster loss')
parser.add_argument('--repulsion_weight', type=float, default=1.0, help='Weight for repulsion term in improved cluster loss')
parser.add_argument('--cluster_temperature', type=float, default=0.1, help='Temperature for cluster similarity')
parser.add_argument('--cluster_loss_weight', type=float, default=2.0, help='Temperature for cluster loss weight')


# æ–°å¢ï¼šç›‘æ§å¹¶è®°å½•ä¸åŒç±»åˆ«ç°‡ä¹‹é—´çš„å¹³å‡è·ç¦»å‡½æ•°
def monitor_cluster_distances(epoch, gc, labels, save_dir="./cluster_distances/"):
    """ç›‘æ§å¹¶è®°å½•ä¸åŒç±»åˆ«ç°‡ä¹‹é—´çš„å¹³å‡è·ç¦»"""
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    gc_normalized = F.normalize(gc, p=2, dim=1)
    
    # è®¡ç®—ç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = torch.mm(gc_normalized, gc_normalized.t())
    
    num_classes = labels.max().item() + 1
    class_distances = torch.zeros((num_classes, num_classes), device=gc.device)
    class_counts = torch.zeros((num_classes, num_classes), device=gc.device)
    
    # èšåˆåŒç±»åˆ«é—´çš„ç›¸ä¼¼åº¦
    for i in range(len(labels)):
        for j in range(len(labels)):
            class_i = labels[i].item()
            class_j = labels[j].item()
            if i != j:  # æ’é™¤è‡ªèº«
                class_distances[class_i, class_j] += 1 - similarity_matrix[i, j]  # ä½¿ç”¨è·ç¦» = 1-ç›¸ä¼¼åº¦
                class_counts[class_i, class_j] += 1
    
    # é¿å…é™¤é›¶
    class_counts[class_counts == 0] = 1
    avg_class_distances = class_distances / class_counts
    
    # è®¡ç®—ç±»é—´å¹³å‡è·ç¦»å’Œæœ€å°è·ç¦»
    mask = torch.ones_like(avg_class_distances, dtype=torch.bool)
    mask.fill_diagonal_(False)
    inter_class_distances = avg_class_distances[mask].view(num_classes, num_classes-1)
    
    avg_distance = inter_class_distances.mean().item()
    min_distance = inter_class_distances.min().item()
    
    # è®°å½•åˆ°æ—¥å¿—
    print(f"Epoch {epoch} - Avg Class Distance: {avg_distance:.4f}, Min Class Distance: {min_distance:.4f}")
    
    # ä¿å­˜è·ç¦»çŸ©é˜µå›¾
    if epoch % 5 == 0:  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡
        plt.figure(figsize=(10, 8))
        plt.imshow(avg_class_distances.cpu().numpy(), cmap='viridis')
        plt.colorbar()
        plt.title(f"Class Distance Matrix - Epoch {epoch}")
        plt.savefig(f'{save_dir}/class_distances_epoch_{epoch}.png')
        plt.close()
    
    return {
        'avg_distance': avg_distance,
        'min_distance': min_distance,
        'distance_matrix': avg_class_distances.cpu().numpy()
    }
def find_latest_encoder_checkpoint(fold_id):
    """æŸ¥æ‰¾ç‰¹å®šfold_idçš„æœ€æ–°encoderæ¨¡å‹æ£€æŸ¥ç‚¹"""
    # æŸ¥æ‰¾ä¸å½“å‰fold_idç›¸å…³çš„æ‰€æœ‰encoderæ¨¡å‹æ–‡ä»¶
    pattern = f'encoder_model_fold_{fold_id}_epoch_*.pth'
    model_files = glob.glob(pattern)
    
    if not model_files:
        return None, 0  # æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ
    
    # ä»æ–‡ä»¶åä¸­æå–epochç¼–å·å¹¶æ‰¾å‡ºæœ€å¤§å€¼
    epoch_numbers = []
    for file in model_files:
        match = re.search(r'epoch_(\d+)\.pth', file)
        if match:
            epoch_numbers.append((int(match.group(1)), file))
    
    if not epoch_numbers:
        return None, 0
    
    # æ‰¾å‡ºæœ€å¤§epochç¼–å·çš„æ–‡ä»¶
    latest_epoch, latest_file = max(epoch_numbers, key=lambda x: x[0])
    return latest_file, latest_epoch


def find_improved_neighbors(g, gs1, gs2, labels):
    """
    ä»ä¸‰ä¸ªå›¾é›†ï¼ˆåŸå§‹å›¾å’Œä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬ï¼‰ä¸­é€‰æ‹©æ­£æ ·æœ¬ï¼ˆåŒç±»ï¼‰å’Œè´Ÿæ ·æœ¬ï¼ˆä¸åŒç±»ï¼‰é‚»å±…
    
    å‚æ•°:
    g, gs1, gs2: ä¸‰ä¸ªå›¾é›†çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º [batch_size, feature_dim]
    labels: æ ‡ç­¾å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size]
    
    è¿”å›:
    positive_neighbors: æ¯ä¸ªæ ·æœ¬çš„æ­£é‚»å±…ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, feature_dim]
    negative_neighbors: æ¯ä¸ªæ ·æœ¬çš„è´Ÿé‚»å±…ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, feature_dim]
    """
    device = g.device
    batch_size = g.size(0)
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    all_features = torch.cat([g, gs1, gs2], dim=0)  # [3*batch_size, feature_dim]
    # æ ‡ç­¾ä¹Ÿè¦è·Ÿç€å¤åˆ¶ä¸‰ä»½
    all_labels = labels.repeat(3)  # [3*batch_size]
    
    pos_neighbors = []  # åŒç±»æ­£é‚»å±…
    neg_neighbors = []  # ä¸åŒç±»è´Ÿé‚»å±…
    
    for i in range(batch_size):
        current_label = labels[i]
        current_embedding = g[i]
        
        # åˆ›å»ºæ©ç ï¼Œæ’é™¤è‡ªèº«
        not_self_mask = torch.ones(3*batch_size, dtype=torch.bool, device=device)
        not_self_mask[i] = False  # æ’é™¤åŸå§‹å›¾ä¸­çš„è‡ªèº«
        not_self_mask[i + batch_size] = False  # æ’é™¤gs1ä¸­å¯¹åº”çš„è‡ªèº«
        not_self_mask[i + 2*batch_size] = False  # æ’é™¤gs2ä¸­å¯¹åº”çš„è‡ªèº«
        
        # æ‰¾åŒç±»æ ·æœ¬
        same_class_mask = (all_labels == current_label) & not_self_mask
        # æ‰¾ä¸åŒç±»æ ·æœ¬
        diff_class_mask = (all_labels != current_label)
        
        # è®¡ç®—ä¸æ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼åº¦ (ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦)
        similarities = F.cosine_similarity(current_embedding.unsqueeze(0), all_features)
        
        # é€‰æ‹©åŒç±»æœ€ç›¸ä¼¼æ ·æœ¬ä½œä¸ºæ­£é‚»å±…
        if same_class_mask.any():
            # å°†ä¸ç¬¦åˆæ¡ä»¶çš„ä½ç½®è®¾ä¸ºå¾ˆå°çš„å€¼
            valid_similarities = similarities.clone()
            valid_similarities[~same_class_mask] = -2.0  # ä½äºä½™å¼¦ç›¸ä¼¼åº¦çš„æœ€å°å€¼-1
            pos_idx = valid_similarities.argmax().item()
            pos_neighbors.append(all_features[pos_idx])
        else:
            # æå°‘æ•°æƒ…å†µï¼šæ²¡æœ‰åŒç±»æ ·æœ¬ï¼Œç”¨è‡ªèº«ä½œä¸ºæ­£é‚»å±…
            pos_neighbors.append(current_embedding)
        
        # é€‰æ‹©ä¸åŒç±»æœ€ç›¸ä¼¼æ ·æœ¬ä½œä¸º"ç¡¬"è´Ÿé‚»å±…ï¼ˆæœ€å…·æŒ‘æˆ˜æ€§çš„è´Ÿæ ·æœ¬ï¼‰
        if diff_class_mask.any():
            valid_similarities = similarities.clone()
            valid_similarities[~diff_class_mask] = -2.0
            neg_idx = valid_similarities.argmax().item()
            neg_neighbors.append(all_features[neg_idx])
        else:
            # æå°‘æ•°æƒ…å†µï¼šæ²¡æœ‰ä¸åŒç±»æ ·æœ¬ï¼Œéšæœºé€‰æ‹©
            rand_idx = torch.randint(3*batch_size, (1,)).item()
            neg_neighbors.append(all_features[rand_idx])
    
    return torch.stack(pos_neighbors), torch.stack(neg_neighbors)
def find_nearest_neighbors(g, gs1, gs2, labels):
    """
    åœ¨ä¸‰ä¸ªå›¾é›†ä¸­æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬çš„æœ€è¿‘é‚»å±…
    
    å‚æ•°:
    g (torch.Tensor): åŸå§‹å›¾ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, project_dim]
    gs1 (torch.Tensor): å¢å¼º1çš„å›¾ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, project_dim]
    gs2 (torch.Tensor): å¢å¼º2çš„å›¾ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, project_dim]
    labels (torch.Tensor): æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º [batch_size]
    
    è¿”å›:
    neighbors (torch.Tensor): é‚»å±…é›†ï¼Œå½¢çŠ¶ä¸º [batch_size, project_dim]
    """
    batch_size = g.size(0)
    device = g.device
    neighbors = torch.zeros_like(g)
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
    all_features = torch.cat([g, gs1, gs2], dim=0)  # [3*batch_size, project_dim]
    all_labels = labels.repeat(3)  # å‡è®¾ä¸‰ä¸ªå›¾é›†çš„æ ‡ç­¾ç›¸åŒ
    
    # è®¡ç®—ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ (ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦)
    normalized_features = F.normalize(all_features, p=2, dim=1)
    similarity_matrix = torch.mm(normalized_features[:batch_size], normalized_features.t())
    
    for i in range(batch_size):
        current_label = labels[i]
        
        # åˆ›å»ºæ©ç æ¥ç­›é€‰ç›¸åŒæ ‡ç­¾çš„æ ·æœ¬
        same_label_mask = (all_labels == current_label)
        
        # åˆ›å»ºæ©ç æ¥æ’é™¤è‡ªèº«
        not_self_mask = torch.ones(3*batch_size, dtype=torch.bool, device=device)
        not_self_mask[i] = False  # æ’é™¤gä¸­çš„è‡ªèº«
        
        # ç»“åˆæ©ç 
        valid_mask = same_label_mask & not_self_mask
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ ·æœ¬
        if not valid_mask.any():
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„é‚»å±…å€™é€‰ï¼Œä½¿ç”¨è‡ªèº«ä½œä¸ºé‚»å±…
            neighbors[i] = g[i]
            continue
        
        # è·å–æœ‰æ•ˆæ ·æœ¬çš„ç›¸ä¼¼åº¦
        valid_similarities = similarity_matrix[i].clone()
        valid_similarities[~valid_mask] = -1  # å°†æ— æ•ˆæ ·æœ¬çš„ç›¸ä¼¼åº¦è®¾ä¸º-1
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ ·æœ¬ç´¢å¼•
        most_similar_idx = valid_similarities.argmax().item()
        
        # ä»å¯¹åº”çš„å›¾é›†ä¸­è·å–é‚»å±…
        if most_similar_idx < batch_size:  # æ¥è‡ªg
            neighbors[i] = g[most_similar_idx]
        elif most_similar_idx < 2*batch_size:  # æ¥è‡ªgs1
            neighbors[i] = gs1[most_similar_idx - batch_size]
        else:  # æ¥è‡ªgs2
            neighbors[i] = gs2[most_similar_idx - 2*batch_size]
    
    return neighbors
# è®¾ç½®æ—¥å¿—é…ç½®
def setup_logger():
    # åˆ›å»ºlogger
    logger = logging.getLogger('training')
    logger.setLevel(logging.INFO)
    
    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
    log_file = 'train1.log'
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # è®¾ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨åˆ°logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger
def process_adjacency_matrices(datareader): 
    """
    å¤„ç†æ•°æ®é›†ä¸­çš„é‚»æ¥çŸ©é˜µï¼Œç”Ÿæˆå¢å¼ºçš„é‚»æ¥çŸ©é˜µ
    Args:
        datareader: åŒ…å«å›¾æ•°æ®çš„è¯»å–å™¨å¯¹è±¡ï¼Œéœ€è¦åŒ…å«adj_listå±æ€§
    """
    dataset_length = len(datareader.data['adj_list'])
    
    for itr in tqdm(range(dataset_length), desc="Processing graphs", unit="graph"):
        # è·å–æ¯ä¸ªå›¾çš„é‚»æ¥çŸ©é˜µ
        A_array = datareader.data['adj_list'][itr]
        G = nx.from_numpy_array(A_array)

        sub_graphs = []
        subgraph_nodes_list = []
        sub_graphs_adj = []
        sub_graph_edges = []
        new_adj = torch.zeros(A_array.shape[0], A_array.shape[0])
        
        # å¯¹æ¯ä¸ªå›¾ç”Ÿæˆå­å›¾ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸å…¶è¿æ¥çš„é‚»å±…æ„æˆä¸€ä¸ªå­å›¾
        for i in np.arange(len(A_array)):
            s_indexes = []
            for j in np.arange(len(A_array)):
                s_indexes.append(i)
                if A_array[i][j] != 0:
                    s_indexes.append(j)
            sub_graphs.append(G.subgraph(s_indexes))
    
        # è·å–æ¯ä¸ªå­å›¾çš„èŠ‚ç‚¹åˆ—è¡¨
        for i in np.arange(len(sub_graphs)):
            subgraph_nodes_list.append(list(sub_graphs[i].nodes))
    
        # è·å–æ¯ä¸ªå­å›¾çš„é‚»æ¥çŸ©é˜µ
        for index in np.arange(len(sub_graphs)):
            sub_graphs_adj.append(nx.adjacency_matrix(sub_graphs[index]).toarray())
    
        # ç»Ÿè®¡æ¯ä¸ªå­å›¾çš„è¾¹æ•°é‡
        for index in np.arange(len(sub_graphs)):
            sub_graph_edges.append(sub_graphs[index].number_of_edges())
    
        # åˆ©ç”¨å­å›¾ä¿¡æ¯æ„é€ æ–°çš„é‚»æ¥çŸ©é˜µ new_adj
        for node in np.arange(len(subgraph_nodes_list)):
            sub_adj = sub_graphs_adj[node]
            for neighbors in np.arange(len(subgraph_nodes_list[node])):
                index = subgraph_nodes_list[node][neighbors]
                count = torch.tensor(0).float()
                if index == node:
                    continue
                else:
                    c_neighbors = set(subgraph_nodes_list[node]).intersection(subgraph_nodes_list[index])
                    if index in c_neighbors:
                        nodes_list = subgraph_nodes_list[node]
                        c_neighbors_list = list(c_neighbors)
                        for i, item1 in enumerate(nodes_list):
                            if item1 in c_neighbors:
                                for item2 in c_neighbors_list:
                                    j = nodes_list.index(item2)
                                    count += sub_adj[i][j]
    
                    new_adj[node][index] = count / 2
                    new_adj[node][index] = new_adj[node][index] / (len(c_neighbors) * (len(c_neighbors) - 1))
                    new_adj[node][index] = new_adj[node][index] * (len(c_neighbors) ** 2)
    
        weight = torch.FloatTensor(new_adj)
        weight = weight / weight.sum(1, keepdim=True)
        weight = weight + torch.FloatTensor(A_array)
    
        coeff = weight.sum(1, keepdim=True)
        coeff = torch.diag((coeff.T)[0])
    
        weight = weight + coeff
    
        weight = weight.detach().numpy()
        weight = np.nan_to_num(weight)
    
        datareader.data['adj_list'][itr] = weight

    # ä¿å­˜å¤„ç†åçš„æ•°æ®é›†
    #with open(save_path, 'wb') as f:
    #    pickle.dump(datareader.data, f)
    #print(f'Processed data saved to {save_path}')

def evaluate(label, pred):
    print(f"çœŸå®ç±»åˆ«æ•°é‡: {len(set(label))}, é¢„æµ‹ç±»åˆ«æ•°é‡: {len(set(pred))}")
    print("çœŸå®æ ‡ç­¾ label:", np.unique(label))
    print("èšç±»ç»“æœ pred:", np.unique(pred))

    # è®¡ç®—æ ‡å‡†åŒ–äº’ä¿¡æ¯ (NMI)
    nmi = metrics.normalized_mutual_info_score(label, pred)
    # è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI)
    ari = metrics.adjusted_rand_score(label, pred)
    # è®¡ç®—Fowlkes-MallowsæŒ‡æ•° (F)
    f = metrics.fowlkes_mallows_score(label, pred)
    # è°ƒæ•´é¢„æµ‹æ ‡ç­¾ï¼Œä½¿å…¶ä¸çœŸå®æ ‡ç­¾å°½å¯èƒ½åŒ¹é…
    pred_adjusted = get_y_preds(label, pred, len(set(label)))
    # è®¡ç®—è°ƒæ•´åçš„å‡†ç¡®ç‡
    acc = metrics.accuracy_score(pred_adjusted, label)
    return nmi, ari, f, acc

def calculate_cost_matrix(C, n_clusters):
    """
    è®¡ç®—ä»£ä»·çŸ©é˜µï¼Œç”¨äºè§£å†³æœ€ä¼˜åŒ¹é…é—®é¢˜ã€‚
    Cï¼šæ··æ·†çŸ©é˜µï¼Œè¡¨ç¤ºèšç±»ç»“æœä¸çœŸå®æ ‡ç­¾çš„å¯¹åº”å…³ç³»ã€‚
    """
    cost_matrix = np.zeros((n_clusters, n_clusters))
    for j in range(n_clusters):
        s = np.sum(C[:, j])
        for i in range(n_clusters):
            t = C[i, j]
            cost_matrix[j, i] = s - t
    print("æ··æ·†çŸ©é˜µ:\n", C)
    print("ä»£ä»·çŸ©é˜µ:\n", cost_matrix)
    return cost_matrix

def get_cluster_labels_from_indices(indices):
    """
    è§£æ Munkres ç®—æ³•è®¡ç®—å¾—åˆ°çš„æœ€ä¼˜åŒ¹é…ç´¢å¼•ï¼Œè¿”å›æœ€ä½³çš„ç°‡æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚
    """
    n_clusters = len(indices)
    cluster_labels = np.zeros(n_clusters)
    for i in range(n_clusters):
        cluster_labels[i] = indices[i][1]
    return cluster_labels

def get_y_preds(y_true, cluster_assignments, n_clusters):
    """
    è°ƒæ•´é¢„æµ‹æ ‡ç­¾ï¼Œä½¿å…¶ä¸çœŸå®æ ‡ç­¾å°½å¯èƒ½åŒ¹é…ã€‚
    """
    confusion_matrix = metrics.confusion_matrix(y_true, cluster_assignments, labels=None)
    cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters)
    indices = Munkres().compute(cost_matrix)
    kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)
    if np.min(cluster_assignments) != 0:
        cluster_assignments = cluster_assignments - np.min(cluster_assignments)
    y_pred = kmeans_to_true_cluster_labels[cluster_assignments]
    print("æœ€ä½³æ˜ å°„å…³ç³»:", kmeans_to_true_cluster_labels)
    print("èšç±»åˆ†é…çš„æœ€å¤§ç´¢å¼•:", np.max(cluster_assignments))
    return y_pred

# å‡è®¾ä½ å·²æœ‰çš„æ··æ·†çŸ©é˜µç»˜åˆ¶å‡½æ•°
def plot_confusion_matrix(cm, n_classes, epoch):
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix at Epoch {epoch}')
    plt.colorbar()
    tick_marks = np.arange(n_classes)
    plt.xticks(tick_marks, tick_marks, rotation=45)
    plt.yticks(tick_marks, tick_marks)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.show()





def main():
    # å¦‚æœç›´æ¥åœ¨å‘½ä»¤è¡Œè¿è¡Œï¼Œåˆ™è§£æå‘½ä»¤è¡Œå‚æ•°ï¼›åœ¨ ipynb ä¸­å¯ä»¥ä¼ å…¥ç©ºåˆ—è¡¨
    args = parser.parse_args()
    print('Loading data')
    device = torch.device(args.device)
    print(f'Using device {device}')
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆGCLC/ï¼‰çš„ç»å¯¹è·¯å¾„
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))  # å…³é”®ä¿®æ­£ï¼

    # åŠ¨æ€ç”Ÿæˆæ•°æ®ç›®å½•è·¯å¾„
    data_dir = os.path.join(project_root, 'Graph_Classification', 'data', 'APPLICATIONS')

    # è°ƒè¯•è¾“å‡º
    print(f"[DEBUG] æ•°æ®é›†ç»å¯¹è·¯å¾„: {data_dir}")
    print(f"[DEBUG] è·¯å¾„æ˜¯å¦å­˜åœ¨? {os.path.exists(data_dir)}")
    
    # åˆå§‹åŒ– DataReader
    datareader = DataReader(
        data_dir=data_dir + '/',  # ç¡®ä¿ä»¥æ–œæ ç»“å°¾
        fold_dir=None,
        rnd_state=np.random.RandomState(args.seed),
        folds=args.n_folds,
        use_cont_node_attr=False
    )
    # æ„å»ºæ•°æ®è¯»å–å™¨ï¼ˆDataReaderï¼‰
    #datareader = DataReader(data_dir='./Graph_Classification/data/%s/' % args.dataset.upper(),
    #                        fold_dir=None,
    #                        rnd_state=np.random.RandomState(args.seed),
    #                        folds=args.n_folds,
    #                       use_cont_node_attr=False)
    print('Loading data completed')
    # =============================================================================
    # å¯¹æ•°æ®ä¸­æ¯ä¸ªå›¾çš„é‚»æ¥çŸ©é˜µè¿›è¡Œå¤„ç†
    # =============================================================================

    #processed_data_path = './processed_data.pkl'
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²å¤„ç†çš„æ•°æ®æ–‡ä»¶
    #if os.path.exists(processed_data_path):
    #    print('Loading processed data')
    #    with open(processed_data_path, 'rb') as f:
    #        datareader.data = pickle.load(f)
    #    print('Loading processed data completed')
    #else:
    #    print('Processing graph data')
    #    process_adjacency_matrices(datareader, processed_data_path)
    #    print('Processing graph data completed')
    print('processing graph data')

    process_adjacency_matrices(datareader)

    print('processing graph data completed')

    # =============================================================================
    # è®­ç»ƒå’Œæµ‹è¯•éƒ¨åˆ†
    # =============================================================================

    
    print('training and testing')
    acc_folds = []
    # accuracy_arr çš„ç»´åº¦ï¼šæŠ˜æ•° x epoch æ•°
    accuracy_arr = np.zeros((args.n_folds, args.epochs), dtype=float)
    
    for fold_id in range(args.n_folds):
        print('\nFOLD', fold_id)
        loaders = []
        for split in ['train', 'test']:
            gdata = GraphData(fold_id=fold_id,
                              datareader=datareader,
                              split=split)
    
            loader = torch.utils.data.DataLoader(gdata, 
                                                 batch_size=args.batch_size,
                                                 shuffle=('train' in split),
                                                 num_workers=args.threads)
            loaders.append(loader)
        
        # æ„å»º GNN æ¨¡å‹
        model = GNN(input_dim=loaders[0].dataset.features_dim,
                    hidden_dim=args.hidden_dim,
                    output_dim=loaders[0].dataset.n_classes,
                    n_layers=args.n_layers,
                    batchnorm_dim=args.batchnorm_dim, 
                    dropout_1=args.dropout_1, 
                    dropout_2=args.dropout_2).to(device)
    
        print('\nInitialize model')
        print(model)
        
        c = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print('N trainable parameters:', c)
        #print("Model device:", next(model.parameters()).device)
        
        #user_input = input("è¯·è¾“å…¥æ•°å­—3ä»¥ç»§ç»­è¿è¡Œ: ")   
        aug1 = GraphAugmentor(drop_prob=0.9, insert_ratio=0.1, edge_prob=0.9)
        aug2 = GraphAugmentor(drop_prob=0.9, insert_ratio=0.1, edge_prob=0.9)

        encoder_model = Encoder(encoder=model, augmentor=(aug1, aug2)).to(device)
        # æŸ¥æ‰¾æœ€æ–°çš„encoderæ£€æŸ¥ç‚¹
        latest_encoder_path, start_epoch = find_latest_encoder_checkpoint(fold_id)
        
        # å¦‚æœæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼ŒåŠ è½½encoderæ¨¡å‹çŠ¶æ€
        if latest_encoder_path:
            print(f'æ‰¾åˆ°æœ€æ–°encoderæ£€æŸ¥ç‚¹ {latest_encoder_path}ï¼Œä» epoch {start_epoch+1} ç»§ç»­è®­ç»ƒ')
            encoder_model.load_state_dict(torch.load(latest_encoder_path))
        else:
            print('æœªæ‰¾åˆ°encoderæ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ')
            start_epoch = 0
        #print("Encoder model device:", next(encoder_model.parameters()).device)
        optimizer = optim.Adam(
                    filter(lambda p: p.requires_grad, encoder_model.parameters()),
                    lr=args.lr,
                    weight_decay=args.wdecay,
                    betas=(0.5, 0.999))
        
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹ï¼Œå°è¯•åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer_path = latest_encoder_path.replace('.pth', '_optimizer.pth') if latest_encoder_path else None
        if optimizer_path and os.path.exists(optimizer_path):
            print(f'åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€: {optimizer_path}')
            optimizer.load_state_dict(torch.load(optimizer_path))
        #scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.5)
        # å¯ä»¥æ ¹æ®ä½ çš„è®­ç»ƒè½®æ•°è°ƒæ•´å­¦ä¹ ç‡ä¸‹é™çš„æ—¶æœº
        scheduler = lr_scheduler.MultiStepLR(
            optimizer, 
            milestones=[25, 35],  # åœ¨1/3å’Œ2/3å¤„é™ä½å­¦ä¹ ç‡
            gamma=0.5
        )
        for _ in range(start_epoch):
            scheduler.step()
        logger = setup_logger()
        # å®šä¹‰è®­ç»ƒå‡½æ•°
        def train(train_loader, epoch):
             # åˆ›å»ºå„ç§æŸå¤±è¿½è¸ªå™¨
            losses = AverageMeter('Loss', ':.4e')
            instance_losses = AverageMeter('Instance Loss', ':.4e')
            cluster_losses = AverageMeter('Cluster Loss', ':.4e')
            consistency_losses = AverageMeter('Consistency Loss', ':.4e')
            entropy_losses = AverageMeter('Entropy Loss', ':.4e')
            repulsion_losses = AverageMeter('Repulsion Loss', ':.4e')  # æ–°å¢
            # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºå™¨
            progress = ProgressMeter(len(train_loader),
                       [losses, instance_losses, cluster_losses, consistency_losses, entropy_losses, repulsion_losses], 
                       prefix="Epoch: [{}]".format(epoch),
                       output_file="progress_log.txt")
            
            encoder_model.train()
            
            #model.train()
            start = time.time()
            train_loss, n_samples = 0, 0
            for batch_idx, data in enumerate(train_loader):
    
                data = [d.to(device) for d in data]
                optimizer.zero_grad()
                
                g, gs1, gs2 = encoder_model(data)
                output = torch.cat((gs1, gs2), dim=0)
                
                gi1 = encoder_model.encoder.instance_projector(gs1)
                gi2 = encoder_model.encoder.instance_projector(gs2)
                
                gc= encoder_model.encoder.cluster_projector(g)
                
                total_loss, consistency_loss, entropy_loss,repulsion_loss = [], [], [], []
                # æ‰¾åˆ°æ­£è´Ÿé‚»å±…(ä»ä¸‰ä¸ªå›¾é›†ä¸­é€‰æ‹©)
                pos_neighbors, neg_neighbors = find_improved_neighbors(g, gs1, gs2, data[4])
                gc_pos = encoder_model.encoder.cluster_projector(pos_neighbors)
                gc_neg = encoder_model.encoder.cluster_projector(neg_neighbors)
                
                total_loss, consistency_loss, entropy_loss, repulsion_loss = improved_cluster_criterion(gc, gc_pos, gc_neg)
            
    
                g1 = gi1.unsqueeze(1)
                g2 = gi2.unsqueeze(1)
                gg = torch.cat([g1, g2], dim=1)
                
                in_loss=criterion(gg, data[4])
                
                #total_loss, consistency_loss, entropy_loss = [], [], []
                # user_input = input("è¯·è¾“å…¥æ•°å­—1ä»¥ç»§ç»­è¿è¡Œ: ")
                
                #total_loss_, consistency_loss_, entropy_loss_ = criterion_cluster(gc1, gc2)
                #c_loss = criterion_cluster(gc1, gc2)
                
                
                cluster_loss = total_loss
                #cluster_loss = torch.sum(torch.stack(total_loss, dim=0))
                if args.used_loss == "instanceonly":
                    loss = in_loss
                elif args.used_loss == "clusteronly":
                    loss = cluster_loss
                else:
                    #loss = 3*in_loss + cluster_loss
                     loss = in_loss + args.cluster_loss_weight * total_loss
                #loss = loss_fn(output, data[4])
                #loss = F.cross_entropy(output, data[4])
                loss.backward()
                optimizer.step()
                
                losses.update(loss.item())
                instance_losses.update(in_loss.item())
                cluster_losses.update(cluster_loss.item())
                consistency_losses.update(consistency_loss.item())  # ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨.item()
                entropy_losses.update(entropy_loss.item())  # ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨.item()
                repulsion_losses.update(repulsion_loss.item())
                
                
                time_iter = time.time() - start
                train_loss += loss.item() * len(output)
                n_samples += len(output)
                scheduler.step()
                if batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1:
                    time_iter = time.time() - start
                    # ä¿®æ”¹: æ›´æ–°æ—¥å¿—æ¶ˆæ¯ï¼Œå¢åŠ æ’æ–¥æŸå¤±ä¿¡æ¯
                    log_message = 'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tInstance Loss: {:.6f}\tCluster Loss: {:.6f}\tConsistency Loss: {:.6f}\tEntropy Loss: {:.6f}\tRepulsion Loss: {:.6f}\tsec/iter: {:.4f}'.format(
                        epoch, (batch_idx + 1) * len(output), len(train_loader.dataset),
                        100. * (batch_idx + 1) / len(train_loader),
                        losses.avg, instance_losses.avg, cluster_losses.avg, 
                        consistency_losses.avg, entropy_losses.avg, repulsion_losses.avg,  # æ–°å¢: æ’æ–¥æŸå¤±
                        time_iter / (batch_idx + 1)
                    )
                    logger.info(log_message)
                    progress.display(batch_idx)
                  # ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹
            # ä¿å­˜encoderæ¨¡å‹ï¼ŒåŒ…å«foldä¿¡æ¯
            encoder_save_path = f'encoder_model_fold_{fold_id}_epoch_{epoch}.pth'
            torch.save(encoder_model.state_dict(), encoder_save_path)
            
            # ä¹Ÿä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä»¥ä¾¿å®Œæ•´æ¢å¤è®­ç»ƒ
            optimizer_save_path = f'encoder_model_fold_{fold_id}_epoch_{epoch}_optimizer.pth'
            torch.save(optimizer.state_dict(), optimizer_save_path)
        def test(test_loader, epoch, log_file="metrics.log"):
            encoder_model.eval()
            start_time = time.time()
            
            # åˆå§‹åŒ–å­˜å‚¨å®¹å™¨
            test_loss, correct, n_samples = 0, 0, 0
            preds_list, true_labels_list = [], []
            embeds_instance, embeds_cluster = [], []  # åˆ†åˆ«å­˜å‚¨ä¸¤ç§åµŒå…¥
            
            # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰åµŒå…¥å’Œæ ‡ç­¾
            all_embeddings = []
            all_true_labels = []
            all_cluster_embeddings = []
            
            with torch.no_grad():
                for batch_idx, data in enumerate(test_loader):
                    data = [d.to(device) for d in data]
                    
                    # è·å–æ¨¡å‹è¾“å‡º
                    g, _, _ = encoder_model(data)
                    
                    # è·å–åµŒå…¥
                    gi = encoder_model.encoder.instance_projector(g)  # å®ä¾‹çº§åµŒå…¥
                    gc = encoder_model.encoder.cluster_projector(g)  # ç°‡çº§åµŒå…¥
                    
                    # è®¡ç®—åˆ†ç±»æŸå¤±
                    loss = F.cross_entropy(gc, data[4], reduction='sum')
                    
                    test_loss += loss.item()
                    n_samples += len(gc)
                    
                    # æ”¶é›†åµŒå…¥å’ŒçœŸå®æ ‡ç­¾
                    all_embeddings.append(gi.cpu())
                    all_true_labels.extend(data[4].cpu().numpy())
                    all_cluster_embeddings.append(gc.cpu()) 
                    
                    # ä¿å­˜ç”¨äºåç»­èšç±»åˆ†æ
                    embeds_instance.append(gi.cpu())
                    embeds_cluster.append(gc.cpu())
            
            # å°†åµŒå…¥è½¬æ¢ä¸ºNumPyæ•°ç»„
            all_embeddings = torch.cat(all_embeddings).numpy()
            all_true_labels = np.array(all_true_labels)
            all_cluster_embeddings_tensor = torch.cat(all_cluster_embeddings)
            
            # æ–°å¢: ç›‘æ§ç°‡é—´è·ç¦»
            cluster_distances = monitor_cluster_distances(
                epoch, 
                all_cluster_embeddings_tensor, 
                torch.tensor(all_true_labels, device=all_cluster_embeddings_tensor.device)
            )
            # ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç°‡ä¸­å¿ƒ
            num_classes = 21  # å‡è®¾æœ‰21ä¸ªç±»åˆ«
            cluster_centers = {}
            
            for class_idx in range(num_classes):
                # è·å–å½“å‰ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
                class_mask = (all_true_labels == class_idx)
                if np.sum(class_mask) > 0:  # ç¡®ä¿è¯¥ç±»åˆ«æœ‰æ ·æœ¬
                    class_embeddings = all_embeddings[class_mask]
                    # è®¡ç®—è¯¥ç±»åˆ«çš„ç°‡ä¸­å¿ƒï¼ˆå¹³å‡å€¼ï¼‰
                    cluster_centers[class_idx] = np.mean(class_embeddings, axis=0)
            
            # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¼ªæ ‡ç­¾ï¼ˆåŸºäºè·ç¦»æœ€è¿‘çš„ç°‡ä¸­å¿ƒï¼‰
            pseudo_labels = []
            
            for i, embedding in enumerate(all_embeddings):
                distances = {}
                for class_idx, center in cluster_centers.items():
                    # è®¡ç®—æ¬§æ°è·ç¦»
                    dist = np.linalg.norm(embedding - center)
                    distances[class_idx] = dist
                
                # é€‰æ‹©è·ç¦»æœ€å°çš„ç±»åˆ«ä½œä¸ºä¼ªæ ‡ç­¾
                pseudo_label = min(distances, key=distances.get)
                pseudo_labels.append(pseudo_label)
            
            pseudo_labels = np.array(pseudo_labels)
            
            # ç¬¬å››æ­¥ï¼šè®¡ç®—è¯„ä¼°æŒ‡æ ‡
            correct = np.sum(pseudo_labels == all_true_labels)
            acc = 100.0 * correct / len(all_true_labels)
            
            # è½¬æ¢ä¸ºå¼ é‡ä»¥ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°å‡½æ•°
            true_labels_tensor = torch.tensor(all_true_labels)
            preds_tensor = torch.tensor(pseudo_labels)
            
            # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
            classnums = 21
            r = recall(preds_tensor, true_labels_tensor, classnums)
            p = precision(preds_tensor, true_labels_tensor, classnums)
            f1 = f1_score(preds_tensor, true_labels_tensor, classnums)
            
            # ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜
            conf_matrix = get_confusion_matrix(true_labels_tensor, preds_tensor)
            plt.figure(figsize=(18, 16), dpi=80)
            plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
            plt.title(f'Confusion Matrix - Cluster Distance Based (Epoch {epoch})')
            plt.colorbar()
            plt.savefig(f'confusion_matrix_cluster_based_epoch_{epoch}.png')
            plt.close()
            
            # ================ èšç±»è¯„ä¼° ================ #
            def evaluate_embeddings(embeds, prefix=""):
                """é€šç”¨åµŒå…¥è¯„ä¼°å‡½æ•°"""
                embeds_np = torch.cat(embeds).numpy()
                true_labels_np = np.array(all_true_labels)
                
                # æ£€æŸ¥åµŒå…¥ç»´åº¦
                assert embeds_np.shape[1] >= 2, f"{prefix}åµŒå…¥ç»´åº¦éœ€â‰¥2ï¼Œå½“å‰ä¸º{embeds_np.shape[1]}"
                
                # ä½¿ç”¨çœŸå®ç±»åˆ«æ•°è¿›è¡Œèšç±»
                n_clusters = len(np.unique(true_labels_np))
                kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0).fit(embeds_np)
                cluster_assignments = kmeans.labels_
                
                # è®¡ç®—èšç±»æŒ‡æ ‡
                nmi = metrics.normalized_mutual_info_score(true_labels_np, cluster_assignments)
                ari = metrics.adjusted_rand_score(true_labels_np, cluster_assignments)
                f = metrics.fowlkes_mallows_score(true_labels_np, cluster_assignments)
                silhouette = metrics.silhouette_score(embeds_np, cluster_assignments)
                
                # è®¡ç®—è°ƒæ•´åå‡†ç¡®ç‡
                pred_adjusted = get_y_preds(true_labels_np, cluster_assignments, n_clusters)
                acc = metrics.accuracy_score(pred_adjusted, true_labels_np)
                
                # å¯è§†åŒ–t-SNEå¹¶ä¿å­˜ï¼ˆä¸æ˜¾ç¤ºï¼‰
                plt.figure(figsize=(10, 8))
                tsne = TSNE(n_components=2, perplexity=30, random_state=0)
                embeds_2d = tsne.fit_transform(embeds_np[:1000])  # é™åˆ¶æ ·æœ¬æ•°é‡
                plt.scatter(embeds_2d[:,0], embeds_2d[:,1], 
                            c=true_labels_np[:1000], 
                            cmap='tab20', alpha=0.6)
                plt.title(f'{prefix}Embedding Space (t-SNE)')
                plt.colorbar()
                plt.savefig(f'{prefix}tsne_epoch_{epoch}.png')
                plt.close()
                
                return {
                    'nmi': nmi,
                    'ari': ari,
                    'f_score': f,
                    'silhouette': silhouette,
                    'acc': acc,
                }

            # è¯„ä¼°å®ä¾‹çº§åµŒå…¥
            metrics_instance = evaluate_embeddings(embeds_instance, "Instance-")
            
            # è¯„ä¼°ç°‡çº§åµŒå…¥
            metrics_cluster = evaluate_embeddings(embeds_cluster, "Cluster-")

            # è¿è¡Œè¯¦ç»†çš„èšç±»åˆ†æ
            instance_analysis = analyze_clustering(embeds_instance, all_true_labels, 
                                                n_classes=21, prefix="Instance-", 
                                                save_dir="./clustering_analysis/", 
                                                epoch=epoch)
            
            cluster_analysis = analyze_clustering(embeds_cluster, all_true_labels, 
                                                n_classes=21, prefix="Cluster-", 
                                                save_dir="./clustering_analysis/", 
                                                epoch=epoch)
            
            # ================ æ—¥å¿—è®°å½• ================ #
            with open(log_file, 'a') as f:
                # æ·»åŠ æ—¶é—´æˆ³å’Œåˆ†éš”çº¿
                f.write(f"\n{'='*80}\n")
                f.write(f"å®éªŒæ—¥æœŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"è½®æ¬¡ (Epoch): {epoch}\n")
                f.write(f"{'-'*80}\n\n")
                
                # åŸºäºç°‡ä¸­å¿ƒè·ç¦»çš„åˆ†ç±»è¯„ä¼°ç»“æœ
                f.write("ã€åŸºäºç°‡ä¸­å¿ƒè·ç¦»çš„åˆ†ç±»è¯„ä¼°ç»“æœã€‘\n")
                f.write(f"æŸå¤± (Loss): {test_loss/n_samples:.4f}\n")
                f.write(f"å‡†ç¡®ç‡ (Accuracy): {acc:.2f}%\n")
                
                
                # æ–°å¢: ç°‡é—´è·ç¦»ä¿¡æ¯
                f.write(f"\nã€ç°‡é—´è·ç¦»ç›‘æ§ã€‘\n")
                f.write(f"å¹³å‡ç°‡é—´è·ç¦»: {cluster_distances['avg_distance']:.4f}\n")
                f.write(f"æœ€å°ç°‡é—´è·ç¦»: {cluster_distances['min_distance']:.4f}\n")
                
                f.write("\nğŸ“Š ç±»åˆ«æŒ‡æ ‡æ±‡æ€»:\n")
                # ä½¿ç”¨æ›´å®½çš„åˆ—å’Œæ˜ç¡®çš„è¾¹æ¡†
                header = f"â”Œ{'â”€'*8}â”¬{'â”€'*8}â”¬{'â”€'*8}â”¬{'â”€'*8}â”\n"
                header += f"â”‚{'ç±»åˆ«ID':^8}â”‚{'å¬å›ç‡':^8}â”‚{'ç²¾ç¡®åº¦':^8}â”‚{'F1å€¼':^8}â”‚\n"
                header += f"â”œ{'â”€'*8}â”¼{'â”€'*8}â”¼{'â”€'*8}â”¼{'â”€'*8}â”¤\n"
                f.write(header)

                for class_idx in range(len(r)):
                    f.write(f"â”‚{class_idx:^8}â”‚{r[class_idx].item():.4f}â”‚{p[class_idx].item():.4f}â”‚{f1[class_idx].item():.4f}â”‚\n")

                f.write(f"â””{'â”€'*8}â”´{'â”€'*8}â”´{'â”€'*8}â”´{'â”€'*8}â”˜\n\n")
                
                # èšç±»è¯„ä¼° - å®ä¾‹çº§åµŒå…¥
                f.write(f"{'-'*80}\n")
                f.write("ã€èšç±»è¯„ä¼° - å®ä¾‹çº§åµŒå…¥ã€‘\n")
                f.write(f"æ ‡å‡†åŒ–äº’ä¿¡æ¯ (NMI): {metrics_instance['nmi']:.4f}\n")
                f.write(f"è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI): {metrics_instance['ari']:.4f}\n")
                f.write(f"Fowlkes-Mallowsåˆ†æ•°: {metrics_instance['f_score']:.4f}\n")
                f.write(f"è½®å»“ç³»æ•° (Silhouette): {metrics_instance['silhouette']:.4f}\n")
                f.write(f"è°ƒæ•´åå‡†ç¡®ç‡ (Adjusted Acc): {metrics_instance['acc']:.4f}\n")
                
                # èšç±»è¯„ä¼° - ç°‡çº§åµŒå…¥
                f.write(f"{'-'*80}\n")
                f.write("ã€èšç±»è¯„ä¼° - ç°‡çº§åµŒå…¥ã€‘\n")
                f.write(f"æ ‡å‡†åŒ–äº’ä¿¡æ¯ (NMI): {metrics_cluster['nmi']:.4f}\n")
                f.write(f"è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI): {metrics_cluster['ari']:.4f}\n")
                f.write(f"Fowlkes-Mallowsåˆ†æ•°: {metrics_cluster['f_score']:.4f}\n")
                f.write(f"è½®å»“ç³»æ•° (Silhouette): {metrics_cluster['silhouette']:.4f}\n")
                f.write(f"è°ƒæ•´åå‡†ç¡®ç‡ (Adjusted Acc): {metrics_cluster['acc']:.4f}\n")
                
                # æ–°å¢ï¼šæ·»åŠ è¯¦ç»†èšç±»åˆ†æç»“æœè·¯å¾„
                f.write(f"{'-'*80}\n")
                f.write("ã€è¯¦ç»†èšç±»åˆ†æã€‘\n")
                f.write(f"å®ä¾‹çº§åµŒå…¥èšç±»åˆ†ææŠ¥å‘Š: {instance_analysis['report_path']}\n")
                f.write(f"ç°‡çº§åµŒå…¥èšç±»åˆ†ææŠ¥å‘Š: {cluster_analysis['report_path']}\n")
                
                # æ€»ç»“ä¿¡æ¯
                f.write(f"{'-'*80}\n")
                f.write(f"æ€»è€—æ—¶: {time.time()-start_time:.2f}ç§’\n")
                f.write(f"{'='*80}\n")
            
            # åªåœ¨æ—¥å¿—ä¸­æ·»åŠ ç®€è¦è¿›åº¦æç¤º
            print(f"Epoch {epoch}: è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {log_file}")
            print(f"è¯¦ç»†èšç±»åˆ†æå·²ä¿å­˜åˆ° clustering_analysis/ ç›®å½•")
            
            return {
                'classification': {
                    'loss': test_loss/n_samples,
                    'acc': acc,
                    'recall': r.tolist(),
                    'precision': p.tolist(),
                    'f1': f1.tolist()
                },
                'instance_embedding': metrics_instance,
                'cluster_embedding': metrics_cluster,
                'instance_clustering_analysis': instance_analysis,
                'cluster_clustering_analysis': cluster_analysis,
                'cluster_centers': {k: v.tolist() for k, v in cluster_centers.items()},  # ä¿å­˜ç°‡ä¸­å¿ƒ
                'cluster_distances': cluster_distances,
                'time': time.time()-start_time
            }   
            
        
        def plot_confusion_matrix(conf_matrix, num_classes, epoch):
            plt.imshow(conf_matrix, cmap=plt.cm.Blues)
            indices = range(len(conf_matrix))
            if num_classes == 21:
                classes = list(range(21))
            elif num_classes == 18:
                classes = list(range(18))
            elif num_classes == 15:
                classes = list(range(15))
            elif num_classes == 27:
                classes = list(range(27))
            elif num_classes == 33:
                classes = list(range(33))
            plt.xticks(indices, classes)
            plt.yticks(indices, classes)
            plt.colorbar()
            plt.xlabel('y_pred')
            plt.ylabel('y_true')
            for first_index in range(len(conf_matrix)):
                for second_index in range(len(conf_matrix[first_index])):
                    plt.text(first_index, second_index, conf_matrix[second_index, first_index])
            plt.savefig('./fig{}.png'.format(epoch), format='png')
            plt.show()
    
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        def get_confusion_matrix(label, pred):
            return confusion_matrix(label, pred)
    
        #loss_fn = F.cross_entropy  # å®šä¹‰æŸå¤±å‡½æ•°
        criterion = SupConLoss(temperature=0.07)
        criterion_cluster = ClusterLoss(args.entropy_weight).to(device)
        improved_cluster_criterion = ImprovedClusterLoss(
            entropy_weight=args.entropy_weight, 
            repulsion_weight=args.repulsion_weight, 
            temperature=args.cluster_temperature
        ).to(device)
        max_acc = 0.0
        t_start = time.time()
        for epoch in range(start_epoch + 1, args.epochs + 1):
            train(loaders[0], epoch)
            #scheduler.step()
            result = test(loaders[1], epoch,log_file="experiment1.log")
            #acc = test(loaders[1], epoch)
            acc=result['classification']['acc']
            accuracy_arr[fold_id][epoch] = acc
            max_acc = max(max_acc, acc)
        print("time: {:.4f}s".format(time.time() - t_start))
        
        acc_folds.append(max_acc)
    
    print("Accuracies for each fold:", acc_folds)
    # ä»¥ä¸‹ä»£ç å¯ç”¨äºè®¡ç®—äº¤å‰éªŒè¯çš„å¹³å‡å‡†ç¡®ç‡å’Œæ ‡å‡†å·®
    # mean_validation = accuracy_arr.mean(axis=0)
    # maximum_epoch = np.argmax(mean_validation)
    # average = np.mean(accuracy_arr[:, maximum_epoch])
    # standard_dev = np.std(accuracy_arr[:, maximum_epoch])
    # print('{}-fold cross validation avg acc (+- std): {} ({})'.format(args.n_folds, average, standard_dev))

if __name__ == '__main__':
    main()